{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cef6f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.2\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a29db25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pubchempy as pcp\n",
    "import numpy as np\n",
    "def isNaN(string):\n",
    "    return string != string\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "from pybatchclassyfire import *\n",
    "import csv\n",
    "import time\n",
    "import json\n",
    "from pandas import json_normalize\n",
    "import wget\n",
    "import string\n",
    "import urllib.parse\n",
    "import openpyxl\n",
    "import statistics\n",
    "import sys\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9d9fdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdFMCS\n",
    "from rdkit.Chem import PandasTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b3387be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_dir = \"/Users/mahnoorzulfiqar/OneDriveUNI/MAW-data/StandardSMarinoi_Data\"\n",
    "#input_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aea6026d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/mahnoorzulfiqar/OneDriveUNI/MAW-data/Test_Python'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define input directory, keep all files in same directory and scripts so getwd works\n",
    "input_dir = \"/Users/mahnoorzulfiqar/OneDriveUNI/MAW-data/Test_Python\"\n",
    "input_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3224a835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the function file\n",
    "from Workflow_Python_Functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08fe4556",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m compdb_postprocessing \u001b[38;5;241m=\u001b[39m \u001b[43msirius_postproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/Users/mahnoorzulfiqar/OneDriveUNI/MAW-data/Test_Python\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mexp_int\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.90\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsi_score\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/OneDriveUNI/MAW/Workflow_Python_Functions.py:689\u001b[0m, in \u001b[0;36msirius_postproc\u001b[0;34m(input_dir, exp_int, csi_score)\u001b[0m\n\u001b[1;32m    685\u001b[0m     ALL_structure_csv\u001b[38;5;241m.\u001b[39mto_csv(sub_dir\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults_for_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mjson_dirALL[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstructure_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mjson_dirALL[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(sub_sub_dirALL_structure_can) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pd\u001b[38;5;241m.\u001b[39mread_csv(sub_sub_dirALL_structure_can, sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))):\n\u001b[0;32m--> 689\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(sub_sub_dirALL_formula_can) \u001b[38;5;129;01mand\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mread_csv(sub_sub_dirALL_formula_can, sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    690\u001b[0m         ALL_formula_csv \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(sub_sub_dirALL_formula_can, sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    691\u001b[0m         ALL_Canopus \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(ALL_Canopus_csv, sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mawRpy/lib/python3.10/site-packages/pandas/core/generic.py:1535\u001b[0m, in \u001b[0;36mNDFrame.__nonzero__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1533\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   1534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__nonzero__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1535\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1536\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe truth value of a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is ambiguous. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1537\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse a.empty, a.bool(), a.item(), a.any() or a.all().\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1538\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "compdb_postprocessing = sirius_postproc(input_dir='/Users/mahnoorzulfiqar/OneDriveUNI/MAW-data/Test_Python',\n",
    "                                        exp_int = 0.90, csi_score = -150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7059551",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mcss_compdb_res \u001b[38;5;241m=\u001b[39m \u001b[43mMCSS_for_SIRIUS\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/OneDriveUNI/MAW/Workflow_Python_Functions.py:875\u001b[0m, in \u001b[0;36mMCSS_for_SIRIUS\u001b[0;34m(input_dir)\u001b[0m\n\u001b[1;32m    873\u001b[0m r \u001b[38;5;241m=\u001b[39m [s \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(sir_file) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m s][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    874\u001b[0m sirius_f \u001b[38;5;241m=\u001b[39m (glob\u001b[38;5;241m.\u001b[39mglob((sir_file\u001b[38;5;241m+\u001b[39mr)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/*.csv\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m--> 875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(sirius_msp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid_X\u001b[39m\u001b[38;5;124m\"\u001b[39m][mz]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;129;01min\u001b[39;00m \u001b[43msirius_f\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[1;32m    876\u001b[0m     \u001b[38;5;28mprint\u001b[39m(sirius_f)\n\u001b[1;32m    877\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sirius_f) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "mcss_compdb_res = MCSS_for_SIRIUS(input_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a37ac06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f166920e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbacc1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_can_score(db, i):\n",
    "    if db['explainedIntensity'][i] >= exp_int and db['CSI:FingerIDScore'][i] >= csi_score:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1043aa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_int = 0.90\n",
    "csi_score = -150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aee679d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/mahnoorzulfiqar/OneDriveUNI/MAW-data/Test_Python'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9dae1a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_02_spring_Fistax_2_1-A-2_01_17276\n",
      "pos_02_summer_Fistax_2_1-A-2_01_17171\n"
     ]
    }
   ],
   "source": [
    "# entry is all files and folders in input_dir\n",
    "for entry in os.listdir(input_dir):\n",
    "    #if the entry is also a directory\n",
    "    if os.path.isdir(os.path.join(input_dir, entry)):\n",
    "        print(entry)\n",
    "        sub_dir = input_dir + \"/\" + entry + '/insilico/SIRIUS/'\n",
    "        msp_csv = input_dir + \"/\" + entry + \"/insilico/MS1DATA.csv\"\n",
    "        if os.path.exists(msp_csv) and os.path.exists(sub_dir):\n",
    "            #output json files from SIRIUS\n",
    "            files_S = (glob.glob(sub_dir+'/*.json'))\n",
    "            #list of precursor m/z\n",
    "            msp = pd.read_csv(msp_csv)\n",
    "\n",
    "            # for each mz\n",
    "            for mz, row in msp.iterrows():\n",
    "                # make a list of files with this mz\n",
    "                files_for_mz = []\n",
    "\n",
    "                for file in files_S:\n",
    "                    if str(msp[\"premz\"][mz]) in file:\n",
    "                        files_for_mz.append(file)\n",
    "                if len(files_for_mz) == 1:\n",
    "                    # extract the formula and structure files\n",
    "                    json_dirALL = next(os.walk(files_for_mz[0]))[1]\n",
    "                    sub_sub_dirALL_structure_can = files_for_mz[0] + \"/\" + json_dirALL[0]  + \"/structure_candidates.tsv\"                   \n",
    "                    sub_sub_dirALL_formula_can = files_for_mz[0] + \"/\" + json_dirALL[0]  + \"/formula_candidates.tsv\" \n",
    "                    ALL_Canopus_csv = files_for_mz[0] + \"/canopus_summary.tsv\"\n",
    "\n",
    "                    # if both structure files exist\n",
    "                    if os.path.exists(sub_sub_dirALL_structure_can) and len(pd.read_csv(sub_sub_dirALL_structure_can, sep = \"\\t\"))>0:\n",
    "                        ALL_structure_csv = pd.read_csv(sub_sub_dirALL_structure_can, sep = \"\\t\")\n",
    "                        ALL_formula_csv = pd.read_csv(sub_sub_dirALL_formula_can, sep = \"\\t\")\n",
    "                        ALL_Canopus = pd.read_csv(ALL_Canopus_csv, sep = \"\\t\")\n",
    "                        # Add the structure and formula files together\n",
    "                        for structure, rows in ALL_structure_csv.iterrows():\n",
    "                            for formula, rows in ALL_formula_csv.iterrows():\n",
    "                                if ALL_structure_csv[\"formulaRank\"][structure] == ALL_formula_csv[\"rank\"][formula]:\n",
    "                                    ALL_structure_csv.loc[structure, 'SiriusScore'] = ALL_formula_csv['SiriusScore'][formula]\n",
    "                                    ALL_structure_csv.loc[structure, 'numExplainedPeaks'] = ALL_formula_csv['numExplainedPeaks'][formula]\n",
    "                                    ALL_structure_csv.loc[structure, 'explainedIntensity'] = ALL_formula_csv['explainedIntensity'][formula]\n",
    "                                    ALL_structure_csv.loc[structure, \"SuspectListEntry\"] = \"FALSE\"\n",
    "                                    if len(ALL_Canopus)>0:\n",
    "                                        if ALL_formula_csv[\"molecularFormula\"][formula] == ALL_Canopus[\"molecularFormula\"][0]:\n",
    "                                            ALL_structure_csv.loc[structure, 'superclass'] = ALL_Canopus['superclass'][0]\n",
    "                                            ALL_structure_csv.loc[structure, 'class'] = ALL_Canopus['class'][0]\n",
    "                                            ALL_structure_csv.loc[structure, 'subclass'] = ALL_Canopus['subclass'][0]\n",
    "                                            \n",
    "                        for str_siriusA, row in ALL_structure_csv.iterrows():\n",
    "                            if not str_can_score(ALL_structure_csv, str_siriusA):\n",
    "                                ALL_structure_csv = ALL_structure_csv.drop(str_siriusA, inplace=False)\n",
    "                        if not os.path.exists(sub_dir+\"results_for_\"+json_dirALL[0].split(\"_\")[-1]):\n",
    "                            os.mkdir(sub_dir+\"results_for_\"+json_dirALL[0].split(\"_\")[-1])\n",
    "                            \n",
    "                        result_sirius_name = (sub_dir+\"results_for_\"+json_dirALL[0].split(\"_\")[-1]+\"_\"+\"structure_\"+json_dirALL[0].split(\"_\")[-1] + \".csv\")\n",
    "                        msp.loc[mz, \"sirius_result_dir\"] = result_sirius_name.replace(input_dir, \".\")\n",
    "\n",
    "                        ALL_structure_csv.to_csv(sub_dir+\"results_for_\"+json_dirALL[0].split(\"_\")[-1]+\"_\"+\"structure_\"+json_dirALL[0].split(\"_\")[-1] + \".csv\")\n",
    "\n",
    "\n",
    "                    elif not (os.path.exists(sub_sub_dirALL_structure_can) and len(pd.read_csv(sub_sub_dirALL_structure_can, sep = \"\\t\"))>0):\n",
    "                        if os.path.exists(sub_sub_dirALL_formula_can) and len(pd.read_csv(sub_sub_dirALL_formula_can, sep = \"\\t\"))>0:\n",
    "                            ALL_formula_csv = pd.read_csv(sub_sub_dirALL_formula_can, sep = \"\\t\")\n",
    "                            ALL_Canopus = pd.read_csv(ALL_Canopus_csv, sep = \"\\t\")\n",
    "                            if len(ALL_Canopus)>0:\n",
    "                                for formula, rows in ALL_formula_csv.iterrows():\n",
    "                                    ALL_formula_csv.loc[formula, 'superclass'] = ALL_Canopus['superclass'][0]\n",
    "                                    ALL_formula_csv.loc[formula, 'class'] = ALL_Canopus['class'][0]\n",
    "                                    ALL_formula_csv.loc[formula, 'subclass'] = ALL_Canopus['subclass'][0]\n",
    "\n",
    "                            for for_siriusA, row in ALL_formula_csv.iterrows():\n",
    "                                if not ALL_formula_csv['explainedIntensity'][for_siriusA] >= exp_int:\n",
    "                                    ALL_formula_csv = ALL_formula_csv.drop(for_siriusA, inplace=False)\n",
    "                            if not os.path.exists(sub_dir+\"results_for_\"+json_dirALL[0].split(\"_\")[-1]):\n",
    "                                os.mkdir(sub_dir+\"results_for_\"+json_dirALL[0].split(\"_\")[-1])\n",
    "\n",
    "                            result_sirius_name = (sub_dir+\"results_for_\"+json_dirALL[0].split(\"_\")[-1]+\"_\"+\"formula_\"+json_dirALL[0].split(\"_\")[-1] + \".csv\")\n",
    "                            msp.loc[mz, \"sirius_result_dir\"] = result_sirius_name.replace(input_dir, \".\")\n",
    "\n",
    "                            ALL_formula_csv.to_csv(sub_dir+\"results_for_\"+json_dirALL[0].split(\"_\")[-1]+\"_\"+\"formula_\"+json_dirALL[0].split(\"_\")[-1] + \".csv\")\n",
    "\n",
    "                        else:\n",
    "                            pass\n",
    "                    else:\n",
    "                        pass\n",
    "            msp.to_csv(msp_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35be4f17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d747318",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b07a62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c77023",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1314bd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38dcd7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chemMN_CandidateSelection(df, tn_sim = 0.85):\n",
    "    \n",
    "    \"\"\"chemMN_CandidateSelection function is used to generate a Cytoscape readable tsv file.\n",
    "    This file contains start(starting SMILES) and end(target SMILES) nodes and the tanimoto \n",
    "    similarity scores between the nodes. User can visualize the structural similarity \n",
    "    between the given SMILES. It provides an \"ALL against ALL\" network.\n",
    "    \n",
    "    Parameters: \n",
    "    df: dataframe that contains \"SMILES\", \"ranks\", \"Source\". This function is specifically for \n",
    "    candidate selection and so these columns are necessary.\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    dataframe: it returns a df with follwoing columns to be loaded into Cytoscape.\n",
    "    1. Start, starting node/SMILES\n",
    "    2. End, ending node/SMILES\n",
    "    3. Tanimoto, Tanimoto between Start and End node\n",
    "    4. Start_SMILES\n",
    "    5. End_SMILES\n",
    "    6. Start_Source\n",
    "    7. End_Source\n",
    "    8. MCSS, Maximum Common Substructure between start and end node/SMILES\n",
    "    9. sorted_row, contains ids of the start and end nodes as a list\n",
    "    \n",
    "    \n",
    "    Usage:\n",
    "    chemMN_CandidateSelection(df)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # define an empty variable \n",
    "    #one_df = []\n",
    "    # define empty variable to save the edges\n",
    "    dbn= []\n",
    "    # for each entry in the df\n",
    "    for i, row in df.iterrows():\n",
    "        # to compare each element with each other element of the df\n",
    "        for j, row in df.iterrows():\n",
    "            try:\n",
    "                # calcultae tanimoto\n",
    "                ms = [Chem.MolFromSmiles(df['SMILES'][i]), Chem.MolFromSmiles(df['SMILES'][j])]\n",
    "                fps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in ms]\n",
    "                tn = DataStructs.FingerprintSimilarity(fps[0],fps[1])\n",
    "                \n",
    "                # save all entries to a matrix\n",
    "                dbn.append({\n",
    "                    'Name_i':df['ranks'][i],\n",
    "                    'Name_j':df['ranks'][j],\n",
    "                    'i': df['SMILES'][i],\n",
    "                    'j': df['SMILES'][j],\n",
    "                    'Source_i':df['Source'][i],\n",
    "                    'Source_j':df['Source'][j],\n",
    "                    'Tanimoto': tn\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e.string)\n",
    "                pass\n",
    "\n",
    "    # save chemical similarities                    \n",
    "    db_edgenode = pd.DataFrame(dbn)\n",
    "\n",
    "    # another empty variable to store the results for final tsv file\n",
    "    dfe = []\n",
    "    \n",
    "    # heavy atoms for MCSS Calculation\n",
    "    heavy_atoms = ['C', 'N', 'P', 'O', 'S']\n",
    "    \n",
    "    # for the previous dataframe\n",
    "    for i, row in db_edgenode.iterrows():      \n",
    "        # if the tanimoto > 0.85 for high similarity\n",
    "        if db_edgenode['Tanimoto'][i] >= tn:\n",
    "            \n",
    "            # calculate MCSS\n",
    "            n = [Chem.MolFromSmiles(db_edgenode['i'][i]),Chem.MolFromSmiles(db_edgenode['j'][i])]\n",
    "            res = rdFMCS.FindMCS(n)\n",
    "            sm_res = Chem.MolToSmiles(Chem.MolFromSmarts(res.smartsString))\n",
    "            \n",
    "            \n",
    "            # Check if the MCSS has one of the heavy atoms and whether they are\n",
    "            # more than 3\n",
    "            elem = [ele for ele in heavy_atoms if(ele in sm_res)]\n",
    "            if elem and len(sm_res)>=3:\n",
    "                MCSS_SMILES = Chem.MolToSmiles(Chem.MolFromSmarts(res.smartsString))\n",
    "            \n",
    "            # save everything into a dataframe\n",
    "            dfe.append({\n",
    "                'Start':db_edgenode['Name_i'][i],\n",
    "                'End':db_edgenode['Name_j'][i],\n",
    "                'Tanimoto':db_edgenode['Tanimoto'][i],\n",
    "                'Start_SMILES':db_edgenode['i'][i],\n",
    "                'End_SMILES':db_edgenode['j'][i],\n",
    "                'Start_Source':db_edgenode['Source_i'][i],\n",
    "                'End_Source':db_edgenode['Source_j'][i],\n",
    "                'MCSS': MCSS_SMILES\n",
    "            })\n",
    "    df_edge = pd.DataFrame(dfe)\n",
    "    # generate a column called sorted_row which contains ids of the start and end nodes as a list\n",
    "    df_edge['Start'] = df_edge['Start'].astype(str)\n",
    "    df_edge['End'] = df_edge['End'].astype(str)\n",
    "    df_edge['sorted_row'] = [sorted([a,b]) for a,b in zip(df_edge.Start,df_edge.End)]\n",
    "    df_edge['sorted_row'] = df_edge['sorted_row'].astype(str)\n",
    "    df_edge.drop_duplicates(subset=['sorted_row'], inplace=True)\n",
    "\n",
    "    #nodes= []\n",
    "    #for i, row in df.iterrows():\n",
    "        #n = df['ranks'][i]\n",
    "        #nodes.append({\n",
    "            #'nodes':n\n",
    "        #})\n",
    "\n",
    "    #node= pd.DataFrame(nodes)\n",
    "\n",
    "    return(df_edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b547d207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_candidate_selection(df, Source = \"SGHM\", tn_ident = 0.99):\n",
    "    \n",
    "    \"\"\"one_candidate_selection function is used to generate a dataframe that tells, \n",
    "    for each candidate SMILES, what was the source or how many sources had the same \n",
    "    candidate. The idea is to merge all candidate SMILES into one list, preserving\n",
    "    the rank and source, and then checking whether these SMILES come from SIRIUS or\n",
    "    any spectral DB. If a SMILE is repeated in more sources, its confidence score \n",
    "    increases and is considered the most likely candidate structure. This function \n",
    "    is not stand-alone and is part of the function CandidateSelection_SimilarityandIdentity\n",
    "\n",
    "    \n",
    "    Parameters: \n",
    "    df: dataframe that contains \"SMILES\", \"ranks\", \"Source\". This function is \n",
    "    specifically for candidate selection and so these columns are necessary.\n",
    "    Source: this depends on how many sources were used. Possiblilities are: \n",
    "    1. SGHM (all)\n",
    "    2. SGM (SIRIUS, GNPS, MassBank)\n",
    "    3. SHM (SIRIUS, HMDB, MassBank)\n",
    "    4. SGH (SIRIUS, GNPS, HMDB)\n",
    "    5. GHM (GNPS, HMDB, MassBank)\n",
    "    6. SG (SIRIUS, GNPS)\n",
    "    7. SH (SIRIUS, HMDB)\n",
    "    8. SM (SIRIUS, MassBank)\n",
    "    9. GM (GNPS, MassBank)\n",
    "    10. GH (GNPS, HMDB)\n",
    "    11. HM (HMDB, MassBank)\n",
    "    \n",
    "    Returns:\n",
    "    dataframe: it returns a df with follwoing columns which can be used to \n",
    "    prioritize a database for the final candidate selection.\n",
    "    1. Source, contains name of the source (SIRIUS, GNPS, HMDB or MassBank)\n",
    "    2. ranks, contains first letter of the source and a rank number seperated \n",
    "    by _ e.g: G_1(GNPS, 1st rank)\n",
    "    3. SMILES\n",
    "    4. SIRIUS, the rank again but only when the corresponding row SMILES is \n",
    "    also part of SIRIUS results\n",
    "    5. GNPS , same as SIRIUS but for GNPS\n",
    "    5. MassBank\n",
    "    6. HMDB\n",
    "    \n",
    "    \n",
    "    Usage:\n",
    "    chemMN_CandidateSelection(df, Source = \"SGHM\")\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # define empty columns for each Source to only fill if the corresponding \n",
    "    # SMILES is also present in the source\n",
    "    \n",
    "    df[\"SIRIUS\"] = np.nan\n",
    "    df[\"GNPS\"] = np.nan\n",
    "    df[\"MassBank\"] = np.nan\n",
    "    df[\"HMDB\"] = np.nan\n",
    "    \n",
    "    # for each SMILES in df\n",
    "    for smiles, rows in df.iterrows():\n",
    "        \n",
    "        # If the source contains SIRIUS\n",
    "        if Source == \"SGHM\" or Source == \"SGM\" or Source == \"SGH\" or Source == \"SHM\" or Source == \"SG\" or Source == \"SM\" or Source == \"SH\":\n",
    "            # sirius_df comes from within the function CandidateSelection_SimilarityandIdentity\n",
    "            for sirius_i, row in sirius_df.iterrows():\n",
    "                # calculate tanimoto\n",
    "                try:\n",
    "                    ms = [Chem.MolFromSmiles(df[\"SMILES\"][smiles]), Chem.MolFromSmiles(sirius_df[\"smiles\"][sirius_i])]\n",
    "                    fps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in ms]\n",
    "                    tn = DataStructs.FingerprintSimilarity(fps[0],fps[1])\n",
    "                    # since we are dealing with idenity here so tanimoto of 0.99 is appropriate\n",
    "                    if tn >= tn_ident:\n",
    "                        \n",
    "                        # if SIRIUS is blank, add the SIRIUS id\n",
    "                        if isNaN(df[\"SIRIUS\"][smiles]):\n",
    "\n",
    "                            df.loc[smiles, \"SIRIUS\"] = sirius_df[\"rank_ids\"][sirius_i]\n",
    "                        # if not empty, add SIRIUS id, with a comma\n",
    "                        else:\n",
    "                            df.loc[smiles, \"SIRIUS\"] = str(df[\"SIRIUS\"][smiles]) + \", \"+ sirius_df[\"rank_ids\"][sirius_i]\n",
    "                            \n",
    "                       \n",
    "                except Exception as e:\n",
    "                    print(e.string)\n",
    "                    pass\n",
    "        \n",
    "        \n",
    "        # If the Source contains GNPS\n",
    "        if Source == \"SGHM\" or Source == \"SGM\" or Source == \"SGH\" or Source == \"GHM\" or Source == \"SG\" or Source == \"GM\" or Source == \"GH\":\n",
    "\n",
    "            # gnps_df comes from within the function CandidateSelection_SimilarityandIdentity\n",
    "            for gnps_i, row in gnps_df.iterrows():\n",
    "                try:\n",
    "                    # calculate tanimoto\n",
    "                    ms = [Chem.MolFromSmiles(df[\"SMILES\"][smiles]), Chem.MolFromSmiles(gnps_df[\"GNPSSMILES\"][gnps_i])]\n",
    "                    fps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in ms]\n",
    "                    tn = DataStructs.FingerprintSimilarity(fps[0],fps[1])\n",
    "                    \n",
    "                    # since we are dealing with idenity here so tanimoto of 0.99 is appropriate\n",
    "                    if tn >= tn_ident:\n",
    "                        \n",
    "                        # if GNPS is blank, add the GNPS id\n",
    "                        if isNaN(df[\"GNPS\"][smiles]):\n",
    "\n",
    "                            df.loc[smiles, \"GNPS\"] = gnps_df[\"rank_ids\"][gnps_i]\n",
    "                        # if not empty, add GNPS id, with a comma\n",
    "                        else:\n",
    "                            df.loc[smiles, \"GNPS\"] = str(df[\"GNPS\"][smiles]) + \", \"+ gnps_df[\"rank_ids\"][gnps_i]          \n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(e.string)\n",
    "                    pass\n",
    "\n",
    "        # If the source contains MassBank\n",
    "        if Source == \"SGHM\" or Source == \"SGM\" or Source == \"SHM\" or Source == \"GHM\" or Source == \"SM\" or Source == \"GM\" or Source == \"HM\":\n",
    "            # mbank_df comes from within the function CandidateSelection_SimilarityandIdentity\n",
    "            for mbank_i, row in mbank_df.iterrows():\n",
    "                try:\n",
    "                    # calculate tanimoto\n",
    "                    ms = [Chem.MolFromSmiles(df[\"SMILES\"][smiles]), Chem.MolFromSmiles(mbank_df[\"MBSMILES\"][mbank_i])]\n",
    "                    fps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in ms]\n",
    "                    tn = DataStructs.FingerprintSimilarity(fps[0],fps[1])\n",
    "                    \n",
    "                    # since we are dealing with idenity here so tanimoto of 0.99 is appropriate\n",
    "                    if tn >= tn_ident:\n",
    "                        \n",
    "                        # if MassBank is blank, add the MassBank id\n",
    "                        if isNaN(df[\"MassBank\"][smiles]):\n",
    "\n",
    "                            df.loc[smiles, \"MassBank\"] = mbank_df[\"rank_ids\"][mbank_i]\n",
    "                        # if not empty, add MassBank id, with a comma\n",
    "                        else:\n",
    "                            df.loc[smiles, \"MassBank\"] = str(df[\"MassBank\"][smiles]) + \", \"+ mbank_df[\"rank_ids\"][mbank_i]\n",
    "                            \n",
    "                       \n",
    "                except Exception as e:\n",
    "                    print(e.string)\n",
    "                    pass\n",
    "\n",
    "\n",
    "        # If the source contains HMDB\n",
    "        if Source == \"SGHM\" or Source == \"SGH\" or Source == \"SHM\" or Source == \"GHM\" or Source == \"SH\" or Source == \"GH\" or Source == \"HM\":\n",
    "\n",
    "\n",
    "            for hmdb_i, row in hmdb_df.iterrows():\n",
    "                try:\n",
    "                    # calculate tanimoto\n",
    "                    ms = [Chem.MolFromSmiles(df[\"SMILES\"][smiles]), Chem.MolFromSmiles(hmdb_df[\"HMDBSMILES\"][hmdb_i])]\n",
    "                    fps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in ms]\n",
    "                    tn = DataStructs.FingerprintSimilarity(fps[0],fps[1])\n",
    "                    \n",
    "                    # since we are dealing with idenity here so tanimoto of 0.99 is appropriate\n",
    "                    if tn >= tn_ident:\n",
    "                        \n",
    "                        # if HMDB is blank, add the HMDB id\n",
    "                        if isNaN(df[\"HMDB\"][smiles]):\n",
    "\n",
    "                            df.loc[smiles, \"HMDB\"] = hmdb_df[\"rank_ids\"][hmdb_i]\n",
    "                        # if not empty, add HMDB id, with a comma\n",
    "                        else:\n",
    "                            df.loc[smiles, \"HMDB\"] = str(df[\"HMDB\"][smiles]) + \", \"+ hmdb_df[\"rank_ids\"][hmdb_i]\n",
    "                            \n",
    "                       \n",
    "                except Exception as e:\n",
    "                    print(e.string)\n",
    "                    pass\n",
    " \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb35f40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CandidateSelection_SimilarityandIdentity(input_dir):\n",
    "\n",
    "    # entry is all files and folders in input_dir\n",
    "    for entry in os.listdir(input_dir):\n",
    "         #if the entry is also a directory\n",
    "        if os.path.isdir(os.path.join(input_dir, entry)):\n",
    "\n",
    "\n",
    "            # reach spectra_dereplication folder\n",
    "            sub_dir_spec = input_dir + \"/\" + entry + '/spectral_dereplication/'\n",
    "            # reach SIRIUS results\n",
    "            sub_dir_sir = input_dir + \"/\" + entry + '/insilico/SIRIUS/'\n",
    "\n",
    "            #list of all csv files in the spectral dereplication foler\n",
    "            spec_msp_csv = (glob.glob(input_dir + \"/\" + entry + '/spectral_dereplication' +'/*.csv'))\n",
    "            # Sirius csv result file\n",
    "            sir_msp_csv = input_dir + \"/\" + entry + \"/insilico/MS1DATA.csv\"\n",
    "\n",
    "            # if both exist; which should be the case, even in case of 0 results\n",
    "            if os.path.exists(sir_msp_csv) and os.path.exists(spec_msp_csv[0]):\n",
    "\n",
    "                # read both csv files\n",
    "                spec_msv = pd.read_csv(spec_msp_csv[0])\n",
    "                sir_msv = pd.read_csv(sir_msp_csv)\n",
    "\n",
    "                spec_msv = spec_msv[[\n",
    "                    'id_X',\n",
    "                    'premz',\n",
    "                    'rtmin',\n",
    "                    'rtmax',\n",
    "                    'rtmed',\n",
    "                    'rtmean',\n",
    "                    'col_eng',\n",
    "                    'pol',\n",
    "                    'int',\n",
    "                    'source_file',\n",
    "                    'mbank_results_csv',\n",
    "                    ]]\n",
    "                sir_msv = sir_msv[['id_X',\n",
    "                    'premz',\n",
    "                    'rtmed',\n",
    "                    'rtmean',\n",
    "                    'int',\n",
    "                    'col_eng',\n",
    "                    'pol',\n",
    "                    'ms2Peaks',\n",
    "                    'ms1Peaks',\n",
    "                    'sirius_result_dir']]\n",
    "\n",
    "                merged_df = sir_msv.merge(spec_msv, \n",
    "                              how='inner', \n",
    "                              left_on=['premz', 'rtmed','rtmean','int','col_eng','pol'], \n",
    "                              right_on=['premz','rtmed','rtmean','int','col_eng','pol'])\n",
    "\n",
    "                for mer, rows in merged_df.iterrows():\n",
    "                    print(mer)\n",
    "\n",
    "                    sirius_csv = merged_df[\"sirius_result_dir\"][mer].replace(\"./\", input_dir+\"/\")\n",
    "                    mbank_csv = merged_df[\"mbank_results_csv\"][mer].replace(\"./\", input_dir+\"/\")\n",
    "                    gnps_csv = merged_df[\"mbank_results_csv\"][mer].replace(\"./\", input_dir+\"/\").replace('mbank', 'gnps').replace('MassBank', 'GNPS')\n",
    "                    hmdb_csv = merged_df[\"mbank_results_csv\"][mer].replace(\"./\", input_dir+\"/\").replace('mbank', 'hmdb').replace('MassBank', 'HMDB')\n",
    "\n",
    "\n",
    "                    if os.path.exists(sirius_csv) and os.path.exists(gnps_csv) and os.path.exists(mbank_csv) and os.path.exists(hmdb_csv):\n",
    "\n",
    "\n",
    "                        sirius_df = pd.read_csv(sirius_csv)\n",
    "                        sirius_df = sirius_df.drop_duplicates('smiles')\n",
    "\n",
    "                        mbank_df = pd.read_csv(mbank_csv)\n",
    "                        mbank_df = mbank_df.drop_duplicates('MBSMILES')\n",
    "\n",
    "                        gnps_df = pd.read_csv(gnps_csv)\n",
    "                        gnps_df = gnps_df.drop_duplicates('GNPSSMILES')\n",
    "\n",
    "                        hmdb_df = pd.read_csv(hmdb_csv)\n",
    "                        hmdb_df = hmdb_df.drop_duplicates('HMDBSMILES')\n",
    "\n",
    "                        #1 SGHM\n",
    "                        if len(sirius_df) > 0 and len(gnps_df) > 0 and len(mbank_df) > 0 and len(hmdb_df) > 0:\n",
    "                            mbank_df[\"rank_ids\"] = [\"M_\" + str(s+1) for s in range(len(mbank_df))]\n",
    "\n",
    "                            gnps_df[\"rank_ids\"] = [\"G_\" + str(s+1) for s in range(len(gnps_df))]\n",
    "\n",
    "                            hmdb_df[\"rank_ids\"] = [\"H_\" + str(s+1) for s in range(len(hmdb_df))]\n",
    "\n",
    "                            sirius_df[\"rank_ids\"] = [\"S_\" + str(s) for s in sirius_df[\"rank\"]]\n",
    "                            sirius_df[\"Source\"] = \"SIRIUS\"\n",
    "\n",
    "\n",
    "                            source_l1 = [*(list(sirius_df[\"Source\"])), \n",
    "                               *(list(gnps_df[\"Source\"]))\n",
    "                               ,*(list(mbank_df[\"Source\"])),\n",
    "                                        *(list(hmdb_df[\"Source\"]))]\n",
    "\n",
    "                            rank_l2 = [*(list(sirius_df[\"rank_ids\"])), \n",
    "                               *(list(gnps_df[\"rank_ids\"]))\n",
    "                               ,*(list(mbank_df[\"rank_ids\"])), \n",
    "                                      *(list(hmdb_df[\"rank_ids\"]))]\n",
    "\n",
    "                            smiles_l3 = [*(list(sirius_df[\"smiles\"])), \n",
    "                               *(list(gnps_df[\"GNPSSMILES\"]))\n",
    "                               ,*(list(mbank_df[\"MBSMILES\"])),\n",
    "                                        *(list(hmdb_df[\"HMDBSMILES\"]))]\n",
    "\n",
    "                            sm = pd.DataFrame(list(zip(source_l1, rank_l2, smiles_l3)), columns = [\"Source\", \"ranks\", \"SMILES\"])   \n",
    "                            \n",
    "                            df_edge = chemMN_CandidateSelection(sm)\n",
    "\n",
    "                            df_edge.to_csv(input_dir + \"/\" + entry + \"/\" + str(merged_df[\"premz\"][mer]) + \"_ChemMNedges.tsv\", sep='\\t')\n",
    "                            one_candidate = one_candidate_selection(sm, Source = \"SGHM\")\n",
    "                            one_candidate.to_csv(input_dir + \"/\" + entry + \"/\" + str(merged_df[\"premz\"][mer]) + \"_one_candidate_list.tsv\", sep='\\t')\n",
    "                            \n",
    "                            \n",
    "                            df_sources = pd.DataFrame({\"SIRIUS\": one_candidate[\"SIRIUS\"], \n",
    "                                                       \"GNPS\": one_candidate[\"GNPS\"],\n",
    "                                                       \"HMDB\": one_candidate[\"HMDB\"],\n",
    "                                                       \"MassBank\": one_candidate[\"MassBank\"]})\n",
    "                            \n",
    "                            \n",
    "                            # now check which rows have a value\n",
    "\n",
    "                            index_SIRIUS = [x for x, row in df_sources.iterrows() if not isNaN(df_sources[\"SIRIUS\"][x])]\n",
    "                            index_GNPS = [x for x, row in df_sources.iterrows() if not isNaN(df_sources[\"GNPS\"][x])]\n",
    "                            index_MassBank = [x for x, row in df_sources.iterrows() if not isNaN(df_sources[\"MassBank\"][x])]\n",
    "                            index_HMDB = [x for x, row in df_sources.iterrows() if not isNaN(df_sources[\"HMDB\"][x])]\n",
    "\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            \n",
    "\n",
    "                        #2 SGM\n",
    "                        elif  len(sirius_df) > 0 and len(gnps_df) > 0 and len(mbank_df) > 0 and len(hmdb_df) == 0:\n",
    "\n",
    "                            mbank_df[\"rank_ids\"] = [\"M_\" + str(s+1) for s in range(len(mbank_df))]\n",
    "\n",
    "                            gnps_df[\"rank_ids\"] = [\"G_\" + str(s+1) for s in range(len(gnps_df))]\n",
    "\n",
    "                            sirius_df[\"rank_ids\"] = [\"S_\" + str(s) for s in sirius_df[\"rank\"]]\n",
    "                            sirius_df[\"Source\"] = \"SIRIUS\"\n",
    "\n",
    "\n",
    "                            source_l1 = [*(list(sirius_df[\"Source\"])), \n",
    "                               *(list(gnps_df[\"Source\"]))\n",
    "                               ,*(list(mbank_df[\"Source\"]))]\n",
    "\n",
    "                            rank_l2 = [*(list(sirius_df[\"rank_ids\"])), \n",
    "                               *(list(gnps_df[\"rank_ids\"]))\n",
    "                               ,*(list(mbank_df[\"rank_ids\"]))]\n",
    "\n",
    "                            smiles_l3 = [*(list(sirius_df[\"smiles\"])), \n",
    "                               *(list(gnps_df[\"GNPSSMILES\"]))\n",
    "                               ,*(list(mbank_df[\"MBSMILES\"]))]\n",
    "\n",
    "                            sm = pd.DataFrame(list(zip(source_l1, rank_l2, smiles_l3)), columns = [\"Source\", \"ranks\", \"SMILES\"])\n",
    "\n",
    "\n",
    "                            df_edge = chemMN_CandidateSelection(sm)\n",
    "\n",
    "                            df_edge.to_csv(input_dir + \"/\" + entry + \"/\" + str(merged_df[\"premz\"][mer]) + \"_ChemMNedges.tsv\", sep='\\t')\n",
    "                            one_candidate = one_candidate_selection(sm, Source = \"SGM\")\n",
    "                            one_candidate.to_csv(input_dir + \"/\" + entry + \"/\" + str(merged_df[\"premz\"][mer]) + \"_one_candidate_list.tsv\", sep='\\t')\n",
    "\n",
    "                        #3 SHM\n",
    "                        elif  len(sirius_df) > 0 and len(gnps_df) == 0 and len(mbank_df) > 0 and len(hmdb_df) > 0:\n",
    "\n",
    "                            mbank_df[\"rank_ids\"] = [\"M_\" + str(s+1) for s in range(len(mbank_df))]\n",
    "\n",
    "                            #gnps_df[\"rank_ids\"] = [\"G_\" + str(s+1) for s in range(len(gnps_df))]\n",
    "\n",
    "                            hmdb_df[\"rank_ids\"] = [\"H_\" + str(s+1) for s in range(len(hmdb_df))]\n",
    "\n",
    "                            sirius_df[\"rank_ids\"] = [\"S_\" + str(s) for s in sirius_df[\"rank\"]]\n",
    "                            sirius_df[\"Source\"] = \"SIRIUS\"\n",
    "\n",
    "\n",
    "                            source_l1 = [*(list(sirius_df[\"Source\"]))\n",
    "                               ,*(list(mbank_df[\"Source\"])),\n",
    "                                        *(list(hmdb_df[\"Source\"]))]\n",
    "\n",
    "                            rank_l2 = [*(list(sirius_df[\"rank_ids\"]))\n",
    "                               ,*(list(mbank_df[\"rank_ids\"])), \n",
    "                                      *(list(hmdb_df[\"rank_ids\"]))]\n",
    "\n",
    "                            smiles_l3 = [*(list(sirius_df[\"smiles\"]))\n",
    "                               ,*(list(mbank_df[\"MBSMILES\"])),\n",
    "                                        *(list(hmdb_df[\"HMDBSMILES\"]))]\n",
    "\n",
    "                            sm = pd.DataFrame(list(zip(source_l1, rank_l2, smiles_l3)), columns = [\"Source\", \"ranks\", \"SMILES\"])\n",
    "\n",
    "\n",
    "                            df_edge = chemMN_CandidateSelection(sm)\n",
    "\n",
    "                            df_edge.to_csv(input_dir + \"/\" + entry + \"/\" + str(merged_df[\"premz\"][mer]) + \"_ChemMNedges.tsv\", sep='\\t')\n",
    "                            one_candidate = one_candidate_selection(sm, Source = \"SHM\")\n",
    "                            one_candidate.to_csv(input_dir + \"/\" + entry + \"/\" + str(merged_df[\"premz\"][mer]) + \"_one_candidate_list.tsv\", sep='\\t')\n",
    "\n",
    "                        #4 SGH\n",
    "                        elif  len(sirius_df) > 0 and len(gnps_df) > 0 and len(mbank_df) == 0 and len(hmdb_df) > 0:\n",
    "                            #mbank_df[\"rank_ids\"] = [\"M_\" + str(s+1) for s in range(len(mbank_df))]\n",
    "\n",
    "                            gnps_df[\"rank_ids\"] = [\"G_\" + str(s+1) for s in range(len(gnps_df))]\n",
    "\n",
    "                            hmdb_df[\"rank_ids\"] = [\"H_\" + str(s+1) for s in range(len(hmdb_df))]\n",
    "\n",
    "                            sirius_df[\"rank_ids\"] = [\"S_\" + str(s) for s in sirius_df[\"rank\"]]\n",
    "                            sirius_df[\"Source\"] = \"SIRIUS\"\n",
    "\n",
    "\n",
    "                            source_l1 = [*(list(sirius_df[\"Source\"]))\n",
    "                               ,*(list(gnps_df[\"Source\"])),\n",
    "                                        *(list(hmdb_df[\"Source\"]))]\n",
    "\n",
    "                            rank_l2 = [*(list(sirius_df[\"rank_ids\"]))\n",
    "                               ,*(list(gnps_df[\"rank_ids\"])), \n",
    "                                      *(list(hmdb_df[\"rank_ids\"]))]\n",
    "\n",
    "                            smiles_l3 = [*(list(sirius_df[\"smiles\"]))\n",
    "                               ,*(list(gnps_df[\"GNPSSMILES\"])),\n",
    "                                        *(list(hmdb_df[\"HMDBSMILES\"]))]\n",
    "\n",
    "                            sm = pd.DataFrame(list(zip(source_l1, rank_l2, smiles_l3)), columns = [\"Source\", \"ranks\", \"SMILES\"])\n",
    "\n",
    "                            df_edge = chemMN_CandidateSelection(sm)\n",
    "\n",
    "                            df_edge.to_csv(input_dir + \"/\" + entry + \"/\" + str(merged_df[\"premz\"][mer]) + \"_ChemMNedges.tsv\", sep='\\t')\n",
    "                            one_candidate = one_candidate_selection(sm, Source = \"SGH\")\n",
    "                            one_candidate.to_csv(input_dir + \"/\" + entry + \"/\" + str(merged_df[\"premz\"][mer]) + \"_one_candidate_list.tsv\", sep='\\t')\n",
    "                            \n",
    "                        #5 GHM\n",
    "                        elif  len(sirius_df) == 0 and len(gnps_df) > 0 and len(mbank_df) > 0 and len(hmdb_df) > 0:\n",
    "                            mbank_df[\"rank_ids\"] = [\"M_\" + str(s+1) for s in range(len(mbank_df))]\n",
    "\n",
    "                            gnps_df[\"rank_ids\"] = [\"G_\" + str(s+1) for s in range(len(gnps_df))]\n",
    "\n",
    "                            hmdb_df[\"rank_ids\"] = [\"H_\" + str(s+1) for s in range(len(hmdb_df))]\n",
    "\n",
    "                            #sirius_df[\"rank_ids\"] = [\"S_\" + str(s) for s in sirius_df[\"rank\"]]\n",
    "                            #sirius_df[\"Source\"] = \"SIRIUS\"\n",
    "\n",
    "\n",
    "                            source_l1 = [*(list(gnps_df[\"Source\"]))\n",
    "                               ,*(list(mbank_df[\"Source\"])),\n",
    "                                        *(list(hmdb_df[\"Source\"]))]\n",
    "\n",
    "                            rank_l2 = [*(list(gnps_df[\"rank_ids\"]))\n",
    "                               ,*(list(mbank_df[\"rank_ids\"])), \n",
    "                                      *(list(hmdb_df[\"rank_ids\"]))]\n",
    "\n",
    "                            smiles_l3 = [*(list(gnps_df[\"GNPSSMILES\"]))\n",
    "                               ,*(list(mbank_df[\"MBSMILES\"])),\n",
    "                                        *(list(hmdb_df[\"HMDBSMILES\"]))]\n",
    "\n",
    "                            sm = pd.DataFrame(list(zip(source_l1, rank_l2, smiles_l3)), columns = [\"Source\", \"ranks\", \"SMILES\"])\n",
    "\n",
    "                            df_edge = chemMN_CandidateSelection(sm)\n",
    "\n",
    "                            df_edge.to_csv(input_dir + \"/\" + entry + \"/\" + str(merged_df[\"premz\"][mer]) + \"_ChemMNedges.tsv\", sep='\\t')\n",
    "                            one_candidate = one_candidate_selection(sm, Source = \"GHM\")\n",
    "                            one_candidate.to_csv(input_dir + \"/\" + entry + \"/\" + str(merged_df[\"premz\"][mer]) + \"_one_candidate_list.tsv\", sep='\\t')\n",
    "                            \n",
    "                        #6 SG\n",
    "                        elif  len(sirius_df) > 0 and len(gnps_df) > 0 and len(mbank_df) == 0 and len(hmdb_df) == 0:\n",
    "                            #mbank_df[\"rank_ids\"] = [\"M_\" + str(s+1) for s in range(len(mbank_df))]\n",
    "\n",
    "                            gnps_df[\"rank_ids\"] = [\"G_\" + str(s+1) for s in range(len(gnps_df))]\n",
    "\n",
    "                            sirius_df[\"rank_ids\"] = [\"S_\" + str(s) for s in sirius_df[\"rank\"]]\n",
    "                            sirius_df[\"Source\"] = \"SIRIUS\"\n",
    "\n",
    "\n",
    "                            source_l1 = [*(list(sirius_df[\"Source\"])), \n",
    "                               *(list(gnps_df[\"Source\"]))]\n",
    "                               #,*(list(mbank_df[\"Source\"]))]\n",
    "\n",
    "                            rank_l2 = [*(list(sirius_df[\"rank_ids\"])), \n",
    "                               *(list(gnps_df[\"rank_ids\"]))]\n",
    "                               #,*(list(mbank_df[\"rank_ids\"]))]\n",
    "\n",
    "                            smiles_l3 = [*(list(sirius_df[\"smiles\"])), \n",
    "                               *(list(gnps_df[\"GNPSSMILES\"]))]\n",
    "                               #,*(list(mbank_df[\"MBSMILES\"]))]\n",
    "\n",
    "                            sm = pd.DataFrame(list(zip(source_l1, rank_l2, smiles_l3)), columns = [\"Source\", \"ranks\", \"SMILES\"])\n",
    "\n",
    "\n",
    "                            df_edge = chemMN_CandidateSelection(sm)\n",
    "\n",
    "                            df_edge.to_csv(input_dir + \"/\" + entry + \"/\" + str(merged_df[\"premz\"][mer]) + \"_ChemMNedges.tsv\", sep='\\t')\n",
    "                            \n",
    "                            one_candidate = one_candidate_selection(sm, Source = \"SG\")\n",
    "                            one_candidate.to_csv(input_dir + \"/\" + entry + \"/\" + str(merged_df[\"premz\"][mer]) + \"_one_candidate_list.tsv\", sep='\\t')\n",
    "\n",
    "                        #7 SH\n",
    "                        elif  len(sirius_df) > 0 and len(gnps_df) == 0 and len(mbank_df) == 0 and len(hmdb_df) > 0:\n",
    "                            #mbank_df[\"rank_ids\"] = [\"M_\" + str(s+1) for s in range(len(mbank_df))]\n",
    "\n",
    "                            #gnps_df[\"rank_ids\"] = [\"G_\" + str(s+1) for s in range(len(gnps_df))]\n",
    "\n",
    "                            hmdb_df[\"rank_ids\"] = [\"H_\" + str(s+1) for s in range(len(hmdb_df))]\n",
    "\n",
    "                            sirius_df[\"rank_ids\"] = [\"S_\" + str(s) for s in sirius_df[\"rank\"]]\n",
    "                            sirius_df[\"Source\"] = \"SIRIUS\"\n",
    "\n",
    "\n",
    "                            source_l1 = [*(list(sirius_df[\"Source\"])),\n",
    "                                        *(list(hmdb_df[\"Source\"]))]\n",
    "\n",
    "                            rank_l2 = [*(list(sirius_df[\"rank_ids\"])), \n",
    "                                      *(list(hmdb_df[\"rank_ids\"]))]\n",
    "\n",
    "                            smiles_l3 = [*(list(sirius_df[\"smiles\"])),\n",
    "                                        *(list(hmdb_df[\"HMDBSMILES\"]))]\n",
    "\n",
    "                            sm = pd.DataFrame(list(zip(source_l1, rank_l2, smiles_l3)), columns = [\"Source\", \"ranks\", \"SMILES\"])\n",
    "                            df_edge = chemMN_CandidateSelection(sm)\n",
    "\n",
    "                            df_edge.to_csv(input_dir + \"/\" + entry + \"/\" + str(merged_df[\"premz\"][mer]) + \"_ChemMNedges.tsv\", sep='\\t')\n",
    "                            \n",
    "                            one_candidate = one_candidate_selection(sm, Source = \"SH\")\n",
    "                            one_candidate.to_csv(input_dir + \"/\" + entry + \"/\" + str(merged_df[\"premz\"][mer]) + \"_one_candidate_list.tsv\", sep='\\t')\n",
    "                        #8 SM\n",
    "                        elif  len(sirius_df) > 0 and len(gnps_df) == 0 and len(mbank_df) > 0 and len(hmdb_df) == 0:\n",
    "                            mbank_df[\"rank_ids\"] = [\"M_\" + str(s+1) for s in range(len(mbank_df))]\n",
    "\n",
    "                            #gnps_df[\"rank_ids\"] = [\"G_\" + str(s+1) for s in range(len(gnps_df))]\n",
    "\n",
    "                            #hmdb_df[\"rank_ids\"] = [\"H_\" + str(s+1) for s in range(len(hmdb_df))]\n",
    "\n",
    "                            sirius_df[\"rank_ids\"] = [\"S_\" + str(s) for s in sirius_df[\"rank\"]]\n",
    "                            sirius_df[\"Source\"] = \"SIRIUS\"\n",
    "\n",
    "\n",
    "                            source_l1 = [*(list(sirius_df[\"Source\"]))\n",
    "                               ,*(list(mbank_df[\"Source\"]))]\n",
    "\n",
    "                            rank_l2 = [*(list(sirius_df[\"rank_ids\"]))\n",
    "                               ,*(list(mbank_df[\"rank_ids\"]))]\n",
    "\n",
    "                            smiles_l3 = [*(list(sirius_df[\"smiles\"]))\n",
    "                               ,*(list(mbank_df[\"MBSMILES\"]))]\n",
    "\n",
    "                            sm = pd.DataFrame(list(zip(source_l1, rank_l2, smiles_l3)), columns = [\"Source\", \"ranks\", \"SMILES\"])\n",
    "\n",
    "\n",
    "                            df_edge = chemMN_CandidateSelection(sm)\n",
    "\n",
    "                            df_edge.to_csv(input_dir + \"/\" + entry + \"/\" + str(merged_df[\"premz\"][mer]) + \"_ChemMNedges.tsv\", sep='\\t')\n",
    "                            \n",
    "                            one_candidate = one_candidate_selection(sm, Source = \"SM\")\n",
    "                            one_candidate.to_csv(input_dir + \"/\" + entry + \"/\" + str(merged_df[\"premz\"][mer]) + \"_one_candidate_list.tsv\", sep='\\t')\n",
    "                        #9 GM\n",
    "                        elif  len(sirius_df) == 0 and len(gnps_df) > 0 and len(mbank_df) > 0 and len(hmdb_df) == 0:\n",
    "                            mbank_df[\"rank_ids\"] = [\"M_\" + str(s+1) for s in range(len(mbank_df))]\n",
    "\n",
    "                            gnps_df[\"rank_ids\"] = [\"G_\" + str(s+1) for s in range(len(gnps_df))]\n",
    "\n",
    "                            #hmdb_df[\"rank_ids\"] = [\"H_\" + str(s+1) for s in range(len(hmdb_df))]\n",
    "\n",
    "                            #sirius_df[\"rank_ids\"] = [\"S_\" + str(s) for s in sirius_df[\"rank\"]]\n",
    "                            #sirius_df[\"Source\"] = \"SIRIUS\"\n",
    "\n",
    "\n",
    "                            source_l1 = [*(list(mbank_df[\"Source\"])),\n",
    "                                        *(list(gnps_df[\"Source\"]))]\n",
    "\n",
    "                            rank_l2 = [*(list(mbank_df[\"rank_ids\"])), \n",
    "                                      *(list(gnps_df[\"rank_ids\"]))]\n",
    "\n",
    "                            smiles_l3 = [*(list(mbank_df[\"MBSMILES\"])),\n",
    "                                        *(list(gnps_df[\"GNPSSMILES\"]))]\n",
    "\n",
    "                            sm = pd.DataFrame(list(zip(source_l1, rank_l2, smiles_l3)), columns = [\"Source\", \"ranks\", \"SMILES\"])\n",
    "\n",
    "\n",
    "                            df_edge = chemMN_CandidateSelection(sm)\n",
    "\n",
    "                            df_edge.to_csv(input_dir + \"/\" + entry + \"/\" + str(merged_df[\"premz\"][mer]) + \"_ChemMNedges.tsv\", sep='\\t')\n",
    "                            \n",
    "                            one_candidate = one_candidate_selection(sm, Source = \"GM\")\n",
    "                            one_candidate.to_csv(input_dir + \"/\" + entry + \"/\" + str(merged_df[\"premz\"][mer]) + \"_one_candidate_list.tsv\", sep='\\t')\n",
    "                        #10 GH\n",
    "                        elif  len(sirius_df) == 0 and len(gnps_df) > 0 and len(mbank_df) == 0 and len(hmdb_df) > 0:\n",
    "                            #mbank_df[\"rank_ids\"] = [\"M_\" + str(s+1) for s in range(len(mbank_df))]\n",
    "\n",
    "                            gnps_df[\"rank_ids\"] = [\"G_\" + str(s+1) for s in range(len(gnps_df))]\n",
    "\n",
    "                            hmdb_df[\"rank_ids\"] = [\"H_\" + str(s+1) for s in range(len(hmdb_df))]\n",
    "\n",
    "                            #sirius_df[\"rank_ids\"] = [\"S_\" + str(s) for s in sirius_df[\"rank\"]]\n",
    "                            #sirius_df[\"Source\"] = \"SIRIUS\"\n",
    "\n",
    "\n",
    "                            source_l1 = [*(list(gnps_df[\"Source\"])),\n",
    "                                        *(list(hmdb_df[\"Source\"]))]\n",
    "\n",
    "                            rank_l2 = [*(list(gnps_df[\"rank_ids\"])),\n",
    "                                      *(list(hmdb_df[\"rank_ids\"]))]\n",
    "\n",
    "                            smiles_l3 = [*(list(gnps_df[\"GNPSSMILES\"])),\n",
    "                                        *(list(hmdb_df[\"HMDBSMILES\"]))]\n",
    "\n",
    "                            sm = pd.DataFrame(list(zip(source_l1, rank_l2, smiles_l3)), columns = [\"Source\", \"ranks\", \"SMILES\"])\n",
    "                            df_edge = chemMN_CandidateSelection(sm)\n",
    "\n",
    "\n",
    "\n",
    "                            df_edge.to_csv(input_dir + \"/\" + entry + \"/\" + str(merged_df[\"premz\"][mer]) + \"_ChemMNedges.tsv\", sep='\\t')\n",
    "                            one_candidate = one_candidate_selection(sm, Source = \"GH\")\n",
    "                            one_candidate.to_csv(input_dir + \"/\" + entry + \"/\" + str(merged_df[\"premz\"][mer]) + \"_one_candidate_list.tsv\", sep='\\t')\n",
    "                        #11 HM\n",
    "                        elif  len(sirius_df) == 0 and len(gnps_df) == 0 and len(mbank_df) > 0 and len(hmdb_df) > 0:\n",
    "                            mbank_df[\"rank_ids\"] = [\"M_\" + str(s+1) for s in range(len(mbank_df))]\n",
    "\n",
    "                            #gnps_df[\"rank_ids\"] = [\"G_\" + str(s+1) for s in range(len(gnps_df))]\n",
    "\n",
    "                            hmdb_df[\"rank_ids\"] = [\"H_\" + str(s+1) for s in range(len(hmdb_df))]\n",
    "\n",
    "                            #sirius_df[\"rank_ids\"] = [\"S_\" + str(s) for s in sirius_df[\"rank\"]]\n",
    "                            #sirius_df[\"Source\"] = \"SIRIUS\"\n",
    "\n",
    "\n",
    "                            source_l1 = [*(list(mbank_df[\"Source\"])),\n",
    "                                        *(list(hmdb_df[\"Source\"]))]\n",
    "\n",
    "                            rank_l2 = [*(list(mbank_df[\"rank_ids\"])), \n",
    "                                      *(list(hmdb_df[\"rank_ids\"]))]\n",
    "\n",
    "                            smiles_l3 = [*(list(mbank_df[\"MBSMILES\"])),\n",
    "                                        *(list(hmdb_df[\"HMDBSMILES\"]))]\n",
    "\n",
    "                            sm = pd.DataFrame(list(zip(source_l1, rank_l2, smiles_l3)), columns = [\"Source\", \"ranks\", \"SMILES\"])\n",
    "\n",
    "\n",
    "                            df_edge = chemMN_CandidateSelection(sm)\n",
    "\n",
    "                            df_edge.to_csv(input_dir + \"/\" + entry + \"/\" + str(merged_df[\"premz\"][mer]) + \"_ChemMNedges.tsv\", sep='\\t')\n",
    "                            one_candidate = one_candidate_selection(sm, Source = \"HM\")\n",
    "                            one_candidate.to_csv(input_dir + \"/\" + entry + \"/\" + str(merged_df[\"premz\"][mer]) + \"_one_candidate_list.tsv\", sep='\\t')\n",
    "\n",
    "                        # S\n",
    "                        #elif  len(sirius_df) > 0 and len(gnps_df) == 0 and len(mbank_df) == 0 and len(hmdb_df) == 0:\n",
    "\n",
    "                        # G\n",
    "                        #elif  len(sirius_df) == 0 and len(gnps_df) > 0 and len(mbank_df) == 0 and len(hmdb_df) == 0:\n",
    "\n",
    "                        # M\n",
    "                        #elif  len(sirius_df) == 0 and len(gnps_df) == 0 and len(mbank_df) > 0 and len(hmdb_df) == 0:\n",
    "\n",
    "                        # H\n",
    "                        #elif  len(sirius_df) == 0 and len(gnps_df) == 0 and len(mbank_df) == 0 and len(hmdb_df) > 0:\n",
    "                merged_df.to_csv(\"mergedResults_withCandidates.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "759e737b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read csv\n",
    "one_can = pd.read_csv(\"/Users/mahnoorzulfiqar/OneDriveUNI/MAW-data/StandardSMarinoi_Data/VN_211016_acetyl_carnitine/204.122756958008_one_candidate_list.tsv\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "077371e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete count column which is wrong\n",
    "del one_can[\"Count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3bf50da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_count_column(df_one_candidate):\n",
    "    # create new df only with the Sources column\n",
    "    df = pd.DataFrame({\"SIRIUS\": one_can[\"SIRIUS\"], \n",
    "                   \"GNPS\": one_can[\"GNPS\"],\n",
    "                   \"MassBank\": one_can[\"MassBank\"], \n",
    "                   \"HMDB\": one_can[\"HMDB\"]})\n",
    "    \n",
    "    # now check which rows have a value\n",
    "\n",
    "    index_SIRIUS = [x for x, row in df.iterrows() if not isNaN(df[\"SIRIUS\"][x])]\n",
    "    index_GNPS = [x for x, row in df.iterrows() if not isNaN(df[\"GNPS\"][x])]\n",
    "    index_MassBank = [x for x, row in df.iterrows() if not isNaN(df[\"MassBank\"][x])]\n",
    "    index_HMDB = [x for x, row in df.iterrows() if not isNaN(df[\"HMDB\"][x])]\n",
    "    \n",
    "    # make a list of the rows\n",
    "    list_of_indices = index_SIRIUS + index_GNPS + index_MassBank + index_HMDB\n",
    "\n",
    "    # count how mnay times one of the rows is appearing and add count\n",
    "    count_list = [[x, list_of_indices.count(x)] for x in set(list_of_indices)]\n",
    "    # add this info to one_can\n",
    "    one_can[\"Count\"] = [count_list[x][1] for x in range(len(count_list))]\n",
    "    # sort the list by count in descending order\n",
    "    sorted_count_one_candidate = df_one_candidate.sort_values(by = \"Count\", ascending = False)\n",
    "    return(sorted_count_one_candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2bb8b2a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Source</th>\n",
       "      <th>ranks</th>\n",
       "      <th>SMILES</th>\n",
       "      <th>SIRIUS</th>\n",
       "      <th>GNPS</th>\n",
       "      <th>MassBank</th>\n",
       "      <th>HMDB</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>SIRIUS</td>\n",
       "      <td>S_1</td>\n",
       "      <td>CC(=O)OC(CC(=O)O)C[N+](C)(C)C</td>\n",
       "      <td>S_1</td>\n",
       "      <td>G_3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>GNPS</td>\n",
       "      <td>G_1</td>\n",
       "      <td>CC(=O)O[C@H](CC(=O)[O-])C[N+](C)(C)C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G_1, G_2, G_4</td>\n",
       "      <td>M_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>GNPS</td>\n",
       "      <td>G_2</td>\n",
       "      <td>CC(=O)OC(CC([O-])=O)C[N+](C)(C)C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G_1, G_2, G_4</td>\n",
       "      <td>M_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>GNPS</td>\n",
       "      <td>G_3</td>\n",
       "      <td>CC(=O)O[C@H](CC(O)=O)C[N+](C)(C)C</td>\n",
       "      <td>S_1</td>\n",
       "      <td>G_3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>GNPS</td>\n",
       "      <td>G_4</td>\n",
       "      <td>CC(=O)OC(CC(=O)[O-])C[N+](C)(C)C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G_1, G_2, G_4</td>\n",
       "      <td>M_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>MassBank</td>\n",
       "      <td>M_1</td>\n",
       "      <td>CC(=O)O[C@@H](CC(=O)[O-])C[N+](C)(C)C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G_1, G_2, G_4</td>\n",
       "      <td>M_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>SIRIUS</td>\n",
       "      <td>S_2</td>\n",
       "      <td>CCC(=O)OC(CC(=O)O)CN(C)C</td>\n",
       "      <td>S_2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>SIRIUS</td>\n",
       "      <td>S_3</td>\n",
       "      <td>CC(=O)OC(CC(=O)OC)CN(C)C</td>\n",
       "      <td>S_3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>SIRIUS</td>\n",
       "      <td>S_4</td>\n",
       "      <td>CC(=O)OC(C[N+](C)(C)C)OC(=O)C</td>\n",
       "      <td>S_4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>SIRIUS</td>\n",
       "      <td>S_5</td>\n",
       "      <td>CC(=O)OC(=O)CC(C[N+](C)(C)C)O</td>\n",
       "      <td>S_5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    Source ranks                                 SMILES SIRIUS  \\\n",
       "0           0    SIRIUS   S_1          CC(=O)OC(CC(=O)O)C[N+](C)(C)C    S_1   \n",
       "5           5      GNPS   G_1   CC(=O)O[C@H](CC(=O)[O-])C[N+](C)(C)C    NaN   \n",
       "6           6      GNPS   G_2       CC(=O)OC(CC([O-])=O)C[N+](C)(C)C    NaN   \n",
       "7           7      GNPS   G_3      CC(=O)O[C@H](CC(O)=O)C[N+](C)(C)C    S_1   \n",
       "8           8      GNPS   G_4       CC(=O)OC(CC(=O)[O-])C[N+](C)(C)C    NaN   \n",
       "9           9  MassBank   M_1  CC(=O)O[C@@H](CC(=O)[O-])C[N+](C)(C)C    NaN   \n",
       "1           1    SIRIUS   S_2               CCC(=O)OC(CC(=O)O)CN(C)C    S_2   \n",
       "2           2    SIRIUS   S_3               CC(=O)OC(CC(=O)OC)CN(C)C    S_3   \n",
       "3           3    SIRIUS   S_4          CC(=O)OC(C[N+](C)(C)C)OC(=O)C    S_4   \n",
       "4           4    SIRIUS   S_5          CC(=O)OC(=O)CC(C[N+](C)(C)C)O    S_5   \n",
       "\n",
       "            GNPS MassBank  HMDB  Count  \n",
       "0            G_3      NaN   NaN      2  \n",
       "5  G_1, G_2, G_4      M_1   NaN      2  \n",
       "6  G_1, G_2, G_4      M_1   NaN      2  \n",
       "7            G_3      NaN   NaN      2  \n",
       "8  G_1, G_2, G_4      M_1   NaN      2  \n",
       "9  G_1, G_2, G_4      M_1   NaN      2  \n",
       "1            NaN      NaN   NaN      1  \n",
       "2            NaN      NaN   NaN      1  \n",
       "3            NaN      NaN   NaN      1  \n",
       "4            NaN      NaN   NaN      1  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_df = add_count_column(df_one_candidate = one_can)\n",
    "count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b68d105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sources_4(df_count):\n",
    "    frame = [df_count[df_count[\"Source\"] == \"GNPS\"], df_count[df_count[\"Source\"] == \"MassBank\"], \n",
    "                 df_count[df_count[\"Source\"] == \"SIRIUS\"], df_count[df_count[\"Source\"] == \"HMDB\"]]\n",
    "    f = pd.concat(frame)\n",
    "    # extract rank number\n",
    "    f[\"rank_num\"] = [int(item) for sublist in list(f[\"ranks\"]) for item in sublist if item.isdigit()]\n",
    "    f[\"MSI-Level\"] = \"Level-2: Probable Structure\"\n",
    "    # sort according to rank number\n",
    "    f_ranked = f.sort_values(by = \"rank_num\")\n",
    "    return(f_ranked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ba9b1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sources_3(df_count, sources):\n",
    "    if \"HMDB\" not in sources:\n",
    "        frame = [df_count[df_count[\"Source\"] == \"GNPS\"], df_count[df_count[\"Source\"] == \"MassBank\"], \n",
    "                 df_count[df_count[\"Source\"] == \"SIRIUS\"]]\n",
    "        f = pd.concat(frame)\n",
    "        # extract rank number\n",
    "        f[\"rank_num\"] = [int(item) for sublist in list(f[\"ranks\"]) for item in sublist if item.isdigit()]\n",
    "        f[\"MSI-Level\"] = \"Level-2: Probable Structure\"\n",
    "        # sort according to rank number\n",
    "        f_ranked = f.sort_values(by = \"rank_num\")\n",
    "\n",
    "    elif \"SIRIUS\" not in sources:\n",
    "        frame = [df_count[df_count[\"Source\"] == \"GNPS\"], df_count[df_count[\"Source\"] == \"MassBank\"], \n",
    "                 df_count[df_count[\"Source\"] == \"HMDB\"]]\n",
    "        f = pd.concat(frame)\n",
    "        # extract rank number\n",
    "        f[\"rank_num\"] = [int(item) for sublist in list(f[\"ranks\"]) for item in sublist if item.isdigit()]\n",
    "        f[\"MSI-Level\"] = \"Level-2: Probable Structure\"\n",
    "        # sort according to rank number\n",
    "        f_ranked = f.sort_values(by = \"rank_num\")\n",
    "\n",
    "    elif \"MassBank\" not in sources:\n",
    "        frame = [df_count[df_count[\"Source\"] == \"GNPS\"], df_count[df_count[\"Source\"] == \"SIRIUS\"], \n",
    "                 df_count[df_count[\"Source\"] == \"HMDB\"]]\n",
    "        f = pd.concat(frame)\n",
    "        # extract rank number\n",
    "        f[\"rank_num\"] = [int(item) for sublist in list(f[\"ranks\"]) for item in sublist if item.isdigit()]\n",
    "        f[\"MSI-Level\"] = \"Level-2: Probable Structure\"\n",
    "        # sort according to rank number\n",
    "        f_ranked = f.sort_values(by = \"rank_num\")\n",
    "\n",
    "\n",
    "    elif \"GNPS\" not in sources:\n",
    "        frame = [df_count[df_count[\"Source\"] == \"MassBank\"], df_count[df_count[\"Source\"] == \"SIRIUS\"], \n",
    "                 df_count[df_count[\"Source\"] == \"HMDB\"]]\n",
    "        f = pd.concat(frame)\n",
    "        # extract rank number\n",
    "        f[\"rank_num\"] = [int(item) for sublist in list(f[\"ranks\"]) for item in sublist if item.isdigit()]\n",
    "        f[\"MSI-Level\"] = \"Level-2: Probable Structure\"\n",
    "        # sort according to rank number\n",
    "        f_ranked = f.sort_values(by = \"rank_num\")\n",
    "        \n",
    "    return(f_ranked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3944fdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sources_2(df_count, sources):\n",
    "    if \"SIRIUS\" not in sources and \"HMDB\" not in sources:\n",
    "        frame = [df_count[df_count[\"Source\"] == \"GNPS\"], df_count[df_count[\"Source\"] == \"MassBank\"]]\n",
    "        f = pd.concat(frame)\n",
    "        # extract rank number\n",
    "        f[\"rank_num\"] = [int(item) for sublist in list(f[\"ranks\"]) for item in sublist if item.isdigit()]\n",
    "        f[\"MSI-Level\"] = \"Level-2: Probable Structure\"\n",
    "        # sort according to rank number\n",
    "        f_ranked = f.sort_values(by = \"rank_num\")\n",
    "\n",
    "    elif \"MassBank\" not in sources_2 and \"HMDB\" not in sources_2:\n",
    "        frame2 = [df_count_2[df_count_2[\"Source\"] == \"GNPS\"], df_count_2[df_count_2[\"Source\"] == \"SIRIUS\"]]\n",
    "        f2 = pd.concat(frame2)\n",
    "        # extract rank number\n",
    "        f2[\"rank_num\"] = [int(item) for sublist in list(f2[\"ranks\"]) for item in sublist if item.isdigit()]\n",
    "        f2[\"MSI-Level\"] = \"Level-2: Probable Structure\"\n",
    "        # sort according to rank number\n",
    "        f2_ranked = f2.sort_values(by = \"rank_num\")\n",
    "\n",
    "    elif \"GNPS\" not in sources_2 and \"HMDB\" not in sources_2:\n",
    "        frame2 = [df_count_2[df_count_2[\"Source\"] == \"MassBank\"], df_count_2[df_count_2[\"Source\"] == \"SIRIUS\"]]\n",
    "        f2 = pd.concat(frame2)\n",
    "        # extract rank number\n",
    "        f2[\"rank_num\"] = [int(item) for sublist in list(f2[\"ranks\"]) for item in sublist if item.isdigit()]\n",
    "        f2[\"MSI-Level\"] = \"Level-2: Probable Structure\"\n",
    "        # sort according to rank number\n",
    "        f2_ranked = f2.sort_values(by = \"rank_num\")\n",
    "\n",
    "    elif \"MassBank\" not in sources_2 and \"SIRIUS\" not in sources_2:\n",
    "        frame2 = [df_count_2[df_count_2[\"Source\"] == \"GNPS\"], df_count_2[df_count_2[\"Source\"] == \"HMDB\"]]\n",
    "        f2 = pd.concat(frame2)\n",
    "        # extract rank number\n",
    "        f2[\"rank_num\"] = [int(item) for sublist in list(f2[\"ranks\"]) for item in sublist if item.isdigit()]\n",
    "        f2[\"MSI-Level\"] = \"Level-2: Probable Structure\"\n",
    "        # sort according to rank number\n",
    "        f2_ranked = f2.sort_values(by = \"rank_num\")\n",
    "\n",
    "    elif \"MassBank\" not in sources_2 and \"GNPS\" not in sources_2:\n",
    "        frame2 = [df_count_2[df_count_2[\"Source\"] == \"SIRIUS\"], df_count_2[df_count_2[\"Source\"] == \"HMDB\"]]\n",
    "        f2 = pd.concat(frame2)\n",
    "        # extract rank number\n",
    "        f2[\"rank_num\"] = [int(item) for sublist in list(f2[\"ranks\"]) for item in sublist if item.isdigit()]\n",
    "        f2[\"MSI-Level\"] = \"Level-2: Probable Structure\"\n",
    "        # sort according to rank number\n",
    "        f2_ranked = f2.sort_values(by = \"rank_num\")\n",
    "\n",
    "    elif \"GNPS\" not in sources_2 and \"SIRIUS\" not in sources_2:\n",
    "        frame2 = [df_count_2[df_count_2[\"Source\"] == \"MassBank\"], df_count_2[df_count_2[\"Source\"] == \"HMDB\"]]\n",
    "        f2 = pd.concat(frame2)\n",
    "        # extract rank number\n",
    "        f2[\"rank_num\"] = [int(item) for sublist in list(f2[\"ranks\"]) for item in sublist if item.isdigit()]\n",
    "        f2[\"MSI-Level\"] = \"Level-2: Probable Structure\"\n",
    "        # sort according to rank number\n",
    "        f2_ranked = f2.sort_values(by = \"rank_num\")\n",
    "        \n",
    "    return(f_ranked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d0ccebf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sources_1(df_count, sources):\n",
    "    if \"GNPS\" in sources:\n",
    "        frame = [df_count[df_count[\"Source\"] == pr_dbs[0]]]\n",
    "        \n",
    "        f = pd.concat(frame)\n",
    "        f[\"rank_num\"] = [int(item) for sublist in list(f[\"ranks\"]) for item in sublist if item.isdigit()]\n",
    "        f[\"MSI-Level\"] = \"Level-2: Probable Structure\"\n",
    "        f_ranked = f.sort_values(by = \"rank_num\") \n",
    "        \n",
    "        \n",
    "    elif \"MassBank\" in sources:\n",
    "        frame = [df_count[df_count[\"Source\"] == pr_dbs[1]]]\n",
    "        \n",
    "        f = pd.concat(frame)\n",
    "        f[\"rank_num\"] = [int(item) for sublist in list(f[\"ranks\"]) for item in sublist if item.isdigit()]\n",
    "        f[\"MSI-Level\"] = \"Level-3: Tentative Candidate\"\n",
    "        f_ranked = f.sort_values(by = \"rank_num\") \n",
    "        \n",
    "    elif \"SIRIUS\" in sources:\n",
    "        frame = [df_count[df_count[\"Source\"] == pr_dbs[2]]]\n",
    "        \n",
    "        f = pd.concat(frame)\n",
    "        f[\"rank_num\"] = [int(item) for sublist in list(f[\"ranks\"]) for item in sublist if item.isdigit()]\n",
    "        f[\"MSI-Level\"] = \"Level-3: Tentative Candidate\"\n",
    "        f_ranked = f.sort_values(by = \"rank_num\") \n",
    "        \n",
    "    elif \"HMDB\" in sources:\n",
    "        frame = [df_count[df_count[\"Source\"] == pr_dbs[3]]]\n",
    "        \n",
    "        f = pd.concat(frame)\n",
    "        f[\"rank_num\"] = [int(item) for sublist in list(f[\"ranks\"]) for item in sublist if item.isdigit()]\n",
    "        f[\"MSI-Level\"] = \"Level-3: Tentative Candidate\"\n",
    "        f_ranked = f.sort_values(by = \"rank_num\") \n",
    "        \n",
    "    return(f_ranked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e543417",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_no = list(np.unique(count_df[\"Count\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "042b9a15",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'if' statement on line 12 (2301084616.py, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [35]\u001b[0;36m\u001b[0m\n\u001b[0;31m    if len(sources_2) == 3:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'if' statement on line 12\n"
     ]
    }
   ],
   "source": [
    "# priority of DBs\n",
    "pr_dbs = [\"GNPS\", \"MassBank\", \"SIRIUS\", \"HMDB\"]\n",
    "\n",
    "# if there are only two counts/sources each time for a candidate\n",
    "if len(list_of_no) == 2:\n",
    "    \n",
    "    #take the first part of the count_df with 2 as count\n",
    "    df_count_2 = [count_df[count_df[\"Count\"] == x ] for x in list_of_no[::-1]][0]\n",
    "    \n",
    "    sources_2 = (list(np.unique(df_count_2[\"Source\"])))\n",
    "    \n",
    "    if len(sources_2) == 4:\n",
    "        \n",
    "        \n",
    "    if len(sources_2) == 3:\n",
    "        \n",
    "            \n",
    "    if len(sources_2) == 2:\n",
    "\n",
    "            \n",
    "            \n",
    "    df_count_1 = [count_df[count_df[\"Count\"] == x ] for x in list_of_no[::-1]][1]\n",
    "    \n",
    "    sources_1 = (list(np.unique(df_count_1[\"Source\"])))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "46d8012b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f2_ranked' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [36]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pd\u001b[38;5;241m.\u001b[39mconcat([\u001b[43mf2_ranked\u001b[49m, f1_ranked])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'f2_ranked' is not defined"
     ]
    }
   ],
   "source": [
    "pd.concat([f2_ranked, f1_ranked])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1eae5eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_dbs = [\"GNPS\", \"MassBank\", \"SIRIUS\", \"HMDB\"]\n",
    "if len(list_of_no) == 4:\n",
    "    df_count_4 = [count_df[count_df[\"Count\"] == x ] for x in list_of_no[::-1]][0]\n",
    "    frame = [high_2[high_2[\"Source\"] == pr_dbs[0]], high_2[high_2[\"Source\"] == pr_dbs[1]], \n",
    "             high_2[high_2[\"Source\"] == pr_dbs[2]], high_2[high_2[\"Source\"] == pr_dbs[3]]]\n",
    "\n",
    "    f = pd.concat(frame)\n",
    "    f[\"rank_num\"] = [int(item) for sublist in list(f[\"ranks\"]) for item in sublist if item.isdigit()]\n",
    "    f[\"MSI-Level\"] = \"Level-2: Probable Structure\"\n",
    "    f.sort_values(by = \"rank_num\")   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1cdcddf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_dbs = [\"GNPS\", \"MassBank\", \"SIRIUS\", \"HMDB\"]\n",
    "if len(list_of_no) == 3:\n",
    "    df_count_3 = [count_df[count_df[\"Count\"] == x ] for x in list_of_no[::-1]][0]\n",
    "    \n",
    "    if \"HMDB\" not in (list(np.unique(df_count_3[\"Source\"]))):\n",
    "        \n",
    "        frame2 = [high_2[high_2[\"Source\"] == pr_dbs[0]], \n",
    "                 high_2[high_2[\"Source\"] == pr_dbs[1]], high_2[high_2[\"Source\"] == pr_dbs[2]]]\n",
    "        f2 = pd.concat(frame)\n",
    "        f2[\"rank_num\"] = [int(item) for sublist in list(f2[\"ranks\"]) for item in sublist if item.isdigit()]\n",
    "        f2[\"MSI-Level\"] = \"Level-2: Probable Structure\"\n",
    "        f2.sort_values(by = \"rank_num\")  \n",
    "        \n",
    "    if \"SIRIUS\" not in pr_dbs:\n",
    "        \n",
    "        frame = [high_2[high_2[\"Source\"] == pr_dbs[0]], \n",
    "                 high_2[high_2[\"Source\"] == pr_dbs[1]], high_2[high_2[\"Source\"] == pr_dbs[3]]]\n",
    "\n",
    "        f = pd.concat(frame)\n",
    "        f[\"rank_num\"] = [int(item) for sublist in list(f[\"ranks\"]) for item in sublist if item.isdigit()]\n",
    "        f[\"MSI-Level\"] = \"Level-2: Probable Structure\"\n",
    "        f.sort_values(by = \"rank_num\") \n",
    "        \n",
    "        \n",
    "    if \"MassBank\" not in pr_dbs:\n",
    "        \n",
    "        frame = [high_2[high_2[\"Source\"] == pr_dbs[0]], \n",
    "                 high_2[high_2[\"Source\"] == pr_dbs[2]], high_2[high_2[\"Source\"] == pr_dbs[3]]]\n",
    "\n",
    "        f = pd.concat(frame)\n",
    "        f[\"rank_num\"] = [int(item) for sublist in list(f[\"ranks\"]) for item in sublist if item.isdigit()]\n",
    "        f[\"MSI-Level\"] = \"Level-2: Probable Structure\"\n",
    "        f.sort_values(by = \"rank_num\")  \n",
    "    \n",
    "    if \"GNPS\" not in pr_dbs:\n",
    "        \n",
    "        frame = [high_2[high_2[\"Source\"] == pr_dbs[1]], \n",
    "                 high_2[high_2[\"Source\"] == pr_dbs[2]], high_2[high_2[\"Source\"] == pr_dbs[3]]]\n",
    "\n",
    "        f = pd.concat(frame)\n",
    "        f[\"rank_num\"] = [int(item) for sublist in list(f[\"ranks\"]) for item in sublist if item.isdigit()]\n",
    "        f[\"MSI-Level\"] = \"Level-2: Probable Structure\"\n",
    "        f.sort_values(by = \"rank_num\")   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6cb14433",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (922130600.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [39]\u001b[0;36m\u001b[0m\n\u001b[0;31m    if \"GNPS\" in :\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pr_dbs = [\"GNPS\", \"MassBank\", \"SIRIUS\", \"HMDB\"]\n",
    "if len(list_of_no) == 2:\n",
    "    df_count_2 = [count_df[count_df[\"Count\"] == x ] for x in list_of_no[::-1]][0]\n",
    "    \n",
    "    if \"GNPS\" in :\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "702f4e62",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1947469004.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [40]\u001b[0;36m\u001b[0m\n\u001b[0;31m    df_count_2 = [count_df[count_df[\"Count\"] == x ] for x in list_of_no[::-1]][2]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "    \n",
    "        df_count_3 = [count_df[count_df[\"Count\"] == x ] for x in list_of_no[::-1]][1]\n",
    "    df_count_2 = [count_df[count_df[\"Count\"] == x ] for x in list_of_no[::-1]][2]\n",
    "    df_count_1 = [count_df[count_df[\"Count\"] == x ] for x in list_of_no[::-1]][3]\n",
    "    \n",
    "    \n",
    "elif len(list_of_no) == 3:\n",
    "    df_count_3 = [count_df[count_df[\"Count\"] == x ] for x in list_of_no[::-1]][0]\n",
    "    df_count_2 = [count_df[count_df[\"Count\"] == x ] for x in list_of_no[::-1]][1]\n",
    "    df_count_1 = [count_df[count_df[\"Count\"] == x ] for x in list_of_no[::-1]][2]\n",
    "    \n",
    "elif len(list_of_no) == 2:\n",
    "    df_count_2 = [count_df[count_df[\"Count\"] == x ] for x in list_of_no[::-1]][0]\n",
    "    df_count_1 = [count_df[count_df[\"Count\"] == x ] for x in list_of_no[::-1]][1]\n",
    "    \n",
    "elif len(list_of_no) == 4:\n",
    "    df_count_1 = [count_df[count_df[\"Count\"] == x ] for x in list_of_no[::-1]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "81644738",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_count_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [41]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m((\u001b[38;5;28mlist\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(\u001b[43mdf_count_2\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSource\u001b[39m\u001b[38;5;124m\"\u001b[39m]))))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_count_2' is not defined"
     ]
    }
   ],
   "source": [
    "len((list(np.unique(df_count_2[\"Source\"]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7c64520b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'high_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [42]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m frame \u001b[38;5;241m=\u001b[39m [\u001b[43mhigh_2\u001b[49m[high_2[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSource\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m a], high_2[high_2[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSource\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m b], high_2[high_2[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSource\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m c]]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'high_2' is not defined"
     ]
    }
   ],
   "source": [
    "frame = [high_2[high_2[\"Source\"] == a], high_2[high_2[\"Source\"] == b], high_2[high_2[\"Source\"] == c]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0f6aed20",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'frame' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [43]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m f \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(\u001b[43mframe\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'frame' is not defined"
     ]
    }
   ],
   "source": [
    "f = pd.concat(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "86cba225",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [44]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m f[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank_num\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(item) \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[43mf\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mranks\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m sublist \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39misdigit()]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'f' is not defined"
     ]
    }
   ],
   "source": [
    "f[\"rank_num\"] = [int(item) for sublist in list(f[\"ranks\"]) for item in sublist if item.isdigit()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6fe22861",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [45]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mf\u001b[49m\u001b[38;5;241m.\u001b[39msort_values(by \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank_num\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'f' is not defined"
     ]
    }
   ],
   "source": [
    "f.sort_values(by = \"rank_num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc74c3fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mawRpy)",
   "language": "python",
   "name": "mawrpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
