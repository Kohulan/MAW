{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d79995f",
   "metadata": {},
   "source": [
    "## SIRIUS_Metfrag_SList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e20dd5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.2\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "611cc70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import pubchempy as pcp\n",
    "import numpy as np\n",
    "def isNaN(string):\n",
    "    return string != string\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "from pybatchclassyfire import *\n",
    "import csv \n",
    "import time\n",
    "import json\n",
    "from pandas import json_normalize\n",
    "import wget\n",
    "import string\n",
    "import urllib.parse\n",
    "import openpyxl\n",
    "import statistics\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42327aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:rdkit:Enabling RDKit 2021.09.4 jupyter extensions\n"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdFMCS\n",
    "from rdkit.Chem import PandasTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63568ef8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd8a7e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure your Smiles entries in the suspect list csv are in a column named \"SMILES\"\n",
    "def slist_metfrag(input_dir, slist_csv, name):\n",
    "    \"\"\"slist_metfrag is used to create a txt file that contains a list of \n",
    "    InChIKeys. This list is later used by MetFrag to use these compounds \n",
    "    as a Suspect List.\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored. For this \n",
    "    function this directory must contain a csv file that has a column \n",
    "    named \"SMILES\".\n",
    "    \n",
    "    slist_csv (str): This is the csv file that contains a column of \n",
    "    \"SMILES\". Additionally this file can contain other information \n",
    "    about the compounds, but for this function, column of \"SMILES\", \n",
    "    named as \"SMILES\" is necessary.\n",
    "\n",
    "    Returns:\n",
    "    list: list of InChIKeys\n",
    "    txt: a txt file of list of InChIKeys, is stored in input_dir\n",
    "    \n",
    "    Usage:\n",
    "    slist_metfrag(input_dir = \"/user/project/\", slist_csv = \n",
    "    \"suspectlist.csv\")\n",
    "    \n",
    "    \"\"\"\n",
    "    sl = pd.read_csv(slist_csv)\n",
    "    sl_mtfrag= []\n",
    "    for i, rows in sl.iterrows():\n",
    "        if i is not None:\n",
    "            mols = Chem.MolFromSmiles(sl['SMILES'][i])\n",
    "            try:\n",
    "                sl.loc[i, 'InChIKey'] = Chem.inchi.MolToInchiKey(mols)\n",
    "                sl_mtfrag.append(sl['InChIKey'][i])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    \n",
    "    with open((input_dir + \"/SL_\"+ name + '.txt'), 'w') as filehandle:\n",
    "        for listitem in sl_mtfrag:\n",
    "            filehandle.write('%s\\n' % listitem)\n",
    "    return(sl_mtfrag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09b81b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(slist_metfrag.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad461a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cd169d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slist_sirius(input_dir, slist_csv, substring = None):\n",
    "    \n",
    "    \"\"\"slist_sirius is used to create a tsv file that contains a list of \n",
    "    SMILES. The function also runs the sirius command custom db to create\n",
    "    fingerprints for each SMILES in a folder that we by default name as\n",
    "    SL_Frag/. This fingerprints folder is later used by SIRIUS to use \n",
    "    these compounds as a another small list of compounds to match against\n",
    "    the input spectra fingerprints.\n",
    "    Since SIRIUS doesn't take disconnected structure, Multiply charged, \n",
    "    Incorrect syntax, wild card(*) in smiles; this function removes all\n",
    "    such SMILES from the Suspect List.\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored. For this \n",
    "    function this directory must contain a csv file that has a column \n",
    "    named \"SMILES\".\n",
    "    \n",
    "    slist_csv (str): This is the csv file that contains a column of \n",
    "    \"SMILES\". Additionally this file can contain other information \n",
    "    about the compounds, but for this function, column of \"SMILES\", \n",
    "    named as \"SMILES\" is necessary.\n",
    "    \n",
    "    substring (list): provide a list of strings of SMILES that \n",
    "    shouldn't be considered, provide a list even if there is one string\n",
    "    that shouldnt be considered. e.g: \"[Fe+2]\". \n",
    "\n",
    "    Returns:\n",
    "    tsv: a tsv file of list of SMILES, named as SL_Sirius.tsv, is stored \n",
    "    in input_dir\n",
    "    directory: a directory with compound fragmentations will be created \n",
    "    in a folder named SL_Frag/ within the same input_dir\n",
    "    \n",
    "    \n",
    "    Usage:\n",
    "    slist_sirius(\"/user/project/\", \"suspectlist.csv\", \n",
    "    substring = None)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    sl = pd.read_csv(slist_csv)\n",
    "    \n",
    "    # define function to neutralize the charged SMILES\n",
    "    def neutralize_atoms(mol):\n",
    "        \n",
    "        pattern = Chem.MolFromSmarts(\"[+1!h0!$([*]~[-1,-2,-3,-4]),-1!$([*]~[+1,+2,+3,+4])]\")\n",
    "        at_matches = mol.GetSubstructMatches(pattern)\n",
    "        at_matches_list = [y[0] for y in at_matches]\n",
    "        if len(at_matches_list) > 0:\n",
    "            for at_idx in at_matches_list:\n",
    "                atom = mol.GetAtomWithIdx(at_idx)\n",
    "                chg = atom.GetFormalCharge()\n",
    "                hcount = atom.GetTotalNumHs()\n",
    "                atom.SetFormalCharge(0)\n",
    "                atom.SetNumExplicitHs(hcount - chg)\n",
    "                atom.UpdatePropertyCache()\n",
    "        return mol\n",
    "    \n",
    "\n",
    "    for i, row in sl.iterrows():\n",
    "        # remove SMILES with wild card\n",
    "        if \"*\" in sl[\"SMILES\"][i]:\n",
    "            sl = sl.drop(labels = i, axis = 0) \n",
    "    for i, row in sl.iterrows():\n",
    "        # remove SMILES with any string present in the substring\n",
    "        if substring:\n",
    "            if bool([ele for ele in substring if(ele in sl[\"SMILES\"][i])]):\n",
    "                sl = sl.drop(labels = i, axis = 0)\n",
    "    for i, row in sl.iterrows():\n",
    "        if \".\" in sl[\"SMILES\"][i]:\n",
    "            sl.loc[i, \"SMILES\"] = sl[\"SMILES\"][i].split('.')[0]\n",
    "    # Neutralize the charged SMILES\n",
    "    for i, row in sl.iterrows():\n",
    "        if \"+\" in sl[\"SMILES\"][i] or \"-\" in sl[\"SMILES\"][i]:\n",
    "            mol = Chem.MolFromSmiles(sl[\"SMILES\"][i])\n",
    "            neutralize_atoms(mol)\n",
    "            sl.loc[i, \"SMILES\"] = Chem.MolToSmiles(mol)\n",
    "            \n",
    "            # Remove multiple charged SMILES\n",
    "            if \"+\" in sl[\"SMILES\"][i] or \"-\" in sl[\"SMILES\"][i]:\n",
    "                pos = sl[\"SMILES\"][i].count('+')\n",
    "                neg = sl[\"SMILES\"][i].count('-')\n",
    "                charge = pos + neg \n",
    "                if charge > 1:\n",
    "                    sl = sl.drop(labels = i, axis = 0) \n",
    "                    \n",
    "    slsirius = pd.DataFrame({'smiles':sl[\"SMILES\"]})\n",
    "    slsirius.to_csv(input_dir+ \"SL_Sirius.tsv\", sep = \"\\t\", header = False, index = False)\n",
    "    os.system(\"sirius --input \" + input_dir + \"SL_Sirius.tsv custom-db --name=SL_Frag --output \"+ input_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ef42aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(slist_sirius.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc7a19e",
   "metadata": {},
   "source": [
    "### SIRIUS post processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e00a88",
   "metadata": {},
   "source": [
    "### SIRIUS Result Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e047124f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b65479b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sirius_postProc2.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecf65a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sirius_postProc2(input_dir, input_tablecsv):\n",
    "    \n",
    "    def isNaN(string):\n",
    "        return string != string\n",
    "    \"\"\"sirius_postProc2 is the second part of the function \n",
    "    sirius_postProc defined in R part of the workflow. This function\n",
    "    re-checks the Suspect list, if present or given as a parameter, \n",
    "    whether the candidates have a high similarity with compounds in\n",
    "    Suspect List. It also calculates the Maximum Common Substructure\n",
    "    (MCSS)\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored. For this \n",
    "    function this directory must contain a csv file that has a column \n",
    "    named \"SMILES\".\n",
    "    \n",
    "    input_tablecsv (str): This is the table in csv format (defined in R), \n",
    "    which stores a csv table containing columns \"mzml_files\", which \n",
    "    contains liat of all input files with their relative paths, second\n",
    "    column is \"ResultFileName\" which is a list of the corresponding\n",
    "    result relative directories to each mzml files. Lastly, \"file_id\", \n",
    "    contains a file directory. This table will be used to read the \n",
    "    SIRIUS json files\n",
    "    \n",
    "\n",
    "    Returns:\n",
    "    csv: a result file with additional columns such as those for suspect\n",
    "    list if one is used. It also adds columns on MCSS., named as \n",
    "    \"input_dir/ResultFileName/insilico/SiriusResults.csv\"\n",
    "    \n",
    "    \n",
    "    Usage:\n",
    "    sirius_postProc2(input_dir = \"/user/project/\", \n",
    "    input_table = \"/user/project/suspectlist.csv\")\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Describe the heavy atoms to be considered for MCSS\n",
    "    heavy_atoms = ['C', 'N', 'P', 'O', 'S']\n",
    "    \n",
    "    input_table = pd.read_csv(input_tablecsv)\n",
    "    \n",
    "    for m, row in input_table.iterrows():\n",
    "        \n",
    "        # Read the file result_dir/insilico/MS1DATAsirius.csv. \n",
    "        # This file has been produced in R workflow and contains \n",
    "        # SIRIUS results.\n",
    "\n",
    "        file1 = pd.read_csv(input_dir + (input_table['ResultFileNames'][m] + '/insilico/MS1DATAsirius.csv').replace(\"./\", \"\"))\n",
    "        \n",
    "        for i, row in file1.iterrows():\n",
    "            \n",
    "            # if the entry has SMILES extracted for MCSS calculation\n",
    "            if not isNaN(file1['SMILESforMCSS'][i]):\n",
    "                \n",
    "                # split the SMILES using |\n",
    "                top_smiles = file1['SMILESforMCSS'][i].split(\"|\")\n",
    "                \n",
    "                # if there are more than 1 smiles in the top smiles, \n",
    "                if len(top_smiles) > 1:\n",
    "                    mol = []\n",
    "                    for j in top_smiles:\n",
    "                        n = Chem.MolFromSmiles(j)\n",
    "                        mol.append(n)\n",
    "                    # list of mol used to calaculate the MCSS\n",
    "                    res = rdFMCS.FindMCS(mol)\n",
    "                    sm_res = Chem.MolToSmiles(Chem.MolFromSmarts(res.smartsString))\n",
    "                    # Check if the MCSS has one of the heavy atoms and whether they are\n",
    "                    # more than 3\n",
    "                    elem = [ele for ele in heavy_atoms if(ele in sm_res)]\n",
    "                    if elem and len(sm_res)>=3:\n",
    "                        file1.loc[i, 'MCSSstring'] = res.smartsString\n",
    "                        file1.loc[i, 'MCSS_SMILES'] = Chem.MolToSmiles(Chem.MolFromSmarts(res.smartsString))\n",
    "                        \n",
    "                        \n",
    "            if file1[\"FormulaRank\"][i] == 1.0:\n",
    "                sep = 'json/'\n",
    "                strpd = file1[\"dir\"][i].split(sep, 1)[0] +\"json/canopus_summary.tsv\"\n",
    "                if os.path.isfile(strpd):\n",
    "\n",
    "                    canopus = pd.read_csv(strpd, sep='\\t')\n",
    "                    if len(canopus) > 0:\n",
    "                        #file1.loc[i, 'most_specific_class'] = canopus[\"most specific class\"][0]\n",
    "                        #file1.loc[i, 'level _5'] = canopus[\"level 5\"][0]\n",
    "                        file1.loc[i, 'subclass'] = canopus[\"subclass\"][0]\n",
    "                        file1.loc[i, 'class'] = canopus[\"class\"][0]\n",
    "                        file1.loc[i, 'superclass'] = canopus[\"superclass\"][0]\n",
    "                        #file1.loc[i, 'all_classifications'] = canopus[\"all classifications\"][0]\n",
    "                        file1.loc[i, 'Classification_Source'] = 'CANOPUS'\n",
    "                    \n",
    "        \n",
    "        file1.to_csv(input_dir + (input_table['ResultFileNames'][m] + '/insilico/SiriusResults.csv').replace(\"./\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8451182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for one file\n",
    "#sirius_postProc2(input_dir= \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/\", \n",
    "#                 input_tablecsv = \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/input_table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d0fed58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for multiple file\n",
    "#sirius_postProc2(input_dir= \"/Users/mahnoorzulfiqar/Downloads/MAW-main/\", \n",
    "#                 input_tablecsv = \"/Users/mahnoorzulfiqar/Downloads/MAW-main/input_table.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653e9b79",
   "metadata": {},
   "source": [
    "### MetFrag Result Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a01c9220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metfrag_postproc(input_dir, input_tablecsv, sl= True):\n",
    "    \n",
    "    \n",
    "    \"\"\"metfrag_postproc function re-checks the Suspect list, if present \n",
    "    or given as a parameter, whether the candidates have a high \n",
    "    similarity with compounds in Suspect List. It also calculates the \n",
    "    Maximum Common Substructure (MCSS). This function adds top candidates\n",
    "    from PubChem and KEGG as these two databases are used with MetFrag\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored. For this \n",
    "    function this directory must contain a csv file that has a column \n",
    "    named \"SMILES\".\n",
    "    \n",
    "    input_tablecsv (str): This is the table in csv format (defined in R), \n",
    "    which stores a csv table containing columns \"mzml_files\", which \n",
    "    contains liat of all input files with their relative paths, second\n",
    "    column is \"ResultFileName\" which is a list of the corresponding\n",
    "    result relative directories to each mzml files. Lastly, \"file_id\", \n",
    "    contains a file directory. This table will be used to read the \n",
    "    MetFrag csv files\n",
    "\n",
    "    Returns:\n",
    "    csv: a result file with additional columns such as those for suspect\n",
    "    list if one is used. It also adds columns on MCSS., named as \n",
    "    \"input_dir/ResultFileName/insilico/MetFragResults.csv\". It \n",
    "    contains columns for KEGG and PubChem\n",
    "    \n",
    "    \n",
    "    Usage:\n",
    "    metfrag_postproc(input_dir = \"/user/project/\", \n",
    "    input_table = \"/user/project/suspectlist.csv\", sl = True, slistcsv)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Describe the heavy atoms to be considered for MCSS\n",
    "    heavy_atoms = ['C', 'N', 'P', 'O', 'S']\n",
    "    \n",
    "    input_table = pd.read_csv(input_tablecsv)\n",
    "    \n",
    "    for m, row in input_table.iterrows():\n",
    "        \n",
    "        # read SIRIUS results:\n",
    "        \n",
    "        #siriusResults = pd.read_csv(input_dir + (input_table['ResultFileNames'][m] + '/insilico/SiriusResults.csv'))\n",
    "    \n",
    "        # Result directory\n",
    "        result = input_dir + (input_table['ResultFileNames'][m] + \n",
    "                                 '/insilico/MetFrag').replace(\"./\", \"\")\n",
    "\n",
    "        # list of all the csv files in the result directory result_dir/inislico/MetFrag/\n",
    "        files_met = (glob.glob(result+'/*.csv'))\n",
    "\n",
    "        # read the csv file that contains all the features from the input .mzml file\n",
    "        file1  = pd.read_csv(input_dir + (input_table['ResultFileNames'][m] + '/insilico/MS1DATA.csv').replace(\"./\", \"\"))\n",
    "    \n",
    "        # for each feature in the MS1DATA.csv file\n",
    "        for i, row in file1.iterrows():\n",
    "        \n",
    "            # take id as a pattern to differentiate between different ids\n",
    "            pattern = file1.loc[i, \"id_X\"]\n",
    "        \n",
    "            #check which of the csv result files have the same pattern in their names\n",
    "            results = [i for i in files_met if pattern in i]\n",
    "        \n",
    "            # find which of the files with that id have KEGG in their names,\n",
    "            KEGG = [i for i in results if \"KEGG\" in i]\n",
    "        \n",
    "            # if kegg present in the name\n",
    "            if KEGG:\n",
    "            \n",
    "                # read the KEGG csv file for that feature\n",
    "                KEGG_file = pd.read_csv((KEGG)[0])\n",
    "            \n",
    "                # if the KEGG file isn't empty\n",
    "                if len(KEGG_file)>0:\n",
    "                \n",
    "                    # extract only the columns with >0.75 score\n",
    "                    KEGG_file = KEGG_file.drop(KEGG_file[KEGG_file.Score < 0.98].index)\n",
    "                    \n",
    "                    #s_best_kg = []\n",
    "                    #for kg, rows in KEGG_file.iterrows():\n",
    "                        #kg_smiles = Chem.MolToSmiles(Chem.MolFromInchi(KEGG_file[\"InChI\"][kg]))\n",
    "                        #SSmsk = [Chem.MolFromSmiles(kg_smiles), Chem.MolFromSmiles(siriusResults[\"SMILES\"][0])]\n",
    "                        #SSfpsk = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in SSmsk]\n",
    "                        #SStn2k = DataStructs.FingerprintSimilarity(SSfpsk[0],SSfpsk[1])\n",
    "                        #s_best_kg.append(SStn2k)\n",
    "                    #index_kg = np.argmax(s_best_kg)\n",
    "                        \n",
    "                    # add the relevavnt information to the original MS1DATA csv\n",
    "                    file1.loc[i, 'KG_ID'] = KEGG_file.loc[0, 'Identifier']\n",
    "                    file1.loc[i, 'KG_Name'] = KEGG_file.loc[0, 'CompoundName']\n",
    "                    file1.loc[i, 'KG_Formula'] = KEGG_file.loc[0, 'MolecularFormula']\n",
    "                    file1.loc[i, 'KG_expPeaks'] = KEGG_file.loc[0, 'NoExplPeaks']\n",
    "                    file1.loc[i, 'KG_SMILES'] = Chem.MolToSmiles(Chem.MolFromInchi(KEGG_file[\"InChI\"][0]))\n",
    "                    file1.loc[i, 'KG_Score'] = KEGG_file.loc[0, 'Score']\n",
    "                    if sl:\n",
    "                        file1.loc[i, 'KGSL_Score'] = KEGG_file.loc[0, 'SuspectListScore']\n",
    "                    file1.loc[i, 'KG_file'] = KEGG[0]\n",
    "                \n",
    "                    #create empty list of KEGG top smiles\n",
    "                    Kegg_smiles = []\n",
    "                \n",
    "                    # extract only the InChI of the top 5\n",
    "                    for j in KEGG_file[\"InChI\"][0:5].tolist():\n",
    "                        # convert the InChI to SMILES\n",
    "                        mol = Chem.MolToSmiles(Chem.MolFromInchi(j))\n",
    "                        mol2 = Chem.MolFromSmiles(mol)\n",
    "                        Kegg_smiles.append(mol2)\n",
    "                    # if there are more than 1 top smiles\n",
    "                    if len(Kegg_smiles) > 1:\n",
    "                        #calculate the MCSS\n",
    "                        res = rdFMCS.FindMCS(Kegg_smiles)\n",
    "                        sm_res = Chem.MolToSmiles(Chem.MolFromSmarts(res.smartsString))\n",
    "                        # if there are atleast 3 heavy atoms in the MCSS, then add it to the result file\n",
    "                        elem = [ele for ele in heavy_atoms if(ele in sm_res)]\n",
    "                        if elem and len(sm_res)>=3:\n",
    "                            file1.loc[i, 'KG_MCSSstring'] = res.smartsString\n",
    "                            file1.loc[i, 'KG_MCSS_SMILES'] = Chem.MolToSmiles(Chem.MolFromSmarts(res.smartsString))\n",
    "                            \n",
    "            #start here for PubChem; find which of the files with that id have PubChem in their names,\n",
    "            PubChem = [i for i in results if \"PubChem\" in i]\n",
    "            \n",
    "            if PubChem:\n",
    "\n",
    "                PubChem_file = pd.read_csv(PubChem[0])\n",
    "                \n",
    "                # if more candidates\n",
    "                if len(PubChem_file)>0:\n",
    "                    \n",
    "                    # take the ones with more than 0.80 score\n",
    "                    PubChem_file = PubChem_file.drop(PubChem_file[PubChem_file.Score < 0.80].index)\n",
    "                    #s_best_pb = []\n",
    "                    #for pb, rows in PubChem_file.iterrows():\n",
    "                        #SSmsp = [Chem.MolFromSmiles(PubChem_file[\"SMILES\"][pb]), Chem.MolFromSmiles(siriusResults[\"SMILES\"][0])]\n",
    "                        #SSfpsp = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in SSmsp]\n",
    "                        #SStn2p = DataStructs.FingerprintSimilarity(SSfpsp[0],SSfpsp[1])\n",
    "                        #s_best_pb.append(SStn2p)\n",
    "                    #index_pb = np.argmax(s_best_pb)\n",
    "                    # add the relavnt information to the original MS1DATA csv\n",
    "                    file1.loc[i, 'PC_ID'] = PubChem_file.loc[0, 'Identifier']\n",
    "                    file1.loc[i, 'PC_Name'] = PubChem_file.loc[0, 'IUPACName']\n",
    "                    file1.loc[i, 'PC_Formula'] = PubChem_file.loc[0, 'MolecularFormula']\n",
    "                    file1.loc[i, 'PC_expPeaks'] = PubChem_file.loc[0, 'NoExplPeaks']\n",
    "                    file1.loc[i, 'PC_SMILES'] = PubChem_file[\"SMILES\"][0]\n",
    "                    file1.loc[i, 'PC_Score'] = PubChem_file[\"Score\"][0]\n",
    "                    if sl:\n",
    "                        file1.loc[i, 'PCSL_Score'] = PubChem_file.loc[0, 'SuspectListScore']\n",
    "                    file1.loc[i, 'PC_file'] = PubChem[0]\n",
    "                    \n",
    "                    # empty object\n",
    "                    Pubchem_smiles = []\n",
    "                    \n",
    "                    # extract only the SMILES of the top 5\n",
    "                    for j in PubChem_file[\"SMILES\"][0:5].tolist():\n",
    "                        \n",
    "                        # Concert smiles to mol\n",
    "                        sm2 = Chem.MolFromSmiles(j)\n",
    "                        # store mol in Pubchem_smiles\n",
    "                        Pubchem_smiles.append(sm2)\n",
    "                    \n",
    "                    if len(Pubchem_smiles) > 1:\n",
    "                        # calculate MCSS\n",
    "                        res2 = rdFMCS.FindMCS(Pubchem_smiles)\n",
    "                        sm_res = Chem.MolToSmiles(Chem.MolFromSmarts(res2.smartsString))\n",
    "                        # If atleast 3 heavy atoms present\n",
    "                        elem = [ele for ele in heavy_atoms if(ele in sm_res)]\n",
    "                        if elem and len(sm_res)>=3:\n",
    "                            file1.loc[i, 'PC_MCSSstring']= res2.smartsString\n",
    "                            file1.loc[i, 'PC_MCSS_SMILES'] = Chem.MolToSmiles(Chem.MolFromSmarts(res2.smartsString))\n",
    "        file1.to_csv(input_dir + (input_table['ResultFileNames'][m] + '/insilico/MetFragResults.csv').replace(\"./\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5de3b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for multiple file\n",
    "#metfrag_postproc(input_dir= \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/\", \n",
    "#                 input_tablecsv = \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/input_table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c352c1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for multiple file\n",
    "#metfrag_postproc(input_dir= \"/Users/mahnoorzulfiqar/Downloads/MAW-main/\", \n",
    "#                 input_tablecsv = \"/Users/mahnoorzulfiqar/Downloads/MAW-main/input_table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7312134b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(metfrag_postproc.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80fae2f",
   "metadata": {},
   "source": [
    "### COMBINE IN SILICO -All files with SIRIUS results separate and with MetFragresults separate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a14bbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_insilico(input_dir, input_tablecsv, Source = \"all_insilico\"):\n",
    "    \n",
    "    \"\"\"combine_insilico function combines the Sirius results from all\n",
    "    result directories for each input mzml file. It does same for \n",
    "    Metfrag.\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "    \n",
    "    input_table (str): This is the table in csv format (defined in R), \n",
    "    which stores a csv table containing columns \"mzml_files\", which \n",
    "    contains liat of all input files with their relative paths, second\n",
    "    column is \"ResultFileName\" which is a list of the corresponding\n",
    "    result relative directories to each mzml files. Lastly, \"file_id\", \n",
    "    contains a file directory. This table will be used to read the \n",
    "    Sirius and MetFrag result csv files\n",
    "    \n",
    "    Source (str): either \"SIRIUS\" or \"MetFrag\"\n",
    "\n",
    "    Returns:\n",
    "    \n",
    "    dataframe: of combined SIRIUS/MetFrag results\n",
    "    \n",
    "    csv: stores the dataframe in a csv, named as \n",
    "    \"input_dir/ResultFileName/MetabolomicsResults/SIRIUS_combined.csv\" \n",
    "    OR/AND \n",
    "    \"input_dir/ResultFileName/MetabolomicsResults/MetFrag_combined.csv\"\n",
    "    \n",
    "    \n",
    "    Usage:\n",
    "    combine_insilico(input_dir = \"/user/project/\", \n",
    "    input_table = \"/user/project/suspectlist.csv\", Source = \"SIRIUS\")\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    input_table = pd.read_csv(input_tablecsv)\n",
    "    # create a new directory to store all results /MetabolomicsResults/\n",
    "    path = os.path.join(input_dir, \"MetabolomicsResults\")\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)    \n",
    "    # if Sirius results are to be combined\n",
    "    if Source == \"all_insilico\" or Source == \"SIRIUS\":\n",
    "        \n",
    "        # store all files paths here\n",
    "        all_files = []\n",
    "        for n, row in input_table.iterrows():\n",
    "            all_files.append(input_dir + input_table['ResultFileNames'][n].replace(\"./\", \"\") + '/insilico/SiriusResults.csv')\n",
    "        \n",
    "        # store all dataframes of the results here\n",
    "        li = []\n",
    "    \n",
    "        for filename in all_files:\n",
    "            df = pd.read_csv(filename, index_col=None, header=0)\n",
    "            df[\"ResultFileNames\"] = filename\n",
    "            li.append(df)\n",
    "            \n",
    "        # join all resulst dataframe\n",
    "        frame = pd.concat(li, axis=0, ignore_index=True)\n",
    "        frame.to_csv(input_dir + '/MetabolomicsResults/SIRIUS_combined.csv')       \n",
    "    \n",
    "    # if MetFrag results are to be combined\n",
    "    if Source == \"all_insilico\" or Source == \"MetFrag\":\n",
    "        \n",
    "        # store all files paths here\n",
    "        all_files = []\n",
    "        for m, row in input_table.iterrows():\n",
    "            all_files.append(input_dir + input_table['ResultFileNames'][m].replace(\"./\", \"\") + '/insilico/MetFragResults.csv')\n",
    "        li = []\n",
    "\n",
    "        for filename in all_files:\n",
    "            df = pd.read_csv(filename, index_col=None, header=0)\n",
    "            df[\"result_dir\"] = filename\n",
    "            li.append(df)\n",
    "\n",
    "        frame = pd.concat(li, axis=0, ignore_index=True)\n",
    "        frame.to_csv(input_dir+'MetabolomicsResults/MetFrag_combined.csv')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26b6ec8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for one file\n",
    "#combine_insilico(input_dir= \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/\", \n",
    " #                input_tablecsv = \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/input_table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14179601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for multiple file\n",
    "##combine_insilico(input_dir= \"/Users/mahnoorzulfiqar/Downloads/MAW-main/\", \n",
    "#                 input_tablecsv = \"/Users/mahnoorzulfiqar/Downloads/MAW-main/input_table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16c4edfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(combine_insilico.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aef0fd6",
   "metadata": {},
   "source": [
    "# Spectral DB Dereplication Results Post Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf3d8c1",
   "metadata": {},
   "source": [
    "### GNPS, MassBank and HMDB Results post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51160acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spec_postproc(input_dir, Source = \"all\"):\n",
    "    \n",
    "    \"\"\"spec_postproc function processes the resulst from dereplication \n",
    "    using different spectral DBs. \n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "    \n",
    "    Source (str): either \"mbank\" or \"hmdb\" or \"gnps\", or \"all\"\n",
    "\n",
    "    Returns:\n",
    "    \n",
    "    dataframe: of the paths of the processed DB results\n",
    "    \n",
    "    \n",
    "    Usage:\n",
    "    spec_postproc(input_dir = \"/user/project/\", Source = \"all\")\n",
    "\n",
    "    \"\"\"\n",
    "    def isNaN(string):\n",
    "        return string != string\n",
    "\n",
    "    # empty lists of csv files paths for each database\n",
    "    GNPScsvfiles = []\n",
    "    HMDBcsvfiles = []\n",
    "    MassBankcsvfiles = []\n",
    "    \n",
    "    #list all files and directories\n",
    "    for entry in os.listdir(input_dir):\n",
    "        if os.path.isdir(os.path.join(input_dir, entry)):\n",
    "            \n",
    "            # enter the directory with /spectral_dereplication/ results\n",
    "            sub_dir = input_dir + entry + '/spectral_dereplication'\n",
    "            if os.path.exists(sub_dir):\n",
    "                files = (glob.glob(sub_dir+'/*.csv'))\n",
    "\n",
    "                for f in files:\n",
    "                    if 'gnps.' in f: \n",
    "                        GNPScsvfiles.append(f)\n",
    "                    if 'hmdb.' in f: \n",
    "                        HMDBcsvfiles.append(f)\n",
    "                    if 'mbank.' in f: \n",
    "                        MassBankcsvfiles.append(f)\n",
    "                            \n",
    "    \n",
    "    if Source == \"hmdb\" or Source == \"all\":\n",
    "\n",
    "        if not os.path.exists(input_dir+\"structures.sdf\"):\n",
    "            #download SDF structures\n",
    "            os.system(\"wget -P \" + input_dir + \" https://hmdb.ca/system/downloads/current/structures.zip\")\n",
    "            os.system(\"unzip \"+ input_dir + \"structures.zip\" + \" -d \" + input_dir)\n",
    "            \n",
    "        # Load the sdf\n",
    "        dframe = PandasTools.LoadSDF((input_dir+\"structures.sdf\"),\n",
    "                                     idName='HMDB_ID',smilesName='SMILES',\n",
    "                                     molColName='Molecule', includeFingerprints=False)\n",
    "        \n",
    "        #### read sdf file from HMDB to collect names and smiles ####\n",
    "    \n",
    "        #HMDB CSV Result file pre_processing\n",
    "        \n",
    "        #open another csv path holding empty list, which will be filled \n",
    "        #with post processed csv results\n",
    "        HMDBcsvfiles2 = []\n",
    "        \n",
    "        for k in HMDBcsvfiles:\n",
    "            \n",
    "            # read the csv files\n",
    "            hmdb_df = pd.read_csv(k)\n",
    "            \n",
    "            # merge on basis of id, frame and hmdb result files\n",
    "            SmilesHM = pd.merge(hmdb_df, dframe, left_on=hmdb_df.HMDBcompoundID, right_on=dframe.DATABASE_ID)\n",
    "            \n",
    "            \n",
    "            for i, row in hmdb_df.iterrows():\n",
    "                \n",
    "                for j, row in SmilesHM.iterrows():\n",
    "                    \n",
    "                    # where index for both match, add the name and SMILES\n",
    "                    if hmdb_df['id_X'][i]== SmilesHM['id_X'][j]:\n",
    "                        hmdb_df.loc[i, 'HMDBSMILES'] = SmilesHM['SMILES'][j]#add SMILES\n",
    "                        hmdb_df.loc[i, 'HMDBcompound_name'] = SmilesHM[\"GENERIC_NAME\"][j]#add name\n",
    "                        hmdb_df.loc[i, 'HMDBformula'] = SmilesHM[\"FORMULA\"][j]#add formula\n",
    "                \n",
    "            csvname = (os.path.splitext(k)[0])+\"proc\"+\".csv\" # name for writing it in a new file\n",
    "            hmdb_df.to_csv(csvname) #write\n",
    "            HMDBcsvfiles2.append(csvname)# add to a list\n",
    "            dict1 = {'HMDBr': HMDBcsvfiles2} \n",
    "            df = pd.DataFrame(dict1)\n",
    "        \n",
    "    #MassBank CSV Result file pre_processing\n",
    "    \n",
    "    if Source == \"mbank\" or Source == \"all\":\n",
    "        \n",
    "        #open another csv path holding empty list, which will be filled \n",
    "        #with post processed csv results\n",
    "        MassBankcsvfiles2 = []\n",
    "        \n",
    "        for l in MassBankcsvfiles:\n",
    "            \n",
    "            # read mbank csv file\n",
    "            mbank_df = pd.read_csv(l)\n",
    "            \n",
    "            for i, row in mbank_df.iterrows():\n",
    "                \n",
    "                inchiK = str(mbank_df[\"MBinchiKEY\"][i])\n",
    "                \n",
    "                #extract inchikeys\n",
    "                y = pcp.get_compounds(inchiK, 'inchikey')#compound based on inchikey\n",
    "                \n",
    "                for compound in y:\n",
    "                    \n",
    "                    #add smiles\n",
    "                    smles = compound.isomeric_smiles   \n",
    "                    mbank_df.loc[i, 'MBSMILES'] = smles\n",
    "                    \n",
    "            csvname = (os.path.splitext(l)[0])+\"proc\"+\".csv\"\n",
    "            mbank_df.to_csv(csvname)\n",
    "            MassBankcsvfiles2.append(csvname)\n",
    "            \n",
    "            dict1 = {'MBr': MassBankcsvfiles2} \n",
    "            df = pd.DataFrame(dict1)\n",
    "    \n",
    "    # GNPS CSV Result file pre_processing\n",
    "    if Source == \"gnps\" or Source == \"all\":\n",
    "        #open another csv path holding empty list, which will be filled \n",
    "        #with post processed csv results\n",
    "        GNPScsvfiles2 = []\n",
    "        #currently only these subsets are removed from the names from GNPS\n",
    "        matches = [\"M+\",\"[M\", \"M-\", \"2M\", \"M*\" \"20.0\", \"50.0\", \"30.0\", \"40.0\", \"60.0\", \"70.0\", \"eV\", \"Massbank\"\n",
    "               , \"Spectral\", \"Match\", \"to\", \"from\", \"NIST14\", \"MoNA\", '[IIN-based:',  '[IIN-based', 'on:', 'CCMSLIB00003136269]']\n",
    "\n",
    "        for l in GNPScsvfiles:\n",
    "            gnps_df = pd.read_csv(l)\n",
    "\n",
    "            for i, row in gnps_df.iterrows():\n",
    "                # if compound name is present\n",
    "                if not isNaN(gnps_df['GNPScompound_name'][i]):\n",
    "                    # split if there is a gap in the names\n",
    "                    string_chng = (gnps_df['GNPScompound_name'][i].split(\" \"))\n",
    "\n",
    "                    # create an empty list\n",
    "                    newstr = []\n",
    "\n",
    "                    # for each part of the string in the names\n",
    "                    chng = []\n",
    "\n",
    "                    for j in range(len(string_chng)):\n",
    "\n",
    "                        # check if the substrings are present in the matches and no - is present\n",
    "                        if not any(x in string_chng[j] for x in matches): #and not '-' == string_chng[j]:\n",
    "\n",
    "                            # IF | and ! not in the substring\n",
    "                            if '|' not in string_chng[j] or '!' not in string_chng[j]:\n",
    "                                newstr.append(string_chng[j])\n",
    "\n",
    "                            # if | present in the substring   \n",
    "                            elif '|' in string_chng[j]:\n",
    "\n",
    "                                #split the string\n",
    "                                jlen = string_chng[j].split(\"|\")\n",
    "                                #how many substrings are left now\n",
    "                                lst = len(jlen)-1\n",
    "                                #append this to chng\n",
    "                                chng.append(jlen[lst])\n",
    "                                break\n",
    "\n",
    "                    # now append chng to newstr            \n",
    "                    chng.append(' '.join(newstr))\n",
    "                    #save this as the correct name\n",
    "                    gnps_df.loc[i, \"corr_names\"] = chng[0]\n",
    "                    if not isNaN(gnps_df['GNPSSMILES'][i]):\n",
    "                        if chng == '':\n",
    "                            break\n",
    "                        elif gnps_df['GNPSSMILES'][i].isalpha():\n",
    "                            s = pcp.get_compounds(chng[0], 'name')\n",
    "                            if s:\n",
    "                                for comp in s:\n",
    "                                    gnps_df[\"GNPSSMILES\"][i] = comp.isomeric_smiles\n",
    "                            else:\n",
    "                                gnps_df[\"GNPSSMILES\"][i] = ''\n",
    "                else:\n",
    "                    gnps_df[\"GNPSSMILES\"][i] = ''\n",
    "\n",
    "            for i, row in gnps_df.iterrows():\n",
    "                if isNaN(gnps_df['GNPSSMILES'][i]):\n",
    "                    if \"[\" in gnps_df['GNPScompound_name'][i].split(\" \")[-1]:\n",
    "                        string_chng = (gnps_df['GNPScompound_name'][i].split(\"[\"))\n",
    "                        #print(gnps_df['GNPScompound_name'][i])\n",
    "                        keep_names = []\n",
    "                        for j in range(len(string_chng)-1):\n",
    "                            gnps_df.loc[i, \"corr_names\"] == string_chng[j]\n",
    "                            s = pcp.get_compounds(string_chng[j], 'name')\n",
    "\n",
    "                            if s:\n",
    "                                for comp in s:\n",
    "                                    gnps_df[\"GNPSSMILES\"][i] = comp.isomeric_smiles\n",
    "                            else:\n",
    "                                gnps_df[\"GNPSSMILES\"][i] = ''\n",
    "                if not isNaN(gnps_df['GNPSSMILES'][i]):\n",
    "                    try:\n",
    "                        sx = pcp.get_compounds(gnps_df['GNPSSMILES'][i], 'smiles')\n",
    "                        if sx:\n",
    "                            sx = str(sx)\n",
    "                            comp = pcp.Compound.from_cid([int(x) for x in re.findall(r'\\b\\d+\\b', sx)])\n",
    "                            gnps_df.loc[i, 'GNPSformula'] = comp.molecular_formula\n",
    "                    except:\n",
    "                        gnps_df.loc[i, 'GNPSformula'] = ''\n",
    "\n",
    "            csvname = (os.path.splitext(l)[0])+\"proc\"+\".csv\"\n",
    "            gnps_df.to_csv(csvname)\n",
    "            GNPScsvfiles2.append(csvname)\n",
    "            dict1 = {'GNPSr': GNPScsvfiles2} \n",
    "            df = pd.DataFrame(dict1)\n",
    "        \n",
    "\n",
    "    if Source == \"all\":\n",
    "        \n",
    "        dict1 = {'GNPSr': GNPScsvfiles2, 'HMDBr': HMDBcsvfiles2, 'MBr': MassBankcsvfiles2} \n",
    "        df = pd.DataFrame(dict1)\n",
    "\n",
    "        return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5137fcd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89471167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for one file\n",
    "#spec_postproc(input_dir= \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/\",\n",
    "#             Source = \"gnps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b793f0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for multiple file\n",
    "#spec_postproc(input_dir= \"/Users/mahnoorzulfiqar/Downloads/MAW-main/\", \n",
    "#             Source = \"gnps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8889acf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(spec_postproc.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6e281b",
   "metadata": {},
   "source": [
    "### Combine_all Spectral DBs for one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60009629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_specdb(input_dir):\n",
    "    \n",
    "    \"\"\"combine_specdb function combines all results from different\n",
    "    spectral dbs. Can only be used if more than one db is used \n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "\n",
    "    Returns:\n",
    "    dataframe: of the paths of the merged results\n",
    "    \n",
    "    \n",
    "    Usage:\n",
    "    combine_specdb(input_dir)\n",
    "\n",
    "    \"\"\"\n",
    "    def isNaN(string):\n",
    "        return string != string\n",
    "\n",
    "    \n",
    "    # empty lists of csv files paths for each database\n",
    "    GNPScsvfiles2 = []\n",
    "    HMDBcsvfiles2 = []\n",
    "    MassBankcsvfiles2 = []\n",
    "    \n",
    "    #list all files and directories\n",
    "    for entry in os.listdir(input_dir):\n",
    "        if os.path.isdir(os.path.join(input_dir, entry)):\n",
    "            \n",
    "            # enter the directory with /spectral_dereplication/ results\n",
    "            sub_dir = input_dir + entry + '/spectral_dereplication'\n",
    "            if os.path.exists(sub_dir):\n",
    "                files = (glob.glob(sub_dir+'/*.csv'))\n",
    "\n",
    "                for f in files:\n",
    "                    if 'gnpsproc.' in f: \n",
    "                        GNPScsvfiles2.append(f)\n",
    "                    if 'hmdbproc.' in f: \n",
    "                        HMDBcsvfiles2.append(f)\n",
    "                    if 'mbankproc.' in f: \n",
    "                        MassBankcsvfiles2.append(f)\n",
    "   \n",
    "    # if all results present\n",
    "    if len(GNPScsvfiles2)>0 and len(HMDBcsvfiles2)>0 and len(MassBankcsvfiles2)>0:\n",
    "        \n",
    "        dict1 = {'GNPSr': GNPScsvfiles2, 'HMDBr': HMDBcsvfiles2, 'MBr': MassBankcsvfiles2} \n",
    "        df = pd.DataFrame(dict1)\n",
    "    \n",
    "        Merged_Result_df = []\n",
    "        for i, row in df.iterrows():\n",
    "            CSVfileG = pd.read_csv(df[\"GNPSr\"][i])\n",
    "            CSVfileH = pd.read_csv(df[\"HMDBr\"][i])\n",
    "            CSVfileM = pd.read_csv(df[\"MBr\"][i])\n",
    "            if os.path.exists(df[\"MBr\"][i]) and os.path.exists(df[\"HMDBr\"][i]) and os.path.exists(df[\"GNPSr\"][i]):\n",
    "                # merge on the basis of Idx\n",
    "                MergedRE = CSVfileG.merge(CSVfileH,on='id_X').merge(CSVfileM,on='id_X')\n",
    "                csvname = (df[\"GNPSr\"][i]).replace(\"gnpsproc\", \"mergedR\")\n",
    "                MergedRE.to_csv(csvname)\n",
    "                Merged_Result_df.append(csvname)\n",
    "                \n",
    "                \n",
    "    # if only GNPS and MassBank           \n",
    "    if len(GNPScsvfiles2)>0 and len(HMDBcsvfiles2)==0 and len(MassBankcsvfiles2)>0:\n",
    "            dict1 = {'GNPSr': GNPScsvfiles2, 'MBr': MassBankcsvfiles2} \n",
    "            df = pd.DataFrame(dict1)\n",
    "            Merged_Result_df = []\n",
    "            for i, row in df.iterrows():\n",
    "                CSVfileG = pd.read_csv(df[\"GNPSr\"][i])\n",
    "                CSVfileM = pd.read_csv(df[\"MBr\"][i])\n",
    "                if os.path.exists(df[\"MBr\"][i]) and os.path.exists(df[\"GNPSr\"][i]):\n",
    "                    # merge on the basis of Idx\n",
    "                    MergedRE = CSVfileG.merge(CSVfileM,on='id_X')\n",
    "                    csvname = (df[\"MBr\"][i]).replace(\"mbankproc\", \"mergedR\")\n",
    "                    MergedRE.to_csv(csvname)\n",
    "                    Merged_Result_df.append(csvname)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    # if only GNPS and Hmdb\n",
    "    if not isNaN(GNPScsvfiles2) and not isNaN(HMDBcsvfiles2) and isNaN(MassBankcsvfiles2):\n",
    "            dict1 = {'GNPSr': GNPScsvfiles2, 'HMDBr': MassBankcsvfiles2} \n",
    "            df = pd.DataFrame(dict1)\n",
    "            Merged_Result_df = []\n",
    "            for i, row in df.iterrows():\n",
    "                CSVfileG = pd.read_csv(df[\"GNPSr\"][i])\n",
    "                CSVfileH = pd.read_csv(df[\"HMDBr\"][i])\n",
    "                if os.path.exists(df[\"HMDBr\"][i]) and os.path.exists(df[\"GNPSr\"][i]):\n",
    "                    # merge on the basis of Idx\n",
    "                    MergedRE = CSVfileG.merge(CSVfileH,on='id_X')\n",
    "                    csvname = (df[\"GNPSr\"][i]).replace(\"gnpsproc\", \"mergedR\")\n",
    "                    MergedRE.to_csv(csvname)\n",
    "                    Merged_Result_df.append(csvname)\n",
    "                \n",
    "                \n",
    "                \n",
    "    # if only MBANK and Hmdb\n",
    "    if not isNaN(GNPScsvfiles2) and isNaN(HMDBcsvfiles2) and isNaN(MassBankcsvfiles2):\n",
    "            dict1 = {'GNPSr': GNPScsvfiles2, 'HMDBr': MassBankcsvfiles2} \n",
    "            df = pd.DataFrame(dict1)   \n",
    "            dict1 = {'GNPSr': GNPScsvfiles2, 'HMDBr': MassBankcsvfiles2} \n",
    "            df = pd.DataFrame(dict1)\n",
    "            Merged_Result_df = []\n",
    "            for i, row in df.iterrows():\n",
    "                CSVfileG = pd.read_csv(df[\"MBr\"][i])\n",
    "                CSVfileH = pd.read_csv(df[\"HMDBr\"][i])\n",
    "                if os.path.exists(df[\"MBr\"][i]) and os.path.exists(df[\"HMDBr\"][i]):\n",
    "                    # merge on the basis of Idx\n",
    "                    MergedRE = CSVfileM.merge(CSVfileH,on='id_X')\n",
    "                    csvname = (df[\"MBr\"][i]).replace(\"mbankproc\", \"mergedR\")\n",
    "                    MergedRE.to_csv(csvname)\n",
    "                    Merged_Result_df.append(csvname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1754a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for multiple file\n",
    "#combine_specdb(input_dir= \"/Users/mahnoorzulfiqar/Downloads/MAW-main/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db3b91f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for one file\n",
    "#combine_specdb(input_dir= \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8e489d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7174f3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(combine_specdb.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e951a6a3",
   "metadata": {},
   "source": [
    "### Combine all files for spectral db dereplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d6dbeda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_allspec(input_dir):\n",
    "    \n",
    "    \"\"\"combine_allspec function combines all results from different\n",
    "    spectral dbs. Can only be used if more than one db is used \n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "    df (dataframe): dataframe from combine_specdb\n",
    "    \n",
    "    Returns:\n",
    "    dataframe: of the paths of the merged results from all files\n",
    "    \n",
    "    Usage:\n",
    "    combine_allspec(input_dir = \"usr/project/\", comb_df)\n",
    "\n",
    "    \"\"\"\n",
    "    def isNaN(string):\n",
    "        return string != string\n",
    "    # create a new directory to store all results /MetabolomicsResults/\n",
    "    path = os.path.join(input_dir, \"MetabolomicsResults\")\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "        \n",
    "        \n",
    "    Mergedcsvfiles = []\n",
    "    single_file = []\n",
    "    \n",
    "    #list all files and directories\n",
    "    for entry in os.listdir(input_dir):\n",
    "        if os.path.isdir(os.path.join(input_dir, entry)):\n",
    "            \n",
    "            # enter the directory with /spectral_dereplication/ results\n",
    "            sub_dir = input_dir + entry + '/spectral_dereplication'\n",
    "            if os.path.exists(sub_dir):\n",
    "                files = (glob.glob(sub_dir+'/*.csv'))\n",
    "\n",
    "                for f in files:\n",
    "                    if 'mergedR.csv' in f: \n",
    "                        Mergedcsvfiles.append(f)\n",
    "                    else:\n",
    "                        single_file.append(f)\n",
    "    \n",
    "    if len(Mergedcsvfiles)>0:\n",
    "        combined_csv = pd.concat([pd.read_csv(l) for l in Mergedcsvfiles], ignore_index=True)\n",
    "        combined_csv.to_csv(input_dir + 'MetabolomicsResults/SD_post_processed_combined_results.csv')\n",
    "        return(combined_csv)\n",
    "    else:\n",
    "        single_csv = pd.read_csv(single_file[0])\n",
    "        single_csv.to_csv(input_dir + 'MetabolomicsResults/SD_post_processed_combined_results.csv')\n",
    "        return(single_csv)\n",
    "    \n",
    "    #for i, row in combined_csv.iterrows():\n",
    "        #if combined_csv['GNPSSMILES'][i] == ' ' or isNaN(combined_csv['GNPSSMILES'][i]):\n",
    "            #combined_csv['GNPSSMILES'][i] = ''\n",
    "            \n",
    "    #for i, row in combined_csv.iterrows():\n",
    "        #if not isNaN(combined_csv['MBinchiKEY'][i]):\n",
    "            #try:\n",
    "                #y = pcp.get_compounds(combined_csv['MBinchiKEY'][i], 'inchikey')\n",
    "                #if len(y)>1:\n",
    "                    #combined_csv['MBSMILES'][i] = y[0].isomeric_smiles\n",
    "            #except:\n",
    "                #pass\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3e236b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for one file\n",
    "#combine_allspec(input_dir= \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4c5d8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for multiple file\n",
    "#combine_allspec(input_dir= \"/Users/mahnoorzulfiqar/Downloads/MAW-main/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "56418fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(combine_allspec.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0558d699",
   "metadata": {},
   "source": [
    "### Scoring Scheme for Spectral DB Dereplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cfc095",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f0f28c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring_spec(input_dir, spec_file):\n",
    "    \n",
    "    \"\"\"scoring_spec extracts the candidates with high scores from\n",
    "    the results from combine_allspec function \n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "    combined (dataframe): dataframe from combine_allspec\n",
    "    \n",
    "    Returns:\n",
    "    dataframe: of the all features and their results\n",
    "    csv: CSV reuslt file named MetabolomicsResults/combinedSpecDB.csv\n",
    "    which contains all the features and their Spec DB annotations\n",
    "    \n",
    "    Usage:\n",
    "    scoring_spec(input_dir = \"usr/project/\", combined)\n",
    "\n",
    "    \"\"\"\n",
    "    def isNaN(string):\n",
    "        return string != string\n",
    "    # the scoring highly depends on the following information:\n",
    "    # similarity scores should be higher than 0.75\n",
    "    # intScore >=0.50\n",
    "    # mzScore >= 0.50\n",
    "    # ratio of the matchingpeaks by the totalpeaks in the query >= 0.50\n",
    "    \n",
    "    combined = pd.read_csv(spec_file)\n",
    "    \n",
    "    def HMDB_Scoring(db, i):\n",
    "        if db['HMDBmax_similarity'][i] >= 0.75 and db['HMDBintScore'][i] >= 0.50 and db['HMDBmzScore'][i] >= 0.50 and db['HQMatchingPeaks'][i]/db['hQueryTotalPeaks'][i] >= 0.50:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    \n",
    "    def GNPS_Scoring(db, i):\n",
    "        if db['GNPSmax_similarity'][i] >= 0.90 and db['GNPSintScore'][i] >= 0.50 and db['GNPSmzScore'][i] >= 0.50 and db['GQMatchingPeaks'][i]/db['gQueryTotalPeaks'][i] >= 0.50:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    \n",
    "    def MB_Scoring(db, i):\n",
    "        if db['MBmax_similarity'][i] >= 0.50 and db['MBintScore'][i] >= 0.50 and db['MBmzScore'][i] >= 0.50 and db['MQMatchingPeaks'][i]/db['mQueryTotalPeaks'][i] >= 0.50:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    \n",
    "    for i, row in combined.iterrows():\n",
    "        \n",
    "        \n",
    "        if 'HMDBSMILES' in combined.columns and 'MBSMILES' in combined.columns and 'GNPSSMILES' in combined.columns:\n",
    "            \n",
    "            # if all DBs show good candidates accorindg to the scoring\n",
    "            if HMDB_Scoring(combined, i) and GNPS_Scoring(combined, i) and MB_Scoring(combined, i) and not isNaN(combined['GNPSSMILES'][i]) and not isNaN(combined['MBSMILES'][i]) and not isNaN(combined['HMDBSMILES'][i]):\n",
    "            \n",
    "                # calulate the tanimoto similarity between the candidates from three DBs\n",
    "            \n",
    "                # hmdb and gnps\n",
    "                HGms = [Chem.MolFromSmiles(combined['HMDBSMILES'][i]), Chem.MolFromSmiles(combined['GNPSSMILES'][i])]\n",
    "                HGfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in HGms]\n",
    "                HGtn = DataStructs.FingerprintSimilarity(HGfps[0],HGfps[1])\n",
    "            \n",
    "                # gnps and mbank\n",
    "                GMms = [Chem.MolFromSmiles(combined['GNPSSMILES'][i]), Chem.MolFromSmiles(combined['MBSMILES'][i])]\n",
    "                GMfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in GMms]\n",
    "                GMtn = DataStructs.FingerprintSimilarity(GMfps[0],GMfps[1])\n",
    "            \n",
    "                # mbank and hmdb\n",
    "                HMms = [Chem.MolFromSmiles(combined['HMDBSMILES'][i]), Chem.MolFromSmiles(combined['MBSMILES'][i])]\n",
    "                HMfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in HMms]\n",
    "                HMtn = DataStructs.FingerprintSimilarity(HMfps[0],HMfps[1])\n",
    "            \n",
    "                # add the following columns\n",
    "                combined.loc[i, 'annotation'] = 'HMDB, GNPS, MassBank'\n",
    "                combined.loc[i, 'tanimotoHG'] = HGtn\n",
    "                combined.loc[i, 'tanimotoGM'] = GMtn\n",
    "                combined.loc[i, 'tanimotoHM'] = HMtn\n",
    "                combined.loc[i, 'occurence'] = 3\n",
    "        \n",
    "            # if HMDB and GNPS show good candidates accorindg to the scoring\n",
    "            if HMDB_Scoring(combined, i) and GNPS_Scoring(combined, i) and not MB_Scoring(combined, i) and not isNaN(combined['GNPSSMILES'][i]) and not isNaN(combined['HMDBSMILES'][i]):\n",
    "                HGms = [Chem.MolFromSmiles(combined['HMDBSMILES'][i]), Chem.MolFromSmiles(combined['GNPSSMILES'][i])]\n",
    "                HGfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in HGms]\n",
    "                HGtn = DataStructs.FingerprintSimilarity(HGfps[0],HGfps[1])\n",
    "        \n",
    "                combined.loc[i, 'annotation'] = 'HMDB, GNPS'\n",
    "                combined.loc[i, 'tanimotoHG'] = HGtn\n",
    "                combined.loc[i, 'tanimotoGM'] = np.nan\n",
    "                combined.loc[i, 'tanimotoHM'] = np.nan\n",
    "                combined.loc[i, 'occurence'] = 2\n",
    "        \n",
    "            # if MassBank and GNPS show good candidates accorindg to the scoring\n",
    "            if not HMDB_Scoring(combined, i) and GNPS_Scoring(combined, i) and MB_Scoring(combined, i) and not isNaN(combined['MBSMILES'][i]) and not isNaN(combined['GNPSSMILES'][i]):\n",
    "                GMms = [Chem.MolFromSmiles(combined['GNPSSMILES'][i]), Chem.MolFromSmiles(combined['MBSMILES'][i])]\n",
    "                GMfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in GMms]\n",
    "                GMtn = DataStructs.FingerprintSimilarity(GMfps[0],GMfps[1])\n",
    "        \n",
    "                combined.loc[i, 'annotation'] = 'GNPS, MassBank'\n",
    "                combined.loc[i, 'tanimotoHG'] = np.nan\n",
    "                combined.loc[i, 'tanimotoGM'] = GMtn\n",
    "                combined.loc[i, 'tanimotoHM'] = np.nan\n",
    "                combined.loc[i, 'occurence'] = 2\n",
    "        \n",
    "            # if MassBank and HMDB show good candidates accorindg to the scoring\n",
    "            if HMDB_Scoring(combined, i) and not GNPS_Scoring(combined, i) and MB_Scoring(combined, i) and not isNaN(combined['MBSMILES'][i]) and not isNaN(combined['HMDBSMILES'][i]):\n",
    "                HMms = [Chem.MolFromSmiles(combined['HMDBSMILES'][i]), Chem.MolFromSmiles(combined['MBSMILES'][i])]\n",
    "                HMfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in HMms]\n",
    "                HMtn = DataStructs.FingerprintSimilarity(HMfps[0],HMfps[1])\n",
    "        \n",
    "                combined.loc[i, 'annotation'] = 'HMDB, MassBank'\n",
    "                combined.loc[i, 'tanimotoHG'] = np.nan\n",
    "                combined.loc[i, 'tanimotoGM'] = np.nan\n",
    "                combined.loc[i, 'tanimotoHM'] = HMtn\n",
    "                combined.loc[i, 'occurence'] = 2\n",
    "        \n",
    "            # only HMDB\n",
    "            if HMDB_Scoring(combined, i) and not GNPS_Scoring(combined, i) and not MB_Scoring(combined, i):\n",
    "        \n",
    "                combined.loc[i, 'annotation'] = 'HMDB'\n",
    "                combined.loc[i, 'tanimotoHG'] = np.nan\n",
    "                combined.loc[i, 'tanimotoGM'] = np.nan\n",
    "                combined.loc[i, 'tanimotoHM'] = np.nan\n",
    "                combined.loc[i, 'occurence'] = 1\n",
    "            \n",
    "            # only GNPS\n",
    "            if not HMDB_Scoring(combined, i) and GNPS_Scoring(combined, i) and not MB_Scoring(combined, i):\n",
    "        \n",
    "                combined.loc[i, 'annotation'] = 'GNPS'\n",
    "                combined.loc[i, 'tanimotoHG'] = np.nan\n",
    "                combined.loc[i, 'tanimotoGM'] = np.nan\n",
    "                combined.loc[i, 'tanimotoHM'] = np.nan\n",
    "                combined.loc[i, 'occurence'] = 1\n",
    "        \n",
    "            # only MassBank\n",
    "            if not HMDB_Scoring(combined, i) and not GNPS_Scoring(combined, i) and MB_Scoring(combined, i):\n",
    "        \n",
    "                combined.loc[i, 'annotation'] = 'MassBank'\n",
    "                combined.loc[i, 'tanimotoHG'] = np.nan\n",
    "                combined.loc[i, 'tanimotoGM'] = np.nan\n",
    "                combined.loc[i, 'tanimotoHM'] = np.nan\n",
    "                combined.loc[i, 'occurence'] = 1\n",
    "        \n",
    "            # none\n",
    "            if not HMDB_Scoring(combined, i) and not GNPS_Scoring(combined, i) and not MB_Scoring(combined, i):\n",
    "                combined.loc[i, 'annotation'] = 'none'\n",
    "                combined.loc[i, 'tanimotoHG'] = np.nan\n",
    "                combined.loc[i, 'tanimotoGM'] = np.nan\n",
    "                combined.loc[i, 'tanimotoHM'] = np.nan\n",
    "                combined.loc[i, 'occurence'] = 0\n",
    "        \n",
    "        if 'HMDBSMILES' not in combined.columns and 'MBSMILES' in combined.columns and 'GNPSSMILES' in combined.columns:\n",
    "\n",
    "            # if MassBank and GNPS show good candidates accorindg to the scoring\n",
    "            if GNPS_Scoring(combined, i) and MB_Scoring(combined, i) and not isNaN(combined['MBSMILES'][i]) and not isNaN(combined['GNPSSMILES'][i]):\n",
    "                GMms = [Chem.MolFromSmiles(combined['GNPSSMILES'][i]), Chem.MolFromSmiles(combined['MBSMILES'][i])]\n",
    "                GMfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in GMms]\n",
    "                GMtn = DataStructs.FingerprintSimilarity(GMfps[0],GMfps[1])\n",
    "        \n",
    "                combined.loc[i, 'annotation'] = 'GNPS, MassBank'\n",
    "                combined.loc[i, 'tanimotoGM'] = GMtn\n",
    "                combined.loc[i, 'occurence'] = 2\n",
    "            # only GNPS\n",
    "            if GNPS_Scoring(combined, i) and not MB_Scoring(combined, i):\n",
    "        \n",
    "                combined.loc[i, 'annotation'] = 'GNPS'\n",
    "                combined.loc[i, 'tanimotoGM'] = np.nan\n",
    "                combined.loc[i, 'occurence'] = 1\n",
    "        \n",
    "            # only MassBank\n",
    "            if not GNPS_Scoring(combined, i) and MB_Scoring(combined, i):\n",
    "        \n",
    "                combined.loc[i, 'annotation'] = 'MassBank'\n",
    "                combined.loc[i, 'tanimotoGM'] = np.nan\n",
    "                combined.loc[i, 'occurence'] = 1\n",
    "                \n",
    "            # none\n",
    "            if not GNPS_Scoring(combined, i) and not MB_Scoring(combined, i):\n",
    "                combined.loc[i, 'annotation'] = 'none'\n",
    "                combined.loc[i, 'tanimotoGM'] = np.nan\n",
    "                combined.loc[i, 'occurence'] = 0\n",
    "                \n",
    "                \n",
    "                \n",
    "        if 'HMDBSMILES' in combined.columns and 'MBSMILES' not in combined.columns and 'GNPSSMILES' in combined.columns:\n",
    "            # if HMDB and GNPS show good candidates accorindg to the scoring\n",
    "            if HMDB_Scoring(combined, i) and GNPS_Scoring(combined, i) and not isNaN(combined['GNPSSMILES'][i]) and not isNaN(combined['HMDBSMILES'][i]):\n",
    "                HGms = [Chem.MolFromSmiles(combined['HMDBSMILES'][i]), Chem.MolFromSmiles(combined['GNPSSMILES'][i])]\n",
    "                HGfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in HGms]\n",
    "                HGtn = DataStructs.FingerprintSimilarity(HGfps[0],HGfps[1])\n",
    "        \n",
    "                combined.loc[i, 'annotation'] = 'HMDB, GNPS'\n",
    "                combined.loc[i, 'tanimotoHG'] = HGtn\n",
    "                combined.loc[i, 'occurence'] = 2\n",
    "        \n",
    "            # only HMDB\n",
    "            if HMDB_Scoring(combined, i) and not GNPS_Scoring(combined, i):\n",
    "        \n",
    "                combined.loc[i, 'annotation'] = 'HMDB'\n",
    "                combined.loc[i, 'tanimotoHG'] = np.nan\n",
    "                combined.loc[i, 'occurence'] = 1\n",
    "            \n",
    "            # only GNPS\n",
    "            if not HMDB_Scoring(combined, i) and GNPS_Scoring(combined, i):\n",
    "        \n",
    "                combined.loc[i, 'annotation'] = 'GNPS'\n",
    "                combined.loc[i, 'tanimotoHG'] = np.nan\n",
    "                combined.loc[i, 'occurence'] = 1\n",
    "            # none\n",
    "            if not HMDB_Scoring(combined, i) and not GNPS_Scoring(combined, i):\n",
    "                combined.loc[i, 'annotation'] = 'none'\n",
    "                combined.loc[i, 'tanimotoHG'] = np.nan\n",
    "                combined.loc[i, 'occurence'] = 0\n",
    "    \n",
    "        if 'HMDBSMILES' in combined.columns and 'MBSMILES' in combined.columns and 'GNPSSMILES' not in combined.columns:\n",
    "            \n",
    "            # if MassBank and HMDB show good candidates accorindg to the scoring\n",
    "            if HMDB_Scoring(combined, i) and MB_Scoring(combined, i) and not isNaN(combined['MBSMILES'][i]) and not isNaN(combined['HMDBSMILES'][i]):\n",
    "                HMms = [Chem.MolFromSmiles(combined['HMDBSMILES'][i]), Chem.MolFromSmiles(combined['MBSMILES'][i])]\n",
    "                HMfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in HMms]\n",
    "                HMtn = DataStructs.FingerprintSimilarity(HMfps[0],HMfps[1])\n",
    "        \n",
    "                combined.loc[i, 'annotation'] = 'HMDB, MassBank'\n",
    "                combined.loc[i, 'tanimotoHM'] = HMtn\n",
    "                combined.loc[i, 'occurence'] = 2\n",
    "                \n",
    "            # only HMDB\n",
    "            if HMDB_Scoring(combined, i) and not MB_Scoring(combined, i):\n",
    "        \n",
    "                combined.loc[i, 'annotation'] = 'HMDB'\n",
    "                combined.loc[i, 'tanimotoHM'] = np.nan\n",
    "                combined.loc[i, 'occurence'] = 1\n",
    "            \n",
    "            # only MassBank\n",
    "            if not HMDB_Scoring(combined, i) and MB_Scoring(combined, i):\n",
    "        \n",
    "                combined.loc[i, 'annotation'] = 'MassBank'\n",
    "                combined.loc[i, 'tanimotoHM'] = np.nan\n",
    "                combined.loc[i, 'occurence'] = 1\n",
    "        \n",
    "            # none\n",
    "            if not HMDB_Scoring(combined, i) and not MB_Scoring(combined, i):\n",
    "                combined.loc[i, 'annotation'] = 'none'\n",
    "                combined.loc[i, 'tanimotoHM'] = np.nan\n",
    "                combined.loc[i, 'occurence'] = 0\n",
    "        \n",
    "        \n",
    "        #If only HMDB was used\n",
    "        \n",
    "        if 'HMDBSMILES' in combined.columns and 'MBSMILES' not in combined.columns and 'GNPSSMILES' not in combined.columns:\n",
    "            # only HMDB\n",
    "            if HMDB_Scoring(combined, i):\n",
    "        \n",
    "                combined.loc[i, 'annotation'] = 'HMDB'\n",
    "                combined.loc[i, 'occurence'] = 1\n",
    "            \n",
    "            # none\n",
    "            if not HMDB_Scoring(combined, i):\n",
    "                combined.loc[i, 'annotation'] = 'none'\n",
    "                combined.loc[i, 'occurence'] = 0\n",
    "                \n",
    "                \n",
    "        #If only MassBank was used      \n",
    "                \n",
    "        if 'HMDBSMILES' not in combined.columns and 'MBSMILES' in combined.columns and 'GNPSSMILES' not in combined.columns:\n",
    "            # only MassBank\n",
    "            if MB_Scoring(combined, i):\n",
    "        \n",
    "                combined.loc[i, 'annotation'] = 'MassBank'\n",
    "                combined.loc[i, 'occurence'] = 1\n",
    "            \n",
    "            # none\n",
    "            if not MB_Scoring(combined, i):\n",
    "                combined.loc[i, 'annotation'] = 'none'\n",
    "                combined.loc[i, 'occurence'] = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "        #If only GNPS was used\n",
    "        \n",
    "        if 'HMDBSMILES' not in combined.columns and 'MBSMILES' not in combined.columns and 'GNPSSMILES' in combined.columns:\n",
    "            # only GNPS\n",
    "            if GNPS_Scoring(combined, i):\n",
    "        \n",
    "                combined.loc[i, 'annotation'] = 'GNPS'\n",
    "                combined.loc[i, 'occurence'] = 1\n",
    "            \n",
    "            # none\n",
    "            if not GNPS_Scoring(combined, i):\n",
    "                combined.loc[i, 'annotation'] = 'none'\n",
    "                combined.loc[i, 'occurence'] = 0\n",
    "                \n",
    "                \n",
    "    combined.to_csv(input_dir + \"MetabolomicsResults/scoredSpecDB.csv\")\n",
    "    return(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aba5416d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scoring_spec(input_dir = \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/\",\n",
    "#             spec_file = \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/MetabolomicsResults/SD_post_processed_combined_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "188fe433",
   "metadata": {},
   "outputs": [],
   "source": [
    "##scoring_spec(input_dir = \"/Users/mahnoorzulfiqar/Downloads/MAW-main/\",\n",
    " #            spec_file = \"/Users/mahnoorzulfiqar/Downloads/MAW-main/MetabolomicsResults/SD_post_processed_combined_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "406fd6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(scoring_spec.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adc81a7",
   "metadata": {},
   "source": [
    "### Suspect List Screening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff1df5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suspectListScreening(input_dir, slistcsv, SpectralDB_Results, db = \"all\"):\n",
    "    \n",
    "    \"\"\"suspectListScreening runs tanoimoto similarity score to between\n",
    "    compounds from the results from spectral DBs and suspect list\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "    slistcsv (str): path to suspect list\n",
    "    SpectralDB_Results (dataframe): dataframe from scoring_spec\n",
    "    db(str): can be all, gnps, mbank, hmdb, gm, hg, hm\n",
    "    \n",
    "    Returns:\n",
    "    dataframe: all features and specDB reults and suspect list screening \n",
    "    results\n",
    "    csv: CSV reuslt file named MetabolomicsResults/SpecDBvsSL.csv\n",
    "    which contains all the features and their Spec DB annotations\n",
    "    and suspect list occurences if any\n",
    "    \n",
    "    Usage:\n",
    "    suspectListScreening(input_dir = \"usr/project/\",\n",
    "    slistcsv = \"usr/project/suspect_list.csv\", \n",
    "    SpectralDB_Results)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    SpectralDB_Results = pd.read_csv(SpectralDB_Results)\n",
    "    Suspect_list = pd.read_csv(slistcsv)\n",
    "    \n",
    "    def isNaN(string):\n",
    "        return string != string\n",
    "    if db == \"hmdb\" or db == \"hm\" or db == \"hg\" or db == \"all\":\n",
    "        \n",
    "        # add columns to the result from scoring_spec\n",
    "        # these columns are for high similiarity canidtes between the databases and suspect list\n",
    "        SpectralDB_Results['HLsmiles'] = np.nan\n",
    "        SpectralDB_Results['HLname'] = np.nan\n",
    "\n",
    "        for i, row in SpectralDB_Results.iterrows():\n",
    "            if not isNaN(SpectralDB_Results['HMDBSMILES'][i]) and SpectralDB_Results['HMDBSMILES'][i] != \" \":\n",
    "                for j, row in Suspect_list.iterrows():\n",
    "                    LHms2 = [Chem.MolFromSmiles(SpectralDB_Results['HMDBSMILES'][i]), Chem.MolFromSmiles(Suspect_list['SMILES'][j])]\n",
    "                    LHfps2 = [AllChem.GetMorganFingerprintAsBitVect(x2,2, nBits=2048) for x2 in LHms2]\n",
    "                    LHtn2 = DataStructs.FingerprintSimilarity(LHfps2[0],LHfps2[1])\n",
    "                    if LHtn2 >= 0.9:\n",
    "                        SpectralDB_Results.loc[i, 'HLsmiles'] = Suspect_list['SMILES'][j]\n",
    "                        SpectralDB_Results.loc[i, 'HLname'] = Suspect_list['Name'][j]\n",
    "\n",
    "        # add annotations and occurences\n",
    "        for i, row in SpectralDB_Results.iterrows():\n",
    "            if not isNaN(SpectralDB_Results['HLname'][i]):\n",
    "                SpectralDB_Results['occurence'][i] = SpectralDB_Results['occurence'][i] + 1\n",
    "                if SpectralDB_Results['annotation'][i] == \"none\":\n",
    "                    SpectralDB_Results['annotation'][i] = 'Suspect_List'\n",
    "                else:\n",
    "                    SpectralDB_Results['annotation'][i] = SpectralDB_Results['annotation'][i] + ', Suspect_List'\n",
    "    \n",
    "    if db == \"gnps\" or db == \"gm\" or db == \"hg\" or db == \"all\":\n",
    "\n",
    "        # add columns to the result from scoring_spec\n",
    "        # these columns are for high similiarity canidtes between the databases and suspect list\n",
    "        SpectralDB_Results['GLsmiles'] = np.nan\n",
    "        SpectralDB_Results['GLname'] = np.nan\n",
    "\n",
    "\n",
    "        for i, row in SpectralDB_Results.iterrows():\n",
    "\n",
    "            if not isNaN(SpectralDB_Results['GNPSSMILES'][i]) and SpectralDB_Results['GNPSSMILES'][i] != \" \":\n",
    "                for k, row in Suspect_list.iterrows():\n",
    "                    LGms2 = [Chem.MolFromSmiles(SpectralDB_Results['GNPSSMILES'][i]), Chem.MolFromSmiles(Suspect_list['SMILES'][k])]\n",
    "                    LGfps2 = [AllChem.GetMorganFingerprintAsBitVect(x2,2, nBits=2048) for x2 in LGms2]\n",
    "                    LGtn2 = DataStructs.FingerprintSimilarity(LGfps2[0],LGfps2[1])\n",
    "                    if LGtn2 >= 0.9:\n",
    "                        SpectralDB_Results.loc[i, 'GLsmiles'] = Suspect_list['SMILES'][k]\n",
    "                        SpectralDB_Results.loc[i, 'GLname'] = Suspect_list['Name'][k]\n",
    "        # add annotations and occurences\n",
    "        for i, row in SpectralDB_Results.iterrows():\n",
    "            if not isNaN(SpectralDB_Results['GLname'][i]):\n",
    "                SpectralDB_Results['occurence'][i] = SpectralDB_Results['occurence'][i] + 1\n",
    "                if SpectralDB_Results['annotation'][i] == \"none\":\n",
    "                    SpectralDB_Results['annotation'][i] = 'Suspect_List'\n",
    "                else:\n",
    "                    SpectralDB_Results['annotation'][i] = SpectralDB_Results['annotation'][i] + ', Suspect_List'\n",
    "    \n",
    "    if db == \"mbank\" or db == \"gm\" or db == \"hm\" or db == \"all\":\n",
    "\n",
    "        # add columns to the result from scoring_spec\n",
    "        # these columns are for high similiarity canidtes between the databases and suspect list\n",
    "        SpectralDB_Results['MLsmiles'] = np.nan\n",
    "        SpectralDB_Results['MLname'] = np.nan\n",
    "\n",
    "\n",
    "        for i, row in SpectralDB_Results.iterrows():\n",
    "            if not isNaN(SpectralDB_Results['MBSMILES'][i]) and SpectralDB_Results['MBSMILES'][i] != \" \":\n",
    "                for l, row in Suspect_list.iterrows():\n",
    "                    LMms2 = [Chem.MolFromSmiles(SpectralDB_Results['MBSMILES'][i]), Chem.MolFromSmiles(Suspect_list['SMILES'][l])]\n",
    "                    LMfps2 = [AllChem.GetMorganFingerprintAsBitVect(x2,2, nBits=2048) for x2 in LMms2]\n",
    "                    LMtn2 = DataStructs.FingerprintSimilarity(LMfps2[0],LMfps2[1])\n",
    "                    if LMtn2 >= 0.9:\n",
    "                        SpectralDB_Results.loc[i, 'MLsmiles'] = Suspect_list['SMILES'][l]\n",
    "                        SpectralDB_Results.loc[i, 'MLname'] = Suspect_list['Name'][l]\n",
    "\n",
    "        # add annotations and occurences\n",
    "        for i, row in SpectralDB_Results.iterrows():\n",
    "            if not isNaN(SpectralDB_Results['MLname'][i]):\n",
    "                SpectralDB_Results['occurence'][i] = SpectralDB_Results['occurence'][i] + 1\n",
    "                if SpectralDB_Results['annotation'][i] == \"none\":\n",
    "                    SpectralDB_Results['annotation'][i] = 'Suspect_List'\n",
    "                else:\n",
    "                    SpectralDB_Results['annotation'][i] = SpectralDB_Results['annotation'][i] + ', Suspect_List'\n",
    "                \n",
    "    SpectralDB_Results.to_csv(input_dir + \"MetabolomicsResults/SpecDBvsSL.csv\")\n",
    "    return(SpectralDB_Results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "764e8b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#suspectListScreening(input_dir = \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/\", \n",
    "                     #slistcsv = \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/SkeletonemaSuspectListV1.csv\", \n",
    "                     #SpectralDB_Results = \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/MetabolomicsResults/scoredSpecDB.csv\", \n",
    "                     #db = \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2cb634f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#suspectListScreening(input_dir = \"/Users/mahnoorzulfiqar/Downloads/MAW-main/\", \n",
    "                     #slistcsv = \"/Users/mahnoorzulfiqar/Downloads/MAW-main/SkeletonemaSuspectListV1.csv\", \n",
    "                     ##SpectralDB_Results = \"/Users/mahnoorzulfiqar/Downloads/MAW-main/MetabolomicsResults/scoredSpecDB.csv\", \n",
    "                     #db = \"gm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eb9290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a4a29d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(suspectListScreening.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82dbf13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce884025",
   "metadata": {},
   "source": [
    "# Final Candidate List Curation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e45b773",
   "metadata": {},
   "source": [
    "## MetFrag Curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc97b626",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e0e33527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metfrag_curation(input_dir, metfragcsv, sl = True):\n",
    "    def isNaN(string):\n",
    "        return string != string\n",
    "    \n",
    "    \"\"\"metfrag_curation checks which database produced results. If both \n",
    "    did, it checks whether it was the same compound as candidate, if not,\n",
    "    add PubChem or any of the two databases with similarity to Suspect\n",
    "    list\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "    metfragcsv (str): path to combined metfrag results:\n",
    "    MetabolomicsResults/MetFrag_combined.csv\n",
    "    \n",
    "    Returns:\n",
    "    dataframe: dataframe of curated metfrag results\n",
    "    csv: MetabolomicsResults/metfrag_curated.csv\n",
    "    \n",
    "    Usage:\n",
    "    metfrag_curation(input_dir = \"usr/project/\", \n",
    "    metfragcsv = \"usr/project/MetabolomicsResults/MetFrag_combined.csv\")\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    metfrag = pd.read_csv(metfragcsv)\n",
    "    for i, row in metfrag.iterrows():\n",
    "        \n",
    "        \n",
    "        # If only KEGG\n",
    "        if not isNaN(metfrag['KG_SMILES'][i]) and isNaN(metfrag['PC_SMILES'][i]):\n",
    "            metfrag.loc[i, 'Annotation_M'] = 'KEGG'\n",
    "            if sl:\n",
    "                if metfrag['KGSL_Score'][i]>=0.9:\n",
    "                    metfrag.loc[i, 'Annotation_M'] = 'KEGG, SuspectList'\n",
    "                else:\n",
    "                    metfrag.loc[i, 'Annotation_M'] = 'KEGG'\n",
    "    \n",
    "        # If only Pubchem\n",
    "        if not isNaN(metfrag['PC_SMILES'][i]) and isNaN(metfrag['KG_SMILES'][i]):\n",
    "            metfrag.loc[i, 'Annotation_M'] = 'PubChem'\n",
    "            if sl:\n",
    "                if metfrag['PCSL_Score'][i]>=0.9:\n",
    "                    metfrag.loc[i, 'Annotation_M'] = 'PubChem, SuspectList'\n",
    "                else:\n",
    "                    metfrag.loc[i, 'Annotation_M'] = 'PubChem'           \n",
    "        \n",
    "    \n",
    "        # If both, calculate the similarity\n",
    "        if not isNaN(metfrag['PC_SMILES'][i]) and not isNaN(metfrag['KG_SMILES'][i]):\n",
    "        \n",
    "            PKms = [Chem.MolFromSmiles(metfrag['KG_SMILES'][i]), Chem.MolFromSmiles(metfrag['PC_SMILES'][i])]\n",
    "            PKfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in PKms]\n",
    "            PKtn = DataStructs.FingerprintSimilarity(PKfps[0],PKfps[1])\n",
    "        \n",
    "            # if both are similar, add both\n",
    "            if PKtn == 1:\n",
    "                metfrag.loc[i, 'Annotation_M'] = 'KEGG, PubChem'\n",
    "                if sl:\n",
    "                    if metfrag['KGSL_Score'][i]>=0.9 and metfrag['PCSL_Score'][i]>=0.9:\n",
    "                        metfrag.loc[i, 'Annotation_M'] = metfrag['Annotation_M'][i] + \", SuspectList\"\n",
    "        \n",
    "            # if not similar:\n",
    "            # check Suspect list score and Fragmenter Score\n",
    "            \n",
    "            else:\n",
    "                if not isNaN(metfrag[\"KG_Score\"][i]):\n",
    "                    metfrag.loc[i, 'Annotation_M'] = 'KEGG'\n",
    "                else:\n",
    "                    metfrag.loc[i, 'Annotation_M'] = 'PubChem'\n",
    "                    \n",
    "                                \n",
    "    metfrag.to_csv(input_dir + \"MetabolomicsResults/metfrag_curated.csv\")  \n",
    "    return(metfrag)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aa2dca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(metfrag_curation.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e1baff43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#metfrag_curation(input_dir = \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/\",\n",
    "                 ##metfragcsv = \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/MetabolomicsResults/MetFrag_combined.csv\", \n",
    "                 #sl = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bb7a26",
   "metadata": {},
   "source": [
    "## SIRIUS Results Curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b4f07a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#metfrag_curation(input_dir = \"/Users/mahnoorzulfiqar/Downloads/MAW-main/\", \n",
    "##                 metfragcsv = \"/Users/mahnoorzulfiqar/Downloads/MAW-main/MetabolomicsResults/MetFrag_combined.csv\", \n",
    "#                 sl = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "73654ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sirius_curation(input_dir, siriuscsv, sl = True):\n",
    "    def isNaN(string):\n",
    "        return string != string\n",
    "    \"\"\"sirius_curation checks if candidate selected has a good score for \n",
    "    explained intensity. It also checks if there was any similarity to\n",
    "    a compound from Suspect list\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "    siriuscsv (str): path to combined metfrag results:\n",
    "    MetabolomicsResults/Sirius_combined.csv\n",
    "    \n",
    "    Returns:\n",
    "    dataframe: dataframe of curated sirius results\n",
    "    csv: MetabolomicsResults/sirius_curated.csv\n",
    "    \n",
    "    Usage:\n",
    "    sirius_curation(input_dir = \"usr/project/\", \n",
    "    siriuscsv = \"usr/project/MetabolomicsResults/Sirius_combined.csv\")\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    sirius = pd.read_csv(siriuscsv)\n",
    "    for i, row in sirius.iterrows():\n",
    "    \n",
    "        # If the explained intensity is greater than 0.70 and there is no suspect list entry\n",
    "        if sirius['exp_int'][i] >= 0.70 and \"SIRIUS_SL\" not in sirius['Result'][i]:\n",
    "            sirius.loc[i, 'Annotation_S'] = 'SIRIUS'\n",
    "            #sirius.loc[i, 'SMILES_final'] = sirius['SMILES'][i]\n",
    "        else:\n",
    "            if sl:\n",
    "                \n",
    "                #If the explained intensity is greater than 0.70 and there is an entry from suspect list\n",
    "                if sirius['exp_int'][i] >= 0.70 and \"SIRIUS_SL\" in sirius['Result'][i]:\n",
    "                    sirius.loc[i, 'Annotation_S'] = 'SIRIUS, SuspectList'\n",
    "                    #sirius.loc[i, 'SMILES_final'] = sirius['SMILES'][i]\n",
    "    \n",
    "                # if the intensity is less thna 0.70 but it still is similar to an entry in Suspect list,\n",
    "                elif sirius['exp_int'][i] < 0.70 and \"SIRIUS_SL\" in sirius['Result'][i]:\n",
    "                    sirius.loc[i, 'Annotation_S'] = 'SIRIUS, SuspectList'\n",
    "                    #sirius.loc[i, 'SMILES_final'] = sirius['SMILES'][i]\n",
    "        \n",
    "    sirius.to_csv(input_dir + \"MetabolomicsResults/sirius_curated.csv\")\n",
    "    return(sirius)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "80945399",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sirius_curation.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e3454398",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sirius_curation(input_dir = \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/\",\n",
    " #                siriuscsv = \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/MetabolomicsResults/SIRIUS_combined.csv\", \n",
    " #                sl = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b7d4cfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sirius_curation(input_dir = \"/Users/mahnoorzulfiqar/Downloads/MAW-main/\", \n",
    "              #   siriuscsv = \"/Users/mahnoorzulfiqar/Downloads/MAW-main/MetabolomicsResults/SIRIUS_combined.csv\", \n",
    "              #   sl = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2ff69c",
   "metadata": {},
   "source": [
    "## combine curated S and M results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8ac2db42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineSM(input_dir, metfragcsv, siriuscsv):\n",
    "    \n",
    "    \"\"\"combineSM prioritizes Sirius and Suspect list over PubChem and\n",
    "    KEGG\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "    sirius (dataframe): result of sirius_curation\n",
    "    metfrag (dataframe): result of metfrag_curation\n",
    "    \n",
    "    Returns:\n",
    "    dataframe: dataframe of combined curated sirius and metfrag results\n",
    "    csv: \"MetabolomicsResults/combinedSM.csv\"\n",
    "    \n",
    "    Usage:\n",
    "    combineSM(input_dir = \"usr/project/\", metfrag, sirius)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def isNaN(string):\n",
    "        return string != string\n",
    "    \n",
    "    metfrag = pd.read_csv(metfragcsv)\n",
    "    sirius = pd.read_csv(siriuscsv)\n",
    "    S_M_CSV = pd.concat([sirius, metfrag], axis = 1, levels = [\"id_X\"])\n",
    "    \n",
    "    for i, rows in S_M_CSV.iterrows():\n",
    "        # if results has Sirius Structure annotation, and the explained inetnsity is >= 0.70, keep the annotation as is.\n",
    "        if S_M_CSV[\"Result\"][i] == \"SIRIUS_STR\" and S_M_CSV['exp_int'][i] >= 0.70:\n",
    "            S_M_CSV.loc[i, 'Annotation_C'] = S_M_CSV['Annotation_S'][i]\n",
    "            \n",
    "            # to add to that annotation\n",
    "            if not isNaN(S_M_CSV[\"Annotation_M\"][i]):\n",
    "                # if annotation has PubChem, by default add SIRIUS\n",
    "                if S_M_CSV[\"Annotation_M\"][i] == \"KEGG\":\n",
    "                    SKms = [Chem.MolFromSmiles(S_M_CSV['SMILES'][i]), Chem.MolFromSmiles(S_M_CSV['KG_SMILES'][i])]\n",
    "                    SKfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in SKms]\n",
    "                    SKtn = DataStructs.FingerprintSimilarity(SKfps[0],SKfps[1])\n",
    "\n",
    "                    if SKtn >= 0.75:\n",
    "\n",
    "                        S_M_CSV.loc[i, 'Annotation_C'] = S_M_CSV['Annotation_S'][i] +', KEGG'\n",
    "\n",
    "                    else:\n",
    "                        S_M_CSV.loc[i, 'Annotation_C'] = S_M_CSV['Annotation_S'][i]\n",
    "                        \n",
    "                # if annotation has PubChem, by default add SIRIUS\n",
    "                if S_M_CSV[\"Annotation_M\"][i] == \"PubChem\":\n",
    "                    PSms = [Chem.MolFromSmiles(S_M_CSV['SMILES'][i]), Chem.MolFromSmiles(S_M_CSV['PC_SMILES'][i])]\n",
    "                    PSfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in PSms]\n",
    "                    PStn = DataStructs.FingerprintSimilarity(PSfps[0],PSfps[1])\n",
    "\n",
    "                    # if similar strcutres, then add Pubchme and sirius\n",
    "                    if PStn >= 0.7:\n",
    "\n",
    "                        S_M_CSV.loc[i, 'Annotation_C'] = S_M_CSV['Annotation_S'][i] + ', PubChem'\n",
    "\n",
    "                    # if not then just keep sirius\n",
    "                    else:\n",
    "                        S_M_CSV.loc[i, 'Annotation_C'] = S_M_CSV['Annotation_S'][i]\n",
    "                        \n",
    "                        \n",
    "                if S_M_CSV[\"Annotation_M\"][i] == \"KEGG, PubChem\":\n",
    "                    SKms = [Chem.MolFromSmiles(S_M_CSV['SMILES'][i]), Chem.MolFromSmiles(S_M_CSV['KG_SMILES'][i])]\n",
    "                    SKfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in SKms]\n",
    "                    SKtn = DataStructs.FingerprintSimilarity(SKfps[0],SKfps[1])\n",
    "                    if SKtn >= 0.7:\n",
    "\n",
    "                        S_M_CSV.loc[i, 'Annotation_C'] = S_M_CSV['Annotation_S'][i] +', KEGG, PubChem'\n",
    "\n",
    "                    else:\n",
    "                        S_M_CSV.loc[i, 'Annotation_C'] = S_M_CSV['Annotation_S'][i]\n",
    "    S_M_CSV.to_csv(input_dir + \"MetabolomicsResults/combinedSM.csv\")\n",
    "    return(S_M_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3c04409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(combineSM.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "db5d8624",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combineSM(input_dir = \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/\",\n",
    "#          metfragcsv = \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/MetabolomicsResults/metfrag_curated.csv\", \n",
    " #         siriuscsv = \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/MetabolomicsResults/sirius_curated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9d60be8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#combineSM(input_dir = \"/Users/mahnoorzulfiqar/Downloads/MAW-main/\",\n",
    " #         metfragcsv = \"/Users/mahnoorzulfiqar/Downloads/MAW-main/MetabolomicsResults/metfrag_curated.csv\", \n",
    " #         siriuscsv = \"/Users/mahnoorzulfiqar/Downloads/MAW-main/MetabolomicsResults/sirius_curated.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28372129",
   "metadata": {},
   "source": [
    "## Spec DB Curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b43fb2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def specDB_Curation(input_dir, combinedx, sl = True, db = \"all\"):\n",
    "    \n",
    "    \"\"\"specDB_Curation prioritizes in the following manner: gnps>\n",
    "    mbank>suspectlist>hmdb\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "    \n",
    "    combined: dataframe from either suspectListScreening function if\n",
    "    sl = True OR from scoring_spec if sl = False\n",
    "    \n",
    "    Returns:\n",
    "    dataframe: with curated Spectral DB results\n",
    "    csv: \"MetabolomicsResults/curatedSDB.csv\"\n",
    "    \n",
    "    Usage:\n",
    "    specDB_Curation(input_dir = \"usr/project/\",combinedx, sl = True)\n",
    "\n",
    "    \"\"\"\n",
    "    def isNaN(string):\n",
    "        return string != string\n",
    "    def HMDB_Scoring(db, i):\n",
    "        if db['HMDBmax_similarity'][i] >= 0.75 and db['HMDBintScore'][i] >= 0.50 and db['HMDBmzScore'][i] >= 0.50 and db['HQMatchingPeaks'][i]/db['hQueryTotalPeaks'][i] >= 0.50:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def GNPS_Scoring(db, i):\n",
    "        if db['GNPSmax_similarity'][i] >= 0.90 and db['GNPSintScore'][i] >= 0.50 and db['GNPSmzScore'][i] >= 0.50 and db['GQMatchingPeaks'][i]/db['gQueryTotalPeaks'][i] >= 0.50:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    \n",
    "    def MB_Scoring(db, i):\n",
    "        if db['MBmax_similarity'][i] >= 0.50 and db['MBintScore'][i] >= 0.50 and db['MBmzScore'][i] >= 0.50 and db['MQMatchingPeaks'][i]/db['mQueryTotalPeaks'][i] >= 0.50:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    combined = pd.read_csv(combinedx)\n",
    "    \n",
    "    \n",
    "    # remove the similarity scores from low scoring candidates\n",
    "    for i, row in combined.iterrows():\n",
    "        if db == \"all\" or db == \"hg\" or db == \"hm\" or db == \"hmdb\":\n",
    "            if not HMDB_Scoring(combined, i):\n",
    "                combined['HMDBcompoundID'][i] = np.nan\n",
    "        if db == \"all\" or db == \"hg\" or db == \"gm\" or db == \"gnps\":\n",
    "            if not GNPS_Scoring(combined, i):\n",
    "                combined['GNPSspectrumID'][i] = np.nan\n",
    "        if db == \"all\" or db == \"gm\" or db == \"hm\" or db == \"mbank\":\n",
    "            if not MB_Scoring(combined, i):\n",
    "                combined['MBspectrumID'][i] = np.nan\n",
    "    \n",
    "    # if sl = True\n",
    "    if sl:\n",
    "        for i, row in combined.iterrows():\n",
    "            # if all databases are used to generate results\n",
    "            if db == \"all\":\n",
    "                \n",
    "                # if all dbs have results\n",
    "                if not isNaN(combined['GNPSspectrumID'][i]) and not isNaN(combined['MBspectrumID'][i]) and not isNaN(combined['HMDBcompoundID'][i]):\n",
    "        \n",
    "                    # entries with same candidate from all Spectral DBs\n",
    "                    if combined['tanimotoHG'][i] == 1.0 and combined['tanimotoGM'][i] == 1.0 and combined['tanimotoHM'][i] == 1.0:\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS, HMDB, MassBank'\n",
    "                        #entries with same candidate in suspect list, as in all Spectral DBs\n",
    "                        if combined['GLname'][i] == combined['HLname'][i]== combined['MLname'][i]:\n",
    "                            combined.loc[i, 'Annotation'] = 'GNPS, HMDB, MassBank, SuspectList'\n",
    "                \n",
    "                    # same candidate from GNPS and HMDB        \n",
    "                    if combined['tanimotoHG'][i] == 1.0 and combined['tanimotoGM'][i] != 1.0 and combined['tanimotoHM'][i] != 1.0:\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS, HMDB'\n",
    "                        # if its present in Suspect List\n",
    "                        if combined['GLname'][i] == combined['HLname'][i]:\n",
    "                            combined.loc[i, 'Annotation'] = 'GNPS, HMDB, SuspectList'\n",
    "        \n",
    "                    # same candidate from GNPS and MassBank        \n",
    "                    if combined['tanimotoHG'][i] != 1.0 and combined['tanimotoGM'][i] == 1.0 and combined['tanimotoHM'][i] != 1.0:\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS, MassBank'\n",
    "                        # if its present in Suspect List\n",
    "                        if combined['GLname'][i] == combined['MLname'][i]:\n",
    "                            combined.loc[i, 'Annotation'] = 'GNPS, MassBank, SuspectList'\n",
    "                \n",
    "                    # same candidate from MassBank and HMDB        \n",
    "                    if combined['tanimotoHG'][i] != 1.0 and combined['tanimotoGM'][i] != 1.0 and combined['tanimotoHM'][i] == 1.0:\n",
    "                        combined.loc[i, 'Annotation'] = 'MassBank, HMDB'\n",
    "                        # if its present in Suspect List\n",
    "                        if combined['MLname'][i] == combined['HLname'][i]:\n",
    "                            combined.loc[i, 'Annotation'] = 'HMDB, MassBank, SuspectList'\n",
    "                    \n",
    "                    # only one database must be selected based on SuspectList annotation\n",
    "                    if combined['tanimotoHG'][i] != 1.0 and combined['tanimotoGM'][i] != 1.0 and combined['tanimotoHM'][i] != 1.0:\n",
    "            \n",
    "                        # only GNPS has SuspectList annotation\n",
    "                        if not isNaN(combined['GLname'][i]):\n",
    "\n",
    "                            combined.loc[i, 'Annotation'] = 'GNPS, SuspectList'\n",
    "            \n",
    "            \n",
    "                        # only MassBank has SuspectList annotation\n",
    "                        elif not isNaN(combined['MLname'][i]):\n",
    "                            combined.loc[i, 'Annotation'] = 'MassBank, SuspectList'\n",
    "            \n",
    "            \n",
    "                        # only HMDB has SuspectList annotation\n",
    "                        #elif not isNaN(combined['HLname'][i]):\n",
    "                            #combined.loc[i, 'Annotation'] = 'HMDB, SuspectList'\n",
    "            \n",
    "        \n",
    "                        # all different annotations, take GNPS\n",
    "                        else:\n",
    "                            if not isNaN(combined['GNPSSMILES'][i]):\n",
    "                                combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "                            else:\n",
    "                                combined.loc[i, 'Annotation'] = 'MassBank'\n",
    "    \n",
    "                #### When there is an annotation from two DBs #####\n",
    "\n",
    "                # only GNPS and HMDB\n",
    "                if not isNaN(combined['GNPSspectrumID'][i]) and isNaN(combined['MBspectrumID'][i]) and not isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    if combined['tanimotoHG'][i] == 1.0:\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS, HMDB'\n",
    "                        if not isNaN(combined['GLname'][i]) and not isNaN(combined['HLname'][i]):\n",
    "                            if combined['GLname'][i] == combined['HLname'][i]:\n",
    "                                combined.loc[i, 'Annotation'] = 'GNPS, HMDB, SuspectList'\n",
    "                    else:\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "                        if not isNaN(combined['GLname'][i]):\n",
    "                            combined.loc[i, 'Annotation'] = 'GNPS, SuspectList'\n",
    "                        #elif not isNaN(combined['HLname'][i]):\n",
    "                            #combined.loc[i, 'Annotation'] = 'HMDB, SuspectList'\n",
    "\n",
    "\n",
    "                # only GNPS and MassBank\n",
    "                if not isNaN(combined['GNPSspectrumID'][i]) and not isNaN(combined['MBspectrumID'][i]) and isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    if combined['tanimotoGM'][i] == 1.0:\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS, MassBank'\n",
    "                        if not isNaN(combined['GLname'][i]) and not isNaN(combined['MLname'][i]):\n",
    "                            if combined['GLname'][i] == combined['MLname'][i]:\n",
    "                                combined.loc[i, 'Annotation'] = 'GNPS, MassBank, SuspectList'\n",
    "                                \n",
    "                    else:\n",
    "                        if not isNaN(combined['GLname'][i]):\n",
    "                            combined.loc[i, 'Annotation'] = 'GNPS, SuspectList'\n",
    "                        elif not isNaN(combined['MLname'][i]):\n",
    "                            combined.loc[i, 'Annotation'] = 'MassBank, SuspectList'\n",
    "                        elif not isNaN(combined['GNPSSMILES'][i]):\n",
    "                            combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "                        else:\n",
    "                            combined.loc[i, 'Annotation'] = 'MassBank'\n",
    "\n",
    "                # only MassBank and HMDB\n",
    "\n",
    "                if isNaN(combined['GNPSspectrumID'][i]) and not isNaN(combined['MBspectrumID'][i]) and not isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    if combined['tanimotoHM'][i] == 1.0:\n",
    "                        combined.loc[i, 'Annotation'] = 'HMDB, MassBank'\n",
    "                        if not isNaN(combined['HLname'][i]) and not isNaN(combined['MLname'][i]):\n",
    "                            if combined['HLname'][i] == combined['MLname'][i]:\n",
    "                                combined.loc[i, 'Annotation'] = 'HMDB, MassBank, SuspectList'\n",
    "                                \n",
    "                    else:\n",
    "                        if not isNaN(combined['MLname'][i]):\n",
    "                            combined.loc[i, 'Annotation'] = 'MassBank, SuspectList'\n",
    "                        #elif not isNaN(combined['MLname'][i]):\n",
    "                            #combined.loc[i, 'Annotation'] = 'MassBank, SuspectList'\n",
    "                        #elif not isNaN(combined['GNPSSMILES'][i]):\n",
    "                            #combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "                        else:\n",
    "                            combined.loc[i, 'Annotation'] = 'MassBank'\n",
    "\n",
    "\n",
    "\n",
    "                ##### When there is an annotation from one DBs #####\n",
    "\n",
    "\n",
    "                # only GNPS\n",
    "                if not isNaN(combined['GNPSspectrumID'][i]) and isNaN(combined['MBspectrumID'][i]) and isNaN(combined['HMDBcompoundID'][i]):\n",
    "\n",
    "                    #If also SuspectList\n",
    "                    if not isNaN(combined['GLname'][i]):\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS, SuspectList'\n",
    "                    elif not isNaN(combined['GNPSSMILES'][i]):\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "\n",
    "                # only MassBank\n",
    "                if isNaN(combined['GNPSspectrumID'][i]) and not isNaN(combined['MBspectrumID'][i]) and isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    combined.loc[i, 'Annotation'] = 'MassBank'\n",
    "                    #If also SuspectList\n",
    "                    if not isNaN(combined['MLname'][i]):\n",
    "                        combined.loc[i, 'Annotation'] = 'MassBank, SuspectList'\n",
    "\n",
    "                # only HMDB\n",
    "                #if isNaN(combined['GNPSspectrumID'][i]) and isNaN(combined['MBspectrumID'][i]) and not isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    #combined.loc[i, 'Annotation'] = 'HMDB'\n",
    "                    #If also SuspectList\n",
    "                    #if not isNaN(combined['HLname'][i]):\n",
    "                        #combined.loc[i, 'Annotation'] = 'HMDB, SuspectList'\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "            \n",
    "            \n",
    "            # if GNPS AND MassBank databases are used to generate results\n",
    "            if db == \"gm\":\n",
    "                \n",
    "                # only GNPS and MassBank\n",
    "                if not isNaN(combined['GNPSspectrumID'][i]) and not isNaN(combined['MBspectrumID'][i]):\n",
    "                    if combined['tanimotoGM'][i] == 1.0:\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS, MassBank'\n",
    "                        if not isNaN(combined['GLname'][i]) and not isNaN(combined['MLname'][i]):\n",
    "                            if combined['GLname'][i] == combined['MLname'][i]:\n",
    "                                combined.loc[i, 'Annotation'] = 'GNPS, MassBank, SuspectList'\n",
    "                                \n",
    "                    else:\n",
    "                        if not isNaN(combined['GLname'][i]):\n",
    "                            combined.loc[i, 'Annotation'] = 'GNPS, SuspectList'\n",
    "                        elif not isNaN(combined['MLname'][i]):\n",
    "                            combined.loc[i, 'Annotation'] = 'MassBank, SuspectList'\n",
    "                        elif not isNaN(combined['GNPSSMILES'][i]):\n",
    "                            combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "                        else:\n",
    "                            combined.loc[i, 'Annotation'] = 'MassBank'\n",
    "\n",
    "                \n",
    "                \n",
    "                # only GNPS\n",
    "                if not isNaN(combined['GNPSspectrumID'][i]) and isNaN(combined['MBspectrumID'][i]):\n",
    "\n",
    "                    #If also SuspectList\n",
    "                    if not isNaN(combined['GLname'][i]):\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS, SuspectList'\n",
    "                    elif not isNaN(combined['GNPSSMILES'][i]):\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "\n",
    "                # only MassBank\n",
    "                if isNaN(combined['GNPSspectrumID'][i]) and not isNaN(combined['MBspectrumID'][i]):\n",
    "                    combined.loc[i, 'Annotation'] = 'MassBank'\n",
    "                    #If also SuspectList\n",
    "                    if not isNaN(combined['MLname'][i]):\n",
    "                        combined.loc[i, 'Annotation'] = 'MassBank, SuspectList'\n",
    "            \n",
    "            \n",
    "            \n",
    "            # if GNPS AND HMDB databases are used to generate results\n",
    "            if db == \"hg\":\n",
    "                \n",
    "                # only GNPS and HMDB\n",
    "                if not isNaN(combined['GNPSspectrumID'][i]) and not isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    if combined['tanimotoHG'][i] == 1.0:\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS, HMDB'\n",
    "                        if not isNaN(combined['GLname'][i]) and not isNaN(combined['HLname'][i]):\n",
    "                            if combined['GLname'][i] == combined['HLname'][i]:\n",
    "                                combined.loc[i, 'Annotation'] = 'GNPS, HMDB, SuspectList'\n",
    "                    else:\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "                        if not isNaN(combined['GLname'][i]):\n",
    "                            combined.loc[i, 'Annotation'] = 'GNPS, SuspectList'\n",
    "                        #elif not isNaN(combined['HLname'][i]):\n",
    "                            #combined.loc[i, 'Annotation'] = 'HMDB, SuspectList'\n",
    "\n",
    "                # only GNPS\n",
    "                if not isNaN(combined['GNPSspectrumID'][i]) and isNaN(combined['HMDBcompoundID'][i]):\n",
    "\n",
    "                    #If also SuspectList\n",
    "                    if not isNaN(combined['GLname'][i]):\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS, SuspectList'\n",
    "                    elif not isNaN(combined['GNPSSMILES'][i]):\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "                # only HMDB\n",
    "                #if isNaN(combined['GNPSspectrumID'][i]) and not isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    #combined.loc[i, 'Annotation'] = 'HMDB'\n",
    "                    #If also SuspectList\n",
    "                    #if not isNaN(combined['HLname'][i]):\n",
    "                        #combined.loc[i, 'Annotation'] = 'HMDB, SuspectList'\n",
    "            \n",
    "            # if MassBank AND HMDB databases are used to generate results\n",
    "            if db == \"hm\":\n",
    "                \n",
    "                # only MassBank and HMDB\n",
    "\n",
    "                if not isNaN(combined['MBspectrumID'][i]) and not isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    if combined['tanimotoHM'][i] == 1.0:\n",
    "                        combined.loc[i, 'Annotation'] = 'HMDB, MassBank'\n",
    "                        if not isNaN(combined['HLname'][i]) and not isNaN(combined['MLname'][i]):\n",
    "                            if combined['HLname'][i] == combined['MLname'][i]:\n",
    "                                combined.loc[i, 'Annotation'] = 'HMDB, MassBank, SuspectList'\n",
    "                                \n",
    "                    else:\n",
    "                        if not isNaN(combined['MLname'][i]):\n",
    "                            combined.loc[i, 'Annotation'] = 'MassBank, SuspectList'\n",
    "                        else:\n",
    "                            combined.loc[i, 'Annotation'] = 'MassBank'\n",
    "                \n",
    "                \n",
    "                \n",
    "                # only MassBank\n",
    "                if not isNaN(combined['MBspectrumID'][i]) and isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    combined.loc[i, 'Annotation'] = 'MassBank'\n",
    "                    #If also SuspectList\n",
    "                    if not isNaN(combined['MLname'][i]):\n",
    "                        combined.loc[i, 'Annotation'] = 'MassBank, SuspectList'\n",
    "\n",
    "                # only HMDB\n",
    "                #if isNaN(combined['MBspectrumID'][i]) and not isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    #combined.loc[i, 'Annotation'] = 'HMDB'\n",
    "                    #If also SuspectList\n",
    "                    #if not isNaN(combined['HLname'][i]):\n",
    "                        #combined.loc[i, 'Annotation'] = 'HMDB, SuspectList'\n",
    "            if db == \"gnps\":\n",
    "                if not isNaN(combined['GNPSspectrumID'][i]):\n",
    "\n",
    "                    #If also SuspectList\n",
    "                    if not isNaN(combined['GLname'][i]):\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS, SuspectList'\n",
    "                    elif not isNaN(combined['GNPSSMILES'][i]):\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "            if db == \"mbank\":\n",
    "                # only MassBank\n",
    "                if not isNaN(combined['MBspectrumID'][i]):\n",
    "                    combined.loc[i, 'Annotation'] = 'MassBank'\n",
    "                    #If also SuspectList\n",
    "                    if not isNaN(combined['MLname'][i]):\n",
    "                        combined.loc[i, 'Annotation'] = 'MassBank, SuspectList'\n",
    "            #if db == \"hmdb\":\n",
    "                # only HMDB\n",
    "                #if not isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    #combined.loc[i, 'Annotation'] = 'HMDB'\n",
    "                    #If also SuspectList\n",
    "                    #if not isNaN(combined['HLname'][i]):\n",
    "                        #combined.loc[i, 'Annotation'] = 'HMDB, SuspectList'\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "    else:\n",
    "        for i, row in combined.iterrows():\n",
    "            #if all databases were used\n",
    "            if db == \"all\":\n",
    "                ##### When there is an annotaion from all DBs #####\n",
    "                #all entries with a high scoring annotation in all DBs,\n",
    "                if not isNaN(combined['GNPSspectrumID'][i]) and not isNaN(combined['MBspectrumID'][i]) and not isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    # entries with same candidate from all Spectral DBs\n",
    "                    if combined['tanimotoHG'][i] == 1.0 and combined['tanimotoGM'][i] == 1.0 and combined['tanimotoHM'][i] == 1.0:\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS, HMDB, MassBank'\n",
    "                \n",
    "                    # same candidate from GNPS and HMDB        \n",
    "                    if combined['tanimotoHG'][i] == 1.0 and combined['tanimotoGM'][i] != 1.0 and combined['tanimotoHM'][i] != 1.0:\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS, HMDB'\n",
    "        \n",
    "                    # same candidate from GNPS and MassBank        \n",
    "                    if combined['tanimotoHG'][i] != 1.0 and combined['tanimotoGM'][i] == 1.0 and combined['tanimotoHM'][i] != 1.0:\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS, MassBank'\n",
    "                \n",
    "                    # same candidate from MassBank and HMDB        \n",
    "                    if combined['tanimotoHG'][i] != 1.0 and combined['tanimotoGM'][i] != 1.0 and combined['tanimotoHM'][i] == 1.0:\n",
    "                        combined.loc[i, 'Annotation'] = 'MassBank, HMDB'\n",
    "                \n",
    "                    # all different annotations, take GNPS\n",
    "                    else:\n",
    "                        if not isNaN(combined['GNPSSMILES'][i]):\n",
    "                            combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "                        else:\n",
    "                            combined.loc[i, 'Annotation'] = 'MassBank'\n",
    "                ##### When there is an annotation from two DBs #####\n",
    "    \n",
    "    \n",
    "                # only GNPS and HMDB\n",
    "                if not isNaN(combined['GNPSspectrumID'][i]) and isNaN(combined['MBspectrumID'][i]) and not isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    if combined['tanimotoHG'][i] == 1.0:\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS, HMDB'\n",
    "                    else:\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "                    \n",
    "                    \n",
    "                # only GNPS and MassBank\n",
    "                if not isNaN(combined['GNPSspectrumID'][i]) and not isNaN(combined['MBspectrumID'][i]) and isNaN(combined['HMDBcompoundID'][i]):\n",
    "\n",
    "                    if combined['tanimotoGM'][i] == 1.0:\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS, MassBank'\n",
    "                    else:\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "    \n",
    "                # only MassBank and HMDB\n",
    "                if isNaN(combined['GNPSspectrumID'][i]) and not isNaN(combined['MBspectrumID'][i]) and not isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    if combined['tanimotoHM'][i] == 1.0:\n",
    "                        combined.loc[i, 'Annotation'] = 'HMDB, MassBank'\n",
    "                    else:\n",
    "                        combined.loc[i, 'Annotation'] = 'MassBank'\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                ##### When there is an annotation from one DBs #####\n",
    "    \n",
    "    \n",
    "                # only GNPS\n",
    "                if not isNaN(combined['GNPSspectrumID'][i]) and isNaN(combined['MBspectrumID'][i]) and isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    if not isNaN(combined['GNPSSMILES'][i]):\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "        \n",
    "                # only MassBank\n",
    "                if isNaN(combined['GNPSspectrumID'][i]) and not isNaN(combined['MBspectrumID'][i]) and isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    combined.loc[i, 'Annotation'] = 'MassBank'\n",
    "    \n",
    "                # only HMDB\n",
    "                    #if isNaN(combined['GNPSspectrumID'][i]) and isNaN(combined['MBspectrumID'][i]) and not isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    #combined.loc[i, 'Annotation'] = 'HMDB'\n",
    "                \n",
    "            \n",
    "            #if GNPS and MassBank databases were used\n",
    "            if db == \"gm\":\n",
    "                # only GNPS and MassBank\n",
    "                if not isNaN(combined['GNPSspectrumID'][i]) and not isNaN(combined['MBspectrumID'][i]):\n",
    "                    if combined['tanimotoGM'][i] == 1.0:\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS, MassBank'\n",
    "                    else:\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "                    \n",
    "                \n",
    "                ##### When there is an annotation from one DBs #####\n",
    "                # only GNPS\n",
    "                if not isNaN(combined['GNPSspectrumID'][i]) and isNaN(combined['MBspectrumID'][i]):\n",
    "                    if not isNaN(combined['GNPSSMILES'][i]):\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "        \n",
    "                # only MassBank\n",
    "                if isNaN(combined['GNPSspectrumID'][i]) and not isNaN(combined['MBspectrumID'][i]):\n",
    "                    combined.loc[i, 'Annotation'] = 'MassBank'\n",
    "                    \n",
    "                    \n",
    "            # only GNPS and HMDB   \n",
    "            if db == \"hg\":\n",
    "                ##### When there is an annotation from two DBs #####\n",
    "    \n",
    "    \n",
    "                # only GNPS and HMDB\n",
    "                if not isNaN(combined['GNPSspectrumID'][i]) and not isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    if combined['tanimotoHG'][i] == 1.0:\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS, HMDB'\n",
    "                    else:\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "                \n",
    "                \n",
    "                ##### When there is an annotation from one DBs #####\n",
    "    \n",
    "                # only GNPS\n",
    "                if not isNaN(combined['GNPSspectrumID'][i]) and isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    if not isNaN(combined['GNPSSMILES'][i]):\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "                # only HMDB\n",
    "                    #if isNaN(combined['GNPSspectrumID'][i]) and not isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    #combined.loc[i, 'Annotation'] = 'HMDB'\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "            # only MassBank and HMDB        \n",
    "            if db == \"hm\":\n",
    "                # only MassBank and HMDB\n",
    "                if not isNaN(combined['MBspectrumID'][i]) and not isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    if combined['tanimotoHM'][i] == 1.0:\n",
    "                        combined.loc[i, 'Annotation'] = 'HMDB, MassBank'\n",
    "                    else:\n",
    "                        combined.loc[i, 'Annotation'] = 'MassBank'\n",
    "                \n",
    "                \n",
    "                \n",
    "                # only MassBank\n",
    "                if not isNaN(combined['MBspectrumID'][i]) and isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    combined.loc[i, 'Annotation'] = 'MassBank'\n",
    "    \n",
    "                # only HMDB\n",
    "                    #if isNaN(combined['MBspectrumID'][i]) and not isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    #combined.loc[i, 'Annotation'] = 'HMDB'\n",
    "                \n",
    "            if db == \"gnps\":\n",
    "                # only GNPS\n",
    "                if not isNaN(combined['GNPSspectrumID'][i]):\n",
    "                    if not isNaN(combined['GNPSSMILES'][i]):\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "            if db == \"mbank\":\n",
    "                # only MassBank\n",
    "                if not isNaN(combined['MBspectrumID'][i]):\n",
    "                    combined.loc[i, 'Annotation'] = 'MassBank'\n",
    "            #if db == \"hmdb\":\n",
    "                # only HMDB\n",
    "                #if not isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    #combined.loc[i, 'Annotation'] = 'HMDB'\n",
    "    combined.to_csv(input_dir + \"MetabolomicsResults/curatedSDB.csv\")\n",
    "    return(combined)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d5d9adc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(specDB_Curation.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "576e9f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specDB_Curation(input_dir = \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/\", \n",
    " #               combinedx = \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/MetabolomicsResults/SpecDBvsSL.csv\",\n",
    "   #             sl = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2ebd2016",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specDB_Curation(input_dir = \"/Users/mahnoorzulfiqar/Downloads/MAW-main/\",\n",
    " #               combinedx = \"/Users/mahnoorzulfiqar/Downloads/MAW-main/MetabolomicsResults/SpecDBvsSL.csv\",\n",
    " #               sl = True,\n",
    "   #            db = \"gm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd0d1cb",
   "metadata": {},
   "source": [
    "# combine curated SDB and CDB (S+M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "900ba52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_CuratedR(input_dir, combinedSDBs, combinedSMs, data_type = \"standards\"):\n",
    "    \n",
    "    \"\"\"combine_CuratedR prioritizes in the following manner: gnps>\n",
    "    mbank>suspectlist>sirius>hmdb>metfrag\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "    \n",
    "    curatedSDB: df from specDB_Curation\n",
    "    combinedSM: df from combineSM\n",
    "    \n",
    "    Returns:\n",
    "    dataframe: with curated Spectral DB results and CDB (S+M) results\n",
    "    csv: \"MetabolomicsResults/final_curation_without_classes.csv\"\n",
    "    \n",
    "    Usage:\n",
    "    combine_CuratedR(input_dir = \"usr/project/\", curatedSDB, combinedSM)\n",
    "\n",
    "    \"\"\"\n",
    "    def isNaN(string):\n",
    "        return string != string\n",
    "\n",
    "    combinedSDB = pd.read_csv(combinedSDBs)\n",
    "    combinedSM = pd.read_csv(combinedSMs)\n",
    "    mega = pd.concat([combinedSM, combinedSDB], axis = 1, levels = [\"id_X\"])\n",
    "    \n",
    "    for i, row in mega.iterrows():\n",
    "    \n",
    "        #if only compound database results\n",
    "        if isNaN(mega['Annotation'][i]) and not isNaN(mega['Annotation_C'][i]):\n",
    "            mega.loc[i, \"Annotation_Source\"] = mega['Annotation_C'][i]\n",
    "        \n",
    "        # if only spectral db results\n",
    "        if not isNaN(mega['Annotation'][i]) and isNaN(mega['Annotation_C'][i]):\n",
    "            mega.loc[i, \"Annotation_Source\"] = mega['Annotation'][i]\n",
    "            \n",
    "        # if both have results\n",
    "        if not isNaN(mega['Annotation'][i]) and not isNaN(mega['Annotation_C'][i]):\n",
    "            ########THREE OR FOUR SDB SOURCES########\n",
    "        \n",
    "            #if three sdb sources or more\n",
    "            # prioritize Spectral DBs\n",
    "            if len(mega['Annotation'][i].split()) >= 3 and 'SIRIUS' in mega['Annotation_C'][i]:\n",
    "                if 'MassBank' in mega['Annotation'][i]:\n",
    "                    SKms = [Chem.MolFromSmiles(mega['MBSMILES'][i]), Chem.MolFromSmiles(mega['SMILES'][i])]\n",
    "                    SKfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in SKms]\n",
    "                    SKtn = DataStructs.FingerprintSimilarity(SKfps[0],SKfps[1])\n",
    "                    if SKtn == 1.0:\n",
    "                        print(SKtn)\n",
    "                        mega.loc[i, \"Annotation_Source\"] = mega['Annotation'][i] + ', ' + mega['Annotation_C'][i]\n",
    "                    else:\n",
    "                        mega.loc[i, \"Annotation_Source\"] = mega['Annotation'][i]\n",
    "            \n",
    "                elif 'HMDB' in mega['Annotation'][i]:\n",
    "                    SKms = [Chem.MolFromSmiles(mega['HMDBSMILES'][i]), Chem.MolFromSmiles(mega['SMILES'][i])]\n",
    "                    SKfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in SKms]\n",
    "                    SKtn = DataStructs.FingerprintSimilarity(SKfps[0],SKfps[1])\n",
    "                    if SKtn == 1.0:\n",
    "                        mega.loc[i, \"Annotation_Source\"] = mega['Annotation'][i] + ', ' + mega['Annotation_C'][i]\n",
    "                    else:\n",
    "                        mega.loc[i, \"Annotation_Source\"] = mega['Annotation'][i]\n",
    "            elif len(mega['Annotation'][i].split()) >= 3 and 'SIRIUS' not in mega['Annotation_C'][i]:\n",
    "                if 'KEGG' in mega['Annotation_C'][i]:\n",
    "                    if 'MassBank' in mega['Annotation'][i]:\n",
    "                        SKms = [Chem.MolFromSmiles(mega['MBSMILES'][i]), Chem.MolFromSmiles(mega['KG_SMILES'][i])]\n",
    "                        SKfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in SKms]\n",
    "                        SKtn = DataStructs.FingerprintSimilarity(SKfps[0],SKfps[1])\n",
    "                        if SKtn == 1.0:\n",
    "                            mega.loc[i, \"Annotation_Source\"] = mega['Annotation'][i] + ', ' + mega['Annotation_C'][i]\n",
    "                        else:\n",
    "                            mega.loc[i, \"Annotation_Source\"] = mega['Annotation'][i]\n",
    "            \n",
    "                    elif 'HMDB' in mega['Annotation'][i]:\n",
    "                        SKms = [Chem.MolFromSmiles(mega['HMDBSMILES'][i]), Chem.MolFromSmiles(mega['KG_SMILES'][i])]\n",
    "                        SKfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in SKms]\n",
    "                        SKtn = DataStructs.FingerprintSimilarity(SKfps[0],SKfps[1])\n",
    "                        if SKtn == 1.0:\n",
    "                            mega.loc[i, \"Annotation_Source\"] = mega['Annotation'][i] + ', ' + mega['Annotation_C'][i]\n",
    "                        else:\n",
    "                            mega.loc[i, \"Annotation_Source\"] = mega['Annotation'][i]\n",
    "                else:\n",
    "                    mega.loc[i, \"Annotation_Source\"] = mega['Annotation'][i]\n",
    "            \n",
    "            \n",
    "            \n",
    "            #######TWO OR ONE SDB SOURCE#########\n",
    "                \n",
    "            #if both 2 SDBs and results from insilico tools\n",
    "            elif len(mega['Annotation'][i].split()) <= 2:\n",
    "                mega.loc[i, \"Annotation_Source\"] = mega['Annotation_C'][i]\n",
    "                \n",
    "                \n",
    "        # if no results from any databases\n",
    "        if isNaN(mega['Annotation'][i]) and isNaN(mega['Annotation_C'][i]) and not isNaN(mega['Formula'][i]):\n",
    "            mega.loc[i, \"Annotation_Source\"] = 'SIRIUS_Formula'\n",
    "        \n",
    "    bef_mega = mega.loc[:,~mega.columns.duplicated()]\n",
    "    for i, row in bef_mega.iterrows():\n",
    "        if not isNaN(bef_mega['Annotation_Source'][i]):\n",
    "            # check if SIRIUS is in the annotation source but keep in mind it shouldnt be SIRIUS_Formula\n",
    "            if 'SIRIUS' in bef_mega['Annotation_Source'][i] and 'SIRIUS_Formula' not in bef_mega['Annotation_Source'][i]:\n",
    "                bef_mega.loc[i, 'SMILES_final'] = bef_mega['SMILES'][i]\n",
    "                bef_mega.loc[i,\"CompoundNames\"] = bef_mega['name'][i]\n",
    "                bef_mega['PC_MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['KG_MCSS_SMILES'][i] = np.nan\n",
    "            elif 'KEGG' in bef_mega['Annotation_Source'][i]:\n",
    "                bef_mega.loc[i, 'SMILES_final'] = bef_mega['KG_SMILES'][i]\n",
    "                bef_mega.loc[i, 'CompoundNames'] = bef_mega['KG_Name'][i]\n",
    "                #bef_mega['most_specific_class'][i] = np.nan\n",
    "                #bef_mega['level _5'][i] = np.nan\n",
    "                bef_mega['subclass'][i] = np.nan\n",
    "                bef_mega['class'][i] = np.nan\n",
    "                bef_mega['superclass'][i] = np.nan\n",
    "                #bef_mega['all_classifications'][i] = np.nan\n",
    "                bef_mega['Classification_Source'][i] = np.nan\n",
    "                bef_mega['MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['PC_MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['Formula'][i] = np.nan\n",
    "            \n",
    "            elif 'GNPS, SuspectList' in bef_mega['Annotation_Source'][i]:\n",
    "                bef_mega.loc[i,'SMILES_final'] = bef_mega['GLsmiles'][i]\n",
    "                bef_mega.loc[i, 'CompoundNames'] = bef_mega['GLname'][i]\n",
    "                bef_mega.loc[i, 'CompoundNames']\n",
    "                #bef_mega['most_specific_class'][i] = np.nan\n",
    "                #bef_mega['level _5'][i] = np.nan\n",
    "                bef_mega['subclass'][i] = np.nan\n",
    "                bef_mega['class'][i] = np.nan\n",
    "                bef_mega['superclass'][i] = np.nan\n",
    "                #bef_mega['all_classifications'][i] = np.nan\n",
    "                bef_mega['MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['Classification_Source'][i] = np.nan\n",
    "                bef_mega['PC_MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['KG_MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['Formula'][i] = np.nan\n",
    "        \n",
    "            elif 'GNPS' in bef_mega['Annotation_Source'][i]:\n",
    "                bef_mega.loc[i,'SMILES_final'] = bef_mega['GNPSSMILES'][i]\n",
    "                bef_mega.loc[i, 'CompoundNames'] = bef_mega['GNPScompound_name'][i]\n",
    "                #bef_mega['most_specific_class'][i] = np.nan\n",
    "                #bef_mega['level _5'][i] = np.nan\n",
    "                bef_mega['subclass'][i] = np.nan\n",
    "                bef_mega['class'][i] = np.nan\n",
    "                bef_mega['superclass'][i] = np.nan\n",
    "                #bef_mega['all_classifications'][i] = np.nan\n",
    "                bef_mega['MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['Classification_Source'][i] = np.nan\n",
    "                bef_mega['PC_MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['KG_MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['Formula'][i] = np.nan\n",
    "            elif 'MassBank' in bef_mega['Annotation_Source'][i]:\n",
    "                bef_mega.loc[i, 'SMILES_final'] = bef_mega['MBSMILES'][i]\n",
    "                bef_mega.loc[i, 'CompoundNames'] = bef_mega['MBcompound_name'][i]\n",
    "                #bef_mega['most_specific_class'][i] = np.nan\n",
    "                #bef_mega['level _5'][i] = np.nan\n",
    "                bef_mega['subclass'][i] = np.nan\n",
    "                bef_mega['class'][i] = np.nan\n",
    "                bef_mega['superclass'][i] = np.nan\n",
    "                #bef_mega['all_classifications'][i] = np.nan\n",
    "                bef_mega['Classification_Source'][i] = np.nan\n",
    "                bef_mega['MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['PC_MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['KG_MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['Formula'][i] = np.nan\n",
    "                \n",
    "                \n",
    "            elif 'PubChem' in bef_mega['Annotation_Source'][i]:\n",
    "                bef_mega.loc[i, 'SMILES_final'] = bef_mega['PC_SMILES'][i]\n",
    "                bef_mega.loc[i, 'CompoundNames'] = bef_mega['PC_Name'][i]\n",
    "                #bef_mega['most_specific_class'][i] = np.nan\n",
    "                #bef_mega['level _5'][i] = np.nan\n",
    "                bef_mega['subclass'][i] = np.nan\n",
    "                bef_mega['class'][i] = np.nan\n",
    "                bef_mega['superclass'][i] = np.nan\n",
    "                #bef_mega['all_classifications'][i] = np.nan\n",
    "                bef_mega['Classification_Source'][i] = np.nan\n",
    "                bef_mega['MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['KG_MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['Formula'][i] = np.nan\n",
    "            \n",
    "            elif 'HMDB' in bef_mega['Annotation_Source'][i]:\n",
    "                bef_mega.loc[i, 'SMILES_final'] = bef_mega['HMDBSMILES'][i]\n",
    "                bef_mega.loc[i, 'CompoundNames'] = bef_mega['HMDBcompound_name'][i]\n",
    "                #bef_mega['most_specific_class'][i] = np.nan\n",
    "                #bef_mega['level _5'][i] = np.nan\n",
    "                bef_mega['subclass'][i] = np.nan\n",
    "                bef_mega['class'][i] = np.nan\n",
    "                bef_mega['superclass'][i] = np.nan\n",
    "                #bef_mega['all_classifications'][i] = np.nan\n",
    "                bef_mega['Classification_Source'][i] = np.nan\n",
    "                bef_mega['MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['PC_MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['KG_MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['Formula'][i] = np.nan\n",
    "                \n",
    "                \n",
    "            elif 'SIRIUS_Formula' in bef_mega['Annotation_Source'][i]:\n",
    "                bef_mega['PC_MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['KG_MCSS_SMILES'][i] = np.nan\n",
    "                \n",
    "                \n",
    "    bef_megaA = bef_mega[['id_X', \n",
    "                          'premz', \n",
    "                          'rtmed', \n",
    "                          'rtmean',\n",
    "                          'int', \n",
    "                          'col_eng', \n",
    "                          'pol', \n",
    "                          'SMILES_final', \n",
    "                          'CompoundNames', \n",
    "                          'MCSS_SMILES', \n",
    "                          'PC_MCSS_SMILES', \n",
    "                          'KG_MCSS_SMILES', \n",
    "                          'subclass', \n",
    "                          'class', \n",
    "                          'superclass', \n",
    "                          'Classification_Source', \n",
    "                          'Annotation_Source'\n",
    "                         ]]\n",
    "            \n",
    "    bef_megaA.rename(columns = {'SMILES_final':'SMILES'}, inplace = True)\n",
    "    \n",
    "    \n",
    "    Standards = ['Experimental']\n",
    "    SpectralDB = ['GNPS', 'HMDB', 'MassBank']\n",
    "    CompoundDB = ['SuspectList', 'SIRIUS', 'KEGG', 'PubChem']\n",
    "    Formula = ['SIRIUS_Formula']\n",
    "\n",
    "    \n",
    "    #bef_megaA['MSI_Level'] = np.nan\n",
    "    for i, rows in bef_megaA.iterrows():\n",
    "        \n",
    "        \n",
    "        if not isNaN(bef_megaA['Annotation_Source'][i]):\n",
    "            \n",
    "            if data_type == \"standards\":\n",
    "                bef_megaA.loc[i, 'Annotation_Source'] = bef_megaA['Annotation_Source'][i] + ', Experimental'\n",
    "\n",
    "                if any(x in bef_megaA['Annotation_Source'][i] for x in SpectralDB):\n",
    "                    bef_megaA.loc[i, 'MSI_Level'] = 'Level_1'\n",
    "                    \n",
    "                elif any(x in bef_megaA['Annotation_Source'][i] for x in CompoundDB) and not any(x in bef_megaA['Annotation_Source'][i] for x in Formula):\n",
    "                    bef_megaA.loc[i, 'MSI_Level'] = 'Level_2/Level_3'\n",
    "                    \n",
    "                elif any(x in bef_megaA['Annotation_Source'][i] for x in Formula):\n",
    "                    bef_megaA.loc[i, 'MSI_Level'] = 'Level_4'\n",
    "                    \n",
    "            else:\n",
    "\n",
    "                if any(x in bef_megaA['Annotation_Source'][i] for x in SpectralDB):\n",
    "                    bef_megaA.loc[i, 'MSI_Level'] = 'Level_2'\n",
    "                    \n",
    "                elif any(x in bef_megaA['Annotation_Source'][i] for x in CompoundDB) and not any(x in bef_megaA['Annotation_Source'][i] for x in Formula):\n",
    "                    bef_megaA.loc[i, 'MSI_Level'] = 'Level_3'\n",
    "                    \n",
    "                elif any(x in bef_megaA['Annotation_Source'][i] for x in Formula):\n",
    "                    bef_megaA.loc[i, 'MSI_Level'] = 'Level_4'\n",
    "                \n",
    "        else:\n",
    "            bef_megaA.loc[i, 'MSI_Level'] = 'Level_5'\n",
    "            \n",
    "                \n",
    "    \n",
    "            \n",
    "    bef_megaA.to_csv(input_dir + \"MetabolomicsResults/final_curation_without_classes.csv\")\n",
    "    return(bef_megaA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6e1e4ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(combine_CuratedR.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "95d4c26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine_CuratedR(input_dir = \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/\", \n",
    "                 #combinedSDBs = \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/MetabolomicsResults/curatedSDB.csv\", \n",
    "                 #combinedSMs = \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/MetabolomicsResults/combinedSM.csv\",\n",
    "                #data_type = \"standards\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9aa52d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine_CuratedR(input_dir = \"/Users/mahnoorzulfiqar/Downloads/MAW-main/\", \n",
    "                 #combinedSDBs = \"/Users/mahnoorzulfiqar/Downloads/MAW-main/MetabolomicsResults/curatedSDB.csv\", \n",
    "                 #combinedSMs =\"/Users/mahnoorzulfiqar/Downloads/MAW-main/MetabolomicsResults/combinedSM.csv\",\n",
    "                #data_type = \"standards\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "82f0a82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkSMILES_validity(input_dir, resultcsv):\n",
    "    def isNaN(string):\n",
    "        return string != string\n",
    "    \"\"\"checkSMILES_validity does exactly as the name says, using\n",
    "    RDKit, whether the SMILES are invalid or have invalid \n",
    "    chemistry\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "    \n",
    "    results: df from combine_CuratedR\n",
    "    \n",
    "    Returns:\n",
    "    dataframe: with valid SMILES\n",
    "    csv: \"MetabolomicsResults/final_curation_with_validSMILES.csv\"\n",
    "    \n",
    "    Usage:\n",
    "    checkSMILES_validity(input_dir = \"usr/project/\", results)\n",
    "\n",
    "    \"\"\"\n",
    "    results = pd.read_csv(resultcsv)\n",
    "    # check validity of SMILES\n",
    "    for i, row in results.iterrows():\n",
    "        if not isNaN(results['SMILES'][i]):\n",
    "            m = Chem.MolFromSmiles(results['SMILES'][i] ,sanitize=False)\n",
    "            if m is None:\n",
    "                results['SMILES_final'][i] = 'invalid_SMILES'\n",
    "            else:\n",
    "                try:\n",
    "                    Chem.SanitizeMol(m)\n",
    "                except:\n",
    "                    results['SMILES_final'][i] = 'invalid_chemistry'\n",
    "    results.to_csv(input_dir + \"MetabolomicsResults/final_curation_with_validSMILES.csv\")\n",
    "    return(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "76c18e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(checkSMILES_validity.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ec74f9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkSMILES_validity(input_dir = '/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/',\n",
    "  #                   resultcsv = '/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/MetabolomicsResults/final_curation_without_classes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b11606a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkSMILES_validity(input_dir = '/Users/mahnoorzulfiqar/Downloads/MAW-main/', \n",
    "      #               resultcsv = '/Users/mahnoorzulfiqar/Downloads/MAW-main/MetabolomicsResults/final_curation_without_classes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbace30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6e99e6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(input_dir, resultcsv):\n",
    "    def isNaN(string):\n",
    "        return string != string\n",
    "    \"\"\"classification function uses ClassyFire ChemONT\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "    \n",
    "    resultcsv: csv of df from combine_CuratedR or checkSMILES_validity\n",
    "    \n",
    "    Returns:\n",
    "    dataframe: with classification\n",
    "    csv: \"MetabolomicsResults/final_curationList.csv\"\n",
    "    \n",
    "    Usage:\n",
    "    checkSMILES_validity(input_dir = \"usr/project/\", frame)\n",
    "\n",
    "    \"\"\"\n",
    "    frame = pd.read_csv(resultcsv)\n",
    "    inchis = []\n",
    "    for i, row in frame.iterrows():\n",
    "        if not isNaN(frame['SMILES'][i]) and isNaN(frame['Classification_Source'][i]):\n",
    "            try:\n",
    "                InChI = Chem.MolToInchi(Chem.MolFromSmiles(frame[\"SMILES\"][i]))\n",
    "                InChIKey = Chem.inchi.InchiToInchiKey(InChI)\n",
    "                inchis.append({\n",
    "                    'index': i,\n",
    "                    'smiles':frame[\"SMILES\"][i],\n",
    "                    'inchi': InChI,\n",
    "                    'inchikey': InChIKey\n",
    "                })\n",
    "            except:\n",
    "                pass\n",
    "    inchis = pd.DataFrame(inchis)\n",
    "    if len(inchis):\n",
    "        inchis = inchis.loc[-isNaN(inchis['inchikey'])]\n",
    "        ## Retrieve ClassyFire classifications ##\n",
    "\n",
    "        # This first step is done using inchikey and interrogation of the gnps classified structures\n",
    "        gnps_proxy = True \n",
    "        url = \"http://classyfire.wishartlab.com\"\n",
    "        proxy_url =  \"https://gnps-classyfire.ucsd.edu\"\n",
    "        chunk_size = 1000\n",
    "        sleep_interval = 12\n",
    "\n",
    "        all_inchi_keys = list(inchis['inchikey'].drop_duplicates())\n",
    "\n",
    "        resolved_ik_number_list = [0, 0]\n",
    "        total_inchikey_number = len(all_inchi_keys)\n",
    "\n",
    "        while True:\n",
    "\n",
    "            #start_time = time.time()\n",
    "\n",
    "            #print('%s inchikey to resolve' % total_inchikey_number )\n",
    "            get_classifications_cf_mod(all_inchi_keys, par_level = 6)\n",
    "\n",
    "            cleanse('all_json.json', 'all_json.json')\n",
    "\n",
    "            with open(\"all_json.json\") as tweetfile:\n",
    "                jsondic = json.loads(tweetfile.read())\n",
    "\n",
    "            df = json_normalize(jsondic)\n",
    "            df = df.drop_duplicates( 'inchikey' )\n",
    "            resolved_ik_number = len( df.drop_duplicates('inchikey').inchikey )\n",
    "            resolved_ik_number_list.append( resolved_ik_number )\n",
    "            #print('%s resolved inchikeys' % resolved_ik_number )\n",
    "            #print(\"done in --- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "            if resolved_ik_number_list[-1] < resolved_ik_number_list[-2] or resolved_ik_number_list[-1] == resolved_ik_number_list[-3]:\n",
    "                break\n",
    "            cleanse('all_json.json', 'all_json_cleaned.json')\n",
    "\n",
    "            with open(\"all_json_cleaned.json\") as tweetfile:\n",
    "                jsondic = json.loads(tweetfile.read())\n",
    "\n",
    "        flattened_classified_json = json_normalize(jsondic)\n",
    "        flattened_df = flattened_classified_json.drop_duplicates('inchikey')\n",
    "        flattened_df['inchikey'] = flattened_df['inchikey'].str.replace(r'InChIKey=', '')\n",
    "        df_merged = pd.merge(inchis, flattened_df, left_on='inchikey', right_on='inchikey', how='left')\n",
    "\n",
    "        for p, rowp in df_merged.iterrows():\n",
    "            for q, rowq in frame.iterrows():\n",
    "                if df_merged[\"smiles_x\"][p] is frame[\"SMILES\"][q]:\n",
    "                    frame.loc[q, 'subclass'] = df_merged[\"subclass.name\"][p]\n",
    "                    frame.loc[q, 'class'] = df_merged[\"class.name\"][p]\n",
    "                    frame.loc[q, 'superclass'] = df_merged[\"superclass.name\"][p]\n",
    "                    frame.loc[q, 'Classification_Source'] = \"ClassyFire\"\n",
    "\n",
    "\n",
    "\n",
    "        frame.to_csv(input_dir + \"MetabolomicsResults/final_curationList.csv\")\n",
    "        return(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d0561a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(classification.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc515e92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4939d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "882d4477",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#classification(input_dir = '/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/',\n",
    "              # resultcsv = '/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/MetabolomicsResults/final_curation_with_validSMILES.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "60f80bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification(input_dir = '/Users/mahnoorzulfiqar/Downloads/MAW-main/',\n",
    "               #resultcsv = '/Users/mahnoorzulfiqar/Downloads/MAW-main/MetabolomicsResults/final_curation_with_validSMILES.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5e1ccf",
   "metadata": {},
   "source": [
    "# Comparison with a list of SMILES from any Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ba5db8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cd = pd.read_csv('/Users/mahnoorzulfiqar/OneDriveUNI/MZML/CD/CD_Results.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dc19f8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def create_SMILES_list(input_dir, compcsv)\n",
    "    #CDCSV = list(cd[-isNaN(cd['SMILES'])]['SMILES'])\n",
    "    #f = open(\"/Users/mahnoorzulfiqar/OneDriveUNI/MZML/CD/CDCSV.txt\", \"w\")\n",
    "    #for item in CDCSV:\n",
    "        #f.write(item + \"\\n\")\n",
    "    #f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "77bdce92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMILESscreening(input_dir, resultcsv, complist, listname):\n",
    "    def isNaN(string):\n",
    "        return string != string\n",
    "    \"\"\"SMILESscreening takes a list of SMILES\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "    \n",
    "    resultcsv: df from combine_CuratedR or checkSMILES_validity or classification\n",
    "    complist: list of /n separated txt file conyaining smiles on each line\n",
    "    listname: name of the list of compounds\n",
    "    \n",
    "    Returns:\n",
    "    dataframe: comparison with another list of compounds\n",
    "    csv: \"MetabolomicsResults/final_curation_with_validSMILES.csv\"\n",
    "    \n",
    "    Usage:\n",
    "    checkSMILES_validity(input_dir = \"usr/project/\", results)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    results = pd.read_csv(resultcsv)\n",
    "    with open(complist, \"r\") as text_file:\n",
    "        cd = text_file.read().split('\\n')\n",
    "    \n",
    "    for i, row in results.iterrows():\n",
    "        if not isNaN(results['SMILES'][i]):\n",
    "            if 'invalid_SMILES' not in results['SMILES'][i] and 'invalid_chemistry' not in results['SMILES'][i]:\n",
    "                for j in cd:\n",
    "                    if not isNaN(j):\n",
    "                        CGms = [Chem.MolFromSmiles(results['SMILES'][i]), Chem.MolFromSmiles(j)]\n",
    "                        CGfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=1024) for x in CGms]\n",
    "                        CGtn = DataStructs.FingerprintSimilarity(CGfps[0],CGfps[1])\n",
    "                        if CGtn == 1 and listname not in results['Annotation_Source'][i]:\n",
    "                            results['Annotation_Source'][i] = results['Annotation_Source'][i] + ', ' + listname\n",
    "    \n",
    "\n",
    "    frame.to_csv(input_dir + \"MetabolomicsResults/final_curationListVS\"+listname+\".csv\")\n",
    "    return(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "82049e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(SMILESscreening.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968d2ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4ba72546",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMILESscreening(input_dir = '/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/',\n",
    "                #resultcsv = '/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/MetabolomicsResults/final_curation_with_validSMILES.csv',\n",
    "                #complist = '/Users/mahnoorzulfiqar/OneDriveUNI/MZML/CD/CDCSV.txt',\n",
    "                #listname = 'CompoundDiscoverer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0d1e4041",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMILESscreening(input_dir = '/Users/mahnoorzulfiqar/Downloads/MAW-main/', \n",
    "                #resultcsv = '/Users/mahnoorzulfiqar/Downloads/MAW-main/MetabolomicsResults/final_curationList.csv', \n",
    "                #complist = '/Users/mahnoorzulfiqar/OneDriveUNI/MZML/CD/CDCSV.txt', \n",
    "                #listname = 'CompoundDiscoverer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44c993e",
   "metadata": {},
   "source": [
    "## NP_Classifier classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "728ebeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Np_pathways(input_dir, resultcsv):\n",
    "    df = pd.read_csv(resultcsv)\n",
    "    npresults = []\n",
    "    for i, row in df.iterrows():\n",
    "        if not isNaN(df['SMILES'][i]):\n",
    "            try:\n",
    "                cvv = Chem.MolFromSmiles(df['SMILES'][i])\n",
    "                cvv = Chem.MolToSmiles(cvv, isomericSmiles = False)\n",
    "                c = urllib.parse.quote_plus(cvv, safe=' ')\n",
    "            \n",
    "                url = 'https://npclassifier.ucsd.edu/classify?smiles='+c\n",
    "                names = str(df['id_X'][i])\n",
    "                outx = str(\"myFile\"+names+\".txt\")\n",
    "                file = wget.download(url, out = outx)\n",
    "                a_dataframe = pd.read_csv(file, delimiter = \"]\")\n",
    "                xox = list(a_dataframe.columns.values)\n",
    "                splitting0 = xox[0].split(':')\n",
    "                xoc = re.sub('\\ |\\[|\\]|\\\"', ' ', splitting0[1]).strip()\n",
    "                splitting1 = xox[1].split(':')\n",
    "                xos = re.sub('\\ |\\[|\\]|\\\"', ' ', splitting1[1]).strip()\n",
    "                #except:\n",
    "                    #splitting1 = xox[1].split(':')\n",
    "                    #xos = re.sub('\\ |\\[|\\]|\\\"', '', splitting1[0])\n",
    "                splitting2 = xox[2].split(':')\n",
    "                xop = re.sub('\\ |\\[|\\]|\\\"', ' ', splitting2[1]).strip()\n",
    "                #df.loc[i, 'npclass'] = xoc\n",
    "                #df.loc[i, 'npsuper_class'] = xos\n",
    "                if not isNaN(df['class'][i]) and df['class'][i] in xoc:\n",
    "                    df.loc[i, 'np_pathway'] = xop\n",
    "                os.remove(outx)\n",
    "                time.sleep(0.5)\n",
    "\n",
    "                npresults.append({\n",
    "                    'index':i,\n",
    "                    #'id': df['file_id'][i],\n",
    "                    'mz': df['premz'][i],\n",
    "                    'rt': df['rtmed'][i],\n",
    "                    'SMILES': df['SMILES'][i],\n",
    "                    'class': xoc,\n",
    "                    'subclass': xos,\n",
    "                    'pathway': xop\n",
    "                })\n",
    "            except:\n",
    "                pass\n",
    "    np_results = pd.DataFrame(npresults)\n",
    "    np_results.to_csv(input_dir + \"/MetabolomicsResults/NPClassifier_Results.csv\")\n",
    "    df.to_csv(input_dir + \"/MetabolomicsResults/final_results_with_Pathways.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "45d88d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "851\n",
      "100% [..........................................................] 18553 / 18553852\n",
      "100% [..........................................................] 18597 / 18597853\n",
      "100% [..........................................................] 18612 / 18612854\n",
      "100% [..........................................................] 18654 / 18654855\n",
      "100% [..........................................................] 18609 / 18609856\n",
      "100% [..........................................................] 18544 / 18544857\n",
      "100% [..........................................................] 18588 / 18588858\n",
      "100% [..........................................................] 18597 / 18597859\n",
      "100% [..........................................................] 18605 / 18605860\n",
      "100% [..........................................................] 18560 / 18560861\n",
      "100% [..........................................................] 18632 / 18632862\n",
      "100% [..........................................................] 18572 / 18572863\n",
      "100% [..........................................................] 18609 / 18609864\n",
      "100% [..........................................................] 18546 / 18546865\n",
      "100% [..........................................................] 18600 / 18600866\n",
      "100% [..........................................................] 18558 / 18558867\n",
      "100% [..........................................................] 18585 / 18585868\n",
      "100% [..........................................................] 18599 / 18599869\n",
      "100% [..........................................................] 18633 / 18633870\n",
      "100% [..........................................................] 18590 / 18590871\n",
      "100% [..........................................................] 18557 / 18557872\n",
      "100% [..........................................................] 18575 / 18575873\n",
      "100% [..........................................................] 18544 / 18544874\n",
      "100% [..........................................................] 18597 / 18597875\n",
      "100% [..........................................................] 18598 / 18598876\n",
      "100% [..........................................................] 18606 / 18606877\n",
      "100% [..........................................................] 18583 / 18583878\n",
      "100% [..........................................................] 18553 / 18553879\n",
      "100% [..........................................................] 18597 / 18597880\n",
      "100% [..........................................................] 18543 / 18543881\n",
      "100% [..........................................................] 18543 / 18543882\n",
      "100% [..........................................................] 18597 / 18597883\n",
      "100% [..........................................................] 18554 / 18554884\n",
      "100% [..........................................................] 18544 / 18544885\n",
      "100% [..........................................................] 18622 / 18622886\n",
      "100% [..........................................................] 18570 / 18570887\n",
      "100% [..........................................................] 18570 / 18570888\n",
      "100% [..........................................................] 18571 / 18571889\n",
      "100% [..........................................................] 18603 / 18603890\n",
      "100% [..........................................................] 18579 / 18579891\n",
      "100% [..........................................................] 18554 / 18554892\n",
      "100% [..........................................................] 18543 / 18543893\n",
      "100% [..........................................................] 18640 / 18640894\n",
      "100% [..........................................................] 18598 / 18598895\n",
      "100% [..........................................................] 18554 / 18554896\n",
      "100% [..........................................................] 18569 / 18569897\n",
      "100% [..........................................................] 18587 / 18587898\n",
      "100% [..........................................................] 18576 / 18576899\n",
      "100% [..........................................................] 18601 / 18601900\n",
      "100% [..........................................................] 18599 / 18599901\n",
      "100% [..........................................................] 18622 / 18622902\n",
      "100% [..........................................................] 18596 / 18596903\n",
      "100% [..........................................................] 18605 / 18605904\n",
      "100% [..........................................................] 18594 / 18594905\n",
      "100% [..........................................................] 18634 / 18634906\n",
      "100% [..........................................................] 18598 / 18598907\n",
      "100% [..........................................................] 18608 / 18608908\n",
      "100% [..........................................................] 18594 / 18594909\n",
      "100% [..........................................................] 18587 / 18587910\n",
      "100% [..........................................................] 18613 / 18613911\n",
      "100% [..........................................................] 18611 / 18611912\n",
      "100% [..........................................................] 18559 / 18559913\n",
      "100% [..........................................................] 18605 / 18605914\n",
      "100% [..........................................................] 18613 / 18613915\n",
      "100% [..........................................................] 18588 / 18588916\n",
      "100% [..........................................................] 18545 / 18545917\n",
      "100% [..........................................................] 18608 / 18608918\n",
      "100% [..........................................................] 18594 / 18594919\n",
      "100% [..........................................................] 18613 / 18613920\n",
      "100% [..........................................................] 18559 / 18559921\n",
      "100% [..........................................................] 18543 / 18543922\n",
      "100% [..........................................................] 18598 / 18598923\n",
      "100% [..........................................................] 18542 / 18542924\n",
      "100% [..........................................................] 18594 / 18594925\n",
      "100% [..........................................................] 18548 / 18548926\n",
      "100% [..........................................................] 18606 / 18606927\n",
      "100% [..........................................................] 18604 / 18604928\n",
      "100% [..........................................................] 18606 / 18606929\n",
      "100% [..........................................................] 18606 / 18606930\n",
      "100% [..........................................................] 18551 / 18551931\n",
      "100% [..........................................................] 18553 / 18553932\n",
      "100% [..........................................................] 18553 / 18553933\n",
      "100% [..........................................................] 18542 / 18542934\n",
      "100% [..........................................................] 18573 / 18573935\n",
      "100% [..........................................................] 18598 / 18598936\n",
      "100% [..........................................................] 18586 / 18586937\n",
      "100% [..........................................................] 18544 / 18544938\n",
      "100% [..........................................................] 18612 / 18612939\n",
      "100% [..........................................................] 18605 / 18605940\n",
      "100% [..........................................................] 18610 / 18610941\n",
      "100% [..........................................................] 18587 / 18587942\n",
      "100% [..........................................................] 18612 / 18612943\n",
      "100% [..........................................................] 18613 / 18613944\n",
      "100% [..........................................................] 18613 / 18613945\n",
      "100% [..........................................................] 18585 / 18585946\n",
      "100% [..........................................................] 18575 / 18575947\n",
      "100% [..........................................................] 18605 / 18605948\n",
      "100% [..........................................................] 18590 / 18590949\n",
      "100% [..........................................................] 18605 / 18605950\n",
      "100% [..........................................................] 18579 / 18579951\n",
      "100% [..........................................................] 18604 / 18604952\n",
      "100% [..........................................................] 18548 / 18548953\n",
      "100% [..........................................................] 18604 / 18604954\n",
      "100% [..........................................................] 18556 / 18556955\n",
      "100% [..........................................................] 18607 / 18607956\n",
      "100% [..........................................................] 18542 / 18542957\n",
      "100% [..........................................................] 18557 / 18557958\n",
      "100% [..........................................................] 18595 / 18595959\n",
      "100% [..........................................................] 18600 / 18600960\n",
      "100% [..........................................................] 18542 / 18542961\n",
      "100% [..........................................................] 18554 / 18554962\n",
      "100% [..........................................................] 18581 / 18581963\n",
      "100% [..........................................................] 18597 / 18597964\n",
      "100% [..........................................................] 18594 / 18594965\n",
      "100% [..........................................................] 18606 / 18606966\n",
      "100% [..........................................................] 18603 / 18603967\n",
      "100% [..........................................................] 18633 / 18633968\n",
      "100% [..........................................................] 18591 / 18591969\n",
      "100% [..........................................................] 18599 / 18599970\n",
      "100% [..........................................................] 18632 / 18632971\n",
      "100% [..........................................................] 18599 / 18599972\n",
      "100% [..........................................................] 18594 / 18594973\n",
      "100% [..........................................................] 18586 / 18586974\n",
      "100% [..........................................................] 18542 / 18542975\n",
      "100% [..........................................................] 18554 / 18554976\n",
      "100% [..........................................................] 18597 / 18597977\n",
      "100% [..........................................................] 18598 / 18598978\n",
      "100% [..........................................................] 18555 / 18555979\n",
      "100% [..........................................................] 18586 / 18586980\n",
      "100% [..........................................................] 18556 / 18556981\n",
      "100% [..........................................................] 18543 / 18543982\n",
      "100% [..........................................................] 18596 / 18596983\n",
      "100% [..........................................................] 18570 / 18570984\n",
      "100% [..........................................................] 18542 / 18542985\n",
      "100% [..........................................................] 18607 / 18607986\n",
      "100% [..........................................................] 18613 / 18613987\n",
      "100% [..........................................................] 18596 / 18596988\n",
      "100% [..........................................................] 18598 / 18598989\n",
      "100% [..........................................................] 18596 / 18596990\n",
      "100% [..........................................................] 18572 / 18572991\n",
      "100% [..........................................................] 18555 / 18555992\n",
      "100% [..........................................................] 18598 / 18598993\n",
      "100% [..........................................................] 18598 / 18598994\n",
      "100% [..........................................................] 18613 / 18613995\n",
      "100% [..........................................................] 18585 / 18585996\n",
      "100% [..........................................................] 18589 / 18589997\n",
      "100% [..........................................................] 18597 / 18597998\n",
      "100% [..........................................................] 18575 / 18575999\n",
      "100% [..........................................................] 18598 / 185981000\n",
      "100% [..........................................................] 18597 / 185971001\n",
      "100% [..........................................................] 18598 / 185981002\n",
      "100% [..........................................................] 18555 / 185551003\n",
      "100% [..........................................................] 18573 / 185731004\n",
      "100% [..........................................................] 18632 / 186321005\n",
      "100% [..........................................................] 18559 / 185591006\n",
      "100% [..........................................................] 18593 / 185931007\n",
      "100% [..........................................................] 18596 / 185961008\n",
      "100% [..........................................................] 18594 / 185941009\n",
      "100% [..........................................................] 18597 / 185971010\n",
      "100% [..........................................................] 18598 / 185981011\n",
      "100% [..........................................................] 18543 / 185431012\n",
      "100% [..........................................................] 18589 / 185891013\n",
      "100% [..........................................................] 18598 / 185981014\n",
      "100% [..........................................................] 18571 / 185711015\n",
      "100% [..........................................................] 18599 / 185991016\n",
      "100% [..........................................................] 18570 / 185701017\n",
      "100% [..........................................................] 18542 / 185421018\n",
      "100% [..........................................................] 18597 / 185971019\n",
      "100% [..........................................................] 18586 / 185861020\n",
      "100% [..........................................................] 18542 / 185421021\n",
      "100% [..........................................................] 18596 / 185961022\n",
      "100% [..........................................................] 18557 / 185571023\n",
      "100% [..........................................................] 18598 / 185981024\n",
      "100% [..........................................................] 18555 / 185551025\n",
      "100% [..........................................................] 18555 / 185551026\n",
      "100% [..........................................................] 18544 / 185441027\n",
      "100% [..........................................................] 18615 / 186151028\n",
      "100% [..........................................................] 18598 / 185981029\n",
      "100% [..........................................................] 18572 / 185721030\n",
      "100% [..........................................................] 18598 / 185981031\n",
      "100% [..........................................................] 18599 / 185991032\n",
      "100% [..........................................................] 18553 / 185531033\n",
      "100% [..........................................................] 18575 / 185751034\n",
      "100% [..........................................................] 18581 / 185811035\n",
      "100% [..........................................................] 18553 / 185531036\n",
      "100% [..........................................................] 18587 / 185871037\n",
      "100% [..........................................................] 18558 / 185581038\n",
      "100% [..........................................................] 18604 / 186041039\n",
      "100% [..........................................................] 18622 / 186221040\n",
      "100% [..........................................................] 18587 / 185871041\n",
      "100% [..........................................................] 18553 / 185531042\n",
      "100% [..........................................................] 18553 / 185531043\n",
      "100% [..........................................................] 18554 / 185541044\n",
      "100% [..........................................................] 18542 / 185421045\n",
      "100% [..........................................................] 18544 / 185441046\n",
      "100% [..........................................................] 18596 / 185961047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [..........................................................] 18622 / 186221048\n",
      "100% [..........................................................] 18542 / 185421049\n",
      "100% [..........................................................] 18553 / 185531050\n",
      "100% [..........................................................] 18542 / 185421051\n",
      "100% [..........................................................] 18556 / 185561052\n",
      "100% [..........................................................] 18606 / 186061053\n",
      "100% [..........................................................] 18600 / 186001054\n",
      "100% [..........................................................] 18585 / 185851055\n",
      "100% [..........................................................] 18594 / 185941056\n",
      "100% [..........................................................] 18607 / 186071057\n",
      "100% [..........................................................] 18553 / 185531058\n",
      "100% [..........................................................] 18542 / 185421059\n",
      "100% [..........................................................] 18554 / 185541060\n",
      "100% [..........................................................] 18553 / 185531061\n",
      "100% [..........................................................] 18542 / 185421062\n",
      "100% [..........................................................] 18604 / 186041063\n",
      "100% [..........................................................] 18575 / 185751064\n",
      "100% [..........................................................] 18542 / 185421065\n",
      "100% [..........................................................] 18542 / 185421066\n",
      "100% [..........................................................] 18622 / 186221067\n",
      "100% [..........................................................] 18575 / 185751068\n",
      "100% [..........................................................] 18581 / 185811069\n",
      "100% [..........................................................] 18542 / 185421070\n",
      "100% [..........................................................] 18542 / 185421071\n",
      "100% [..........................................................] 18542 / 185421072\n",
      "100% [..........................................................] 18553 / 185531073\n",
      "100% [..........................................................] 18575 / 185751074\n",
      "100% [..........................................................] 18554 / 185541075\n",
      "100% [..........................................................] 18553 / 185531076\n",
      "100% [..........................................................] 18605 / 186051077\n",
      "100% [..........................................................] 18592 / 185921078\n",
      "100% [..........................................................] 18543 / 185431079\n",
      "100% [..........................................................] 18555 / 185551080\n",
      "100% [..........................................................] 18542 / 185421081\n",
      "100% [..........................................................] 18556 / 185561082\n",
      "100% [..........................................................] 18577 / 185771083\n",
      "100% [..........................................................] 18557 / 185571084\n",
      "100% [..........................................................] 18574 / 185741085\n",
      "100% [..........................................................] 18542 / 185421086\n",
      "100% [..........................................................] 18575 / 185751087\n",
      "100% [..........................................................] 18570 / 185701088\n",
      "100% [..........................................................] 18542 / 185421089\n",
      "100% [..........................................................] 18542 / 185421090\n",
      "100% [..........................................................] 18622 / 186221091\n",
      "100% [..........................................................] 18575 / 185751092\n",
      "100% [..........................................................] 18588 / 185881093\n",
      "100% [..........................................................] 18542 / 185421094\n",
      "100% [..........................................................] 18542 / 185421095\n",
      "100% [..........................................................] 18583 / 185831096\n",
      "100% [..........................................................] 18555 / 185551097\n",
      "100% [..........................................................] 18596 / 185961098\n",
      "100% [..........................................................] 18597 / 185971099\n",
      "100% [..........................................................] 18607 / 186071100\n",
      "100% [..........................................................] 18587 / 185871101\n",
      "100% [..........................................................] 18609 / 186091102\n",
      "100% [..........................................................] 18595 / 185951103\n",
      "100% [..........................................................] 18542 / 185421104\n",
      "100% [..........................................................] 18553 / 185531105\n",
      "100% [..........................................................] 18608 / 186081106\n",
      "100% [..........................................................] 18554 / 185541107\n",
      "100% [..........................................................] 18542 / 185421108\n",
      "100% [..........................................................] 18542 / 185421109\n",
      "100% [..........................................................] 18553 / 185531110\n",
      "100% [..........................................................] 18606 / 186061111\n",
      "100% [..........................................................] 18553 / 185531112\n",
      "100% [..........................................................] 18606 / 186061113\n",
      "100% [..........................................................] 18606 / 186061114\n",
      "100% [..........................................................] 18588 / 185881115\n",
      "100% [..........................................................] 18575 / 185751116\n",
      "100% [..........................................................] 18585 / 185851117\n",
      "100% [..........................................................] 18622 / 186221118\n",
      "100% [..........................................................] 18632 / 186321119\n",
      "100% [..........................................................] 18632 / 186321120\n",
      "100% [..........................................................] 18597 / 185971121\n",
      "100% [..........................................................] 18570 / 185701122\n",
      "100% [..........................................................] 18601 / 186011123\n",
      "100% [..........................................................] 18570 / 185701124\n",
      "100% [..........................................................] 18608 / 186081125\n",
      "100% [..........................................................] 18594 / 185941126\n",
      "100% [..........................................................] 18627 / 186271127\n",
      "100% [..........................................................] 18613 / 186131128\n",
      "100% [..........................................................] 18605 / 186051129\n",
      "100% [..........................................................] 18577 / 185771130\n",
      "100% [..........................................................] 18594 / 185941131\n",
      "100% [..........................................................] 18586 / 185861132\n",
      "100% [..........................................................] 18614 / 186141133\n",
      "100% [..........................................................] 18605 / 186051134\n",
      "100% [..........................................................] 18542 / 185421135\n",
      "100% [..........................................................] 18632 / 186321136\n",
      "100% [..........................................................] 18555 / 185551137\n",
      "100% [..........................................................] 18594 / 185941138\n",
      "100% [..........................................................] 18605 / 186051139\n",
      "100% [..........................................................] 18606 / 186061140\n",
      "100% [..........................................................] 18606 / 186061141\n",
      "100% [..........................................................] 18603 / 186031142\n",
      "100% [..........................................................] 18664 / 186641143\n",
      "100% [..........................................................] 18575 / 185751144\n",
      "100% [..........................................................] 18593 / 185931145\n",
      "100% [..........................................................] 18556 / 185561146\n",
      "100% [..........................................................] 18556 / 185561147\n",
      "100% [..........................................................] 18602 / 186021148\n",
      "100% [..........................................................] 18589 / 185891149\n",
      "100% [..........................................................] 18542 / 185421150\n",
      "100% [..........................................................] 18581 / 185811151\n",
      "100% [..........................................................] 18543 / 185431152\n",
      "100% [..........................................................] 18542 / 185421153\n",
      "100% [..........................................................] 18568 / 185681154\n",
      "100% [..........................................................] 18605 / 186051155\n",
      "100% [..........................................................] 18542 / 185421156\n",
      "100% [..........................................................] 18542 / 185421157\n",
      "100% [..........................................................] 18553 / 185531158\n",
      "100% [..........................................................] 18542 / 185421159\n",
      "100% [..........................................................] 18557 / 185571160\n",
      "100% [..........................................................] 18575 / 185751161\n",
      "100% [..........................................................] 18553 / 185531162\n",
      "100% [..........................................................] 18598 / 185981163\n",
      "100% [..........................................................] 18591 / 185911164\n",
      "100% [..........................................................] 18570 / 185701165\n",
      "100% [..........................................................] 18553 / 185531166\n",
      "100% [..........................................................] 18542 / 185421167\n",
      "100% [..........................................................] 18575 / 185751168\n",
      "100% [..........................................................] 18599 / 185991169\n",
      "100% [..........................................................] 18596 / 185961170\n",
      "100% [..........................................................] 18632 / 186321171\n",
      "100% [..........................................................] 18594 / 185941172\n",
      "100% [..........................................................] 18555 / 185551173\n",
      "100% [..........................................................] 18591 / 185911174\n",
      "100% [..........................................................] 18542 / 185421175\n",
      "100% [..........................................................] 18585 / 185851176\n",
      "100% [..........................................................] 18542 / 185421177\n",
      "100% [..........................................................] 18542 / 185421178\n",
      "100% [..........................................................] 18599 / 185991179\n",
      "100% [..........................................................] 18568 / 185681180\n",
      "100% [..........................................................] 18587 / 185871181\n",
      "100% [..........................................................] 18575 / 185751182\n",
      "100% [..........................................................] 18606 / 186061183\n",
      "100% [..........................................................] 18599 / 185991184\n",
      "100% [..........................................................] 18587 / 185871185\n",
      "100% [..........................................................] 18554 / 185541186\n",
      "100% [..........................................................] 18555 / 185551187\n",
      "100% [..........................................................] 18554 / 185541188\n",
      "100% [..........................................................] 18554 / 185541189\n",
      "100% [..........................................................] 18553 / 185531190\n",
      "100% [..........................................................] 18544 / 185441191\n",
      "100% [..........................................................] 18542 / 185421192\n",
      "100% [..........................................................] 18605 / 186051193\n",
      "100% [..........................................................] 18542 / 185421194\n",
      "100% [..........................................................] 18542 / 185421195\n",
      "100% [..........................................................] 18598 / 185981196\n",
      "100% [..........................................................] 18553 / 185531197\n",
      "100% [..........................................................] 18542 / 185421198\n",
      "100% [..........................................................] 18542 / 185421199\n",
      "100% [..........................................................] 18610 / 186101200\n",
      "100% [..........................................................] 18585 / 185851201\n",
      "100% [..........................................................] 18545 / 185451202\n",
      "100% [..........................................................] 18596 / 185961203\n",
      "100% [..........................................................] 18542 / 185421204\n",
      "100% [..........................................................] 18595 / 185951205\n",
      "100% [..........................................................] 18597 / 185971206\n",
      "100% [..........................................................] 18542 / 185421207\n",
      "100% [..........................................................] 18597 / 185971208\n",
      "100% [..........................................................] 18603 / 186031209\n",
      "100% [..........................................................] 18594 / 185941210\n",
      "100% [..........................................................] 18586 / 185861211\n",
      "100% [..........................................................] 18586 / 185861212\n",
      "100% [..........................................................] 18598 / 185981213\n",
      "100% [..........................................................] 18614 / 186141214\n",
      "100% [..........................................................] 18547 / 185471215\n",
      "100% [..........................................................] 18546 / 185461216\n",
      "100% [..........................................................] 18553 / 185531217\n",
      "100% [..........................................................] 18553 / 185531218\n",
      "100% [..........................................................] 18555 / 185551219\n",
      "100% [..........................................................] 18553 / 185531220\n",
      "100% [..........................................................] 18603 / 186031221\n",
      "100% [..........................................................] 18553 / 185531222\n",
      "100% [..........................................................] 18586 / 185861223\n",
      "100% [..........................................................] 18575 / 185751224\n",
      "100% [..........................................................] 18555 / 185551225\n",
      "100% [..........................................................] 18575 / 185751226\n",
      "100% [..........................................................] 18577 / 185771227\n",
      "100% [..........................................................] 18544 / 185441228\n",
      "100% [..........................................................] 18553 / 185531229\n",
      "100% [..........................................................] 18544 / 185441230\n",
      "100% [..........................................................] 18542 / 185421231\n",
      "100% [..........................................................] 18544 / 185441232\n",
      "100% [..........................................................] 18591 / 185911233\n",
      "100% [..........................................................] 18542 / 185421234\n",
      "100% [..........................................................] 18553 / 185531235\n",
      "100% [..........................................................] 18542 / 185421236\n",
      "100% [..........................................................] 18542 / 185421237\n",
      "100% [..........................................................] 18542 / 185421238\n",
      "100% [..........................................................] 18542 / 185421239\n",
      "100% [..........................................................] 18594 / 185941240\n",
      "100% [..........................................................] 18600 / 186001241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [..........................................................] 18606 / 186061242\n",
      "100% [..........................................................] 18547 / 185471243\n",
      "100% [..........................................................] 18542 / 185421244\n",
      "100% [..........................................................] 18601 / 186011245\n",
      "100% [..........................................................] 18544 / 185441246\n",
      "100% [..........................................................] 18577 / 185771247\n",
      "100% [..........................................................] 18576 / 185761248\n",
      "100% [..........................................................] 18588 / 185881249\n",
      "100% [..........................................................] 18596 / 185961250\n",
      "100% [..........................................................] 18604 / 186041251\n",
      "100% [..........................................................] 18605 / 186051252\n",
      "100% [..........................................................] 18613 / 186131253\n",
      "100% [..........................................................] 18622 / 186221254\n",
      "100% [..........................................................] 18575 / 185751255\n",
      "100% [..........................................................] 18542 / 185421256\n",
      "100% [..........................................................] 18542 / 185421257\n",
      "100% [..........................................................] 18553 / 185531258\n",
      "100% [..........................................................] 18542 / 185421259\n",
      "100% [..........................................................] 18575 / 185751260\n",
      "100% [..........................................................] 18542 / 185421261\n",
      "100% [..........................................................] 18631 / 186311262\n",
      "100% [..........................................................] 18554 / 185541263\n",
      "100% [..........................................................] 18615 / 186151264\n",
      "100% [..........................................................] 18554 / 185541265\n",
      "100% [..........................................................] 18599 / 185991266\n",
      "100% [..........................................................] 18588 / 185881267\n",
      "100% [..........................................................] 18542 / 185421268\n",
      "100% [..........................................................] 18542 / 185421269\n",
      "100% [..........................................................] 18555 / 185551270\n",
      "100% [..........................................................] 18596 / 185961271\n",
      "100% [..........................................................] 18575 / 185751272\n",
      "100% [..........................................................] 18598 / 185981273\n",
      "100% [..........................................................] 18599 / 185991274\n",
      "100% [..........................................................] 18597 / 185971275\n",
      "100% [..........................................................] 18542 / 185421276\n",
      "100% [..........................................................] 18580 / 185801277\n",
      "100% [..........................................................] 18575 / 185751278\n",
      "100% [..........................................................] 18542 / 185421279\n",
      "100% [..........................................................] 18553 / 185531280\n",
      "100% [..........................................................] 18588 / 185881281\n",
      "100% [..........................................................] 18553 / 185531282\n",
      "100% [..........................................................] 18575 / 185751283\n",
      "100% [..........................................................] 18600 / 186001284\n",
      "100% [..........................................................] 18557 / 185571285\n",
      "100% [..........................................................] 18543 / 185431286\n",
      "100% [..........................................................] 18542 / 185421287\n",
      "100% [..........................................................] 18623 / 186231288\n",
      "100% [..........................................................] 18553 / 185531289\n",
      "100% [..........................................................] 18591 / 185911290\n",
      "100% [..........................................................] 18589 / 185891291\n",
      "100% [..........................................................] 18624 / 186241292\n",
      "100% [..........................................................] 18555 / 185551293\n",
      "100% [..........................................................] 18544 / 185441294\n",
      "100% [..........................................................] 18543 / 185431295\n",
      "100% [..........................................................] 18542 / 185421296\n",
      "100% [..........................................................] 18553 / 185531297\n",
      "100% [..........................................................] 18575 / 185751298\n",
      "100% [..........................................................] 18600 / 186001299\n",
      "100% [..........................................................] 18554 / 185541300\n",
      "100% [..........................................................] 18580 / 185801301\n",
      "100% [..........................................................] 18603 / 186031302\n",
      "100% [..........................................................] 18570 / 185701303\n",
      "100% [..........................................................] 18605 / 186051304\n",
      "100% [..........................................................] 18557 / 185571305\n",
      "100% [..........................................................] 18557 / 185571306\n",
      "100% [..........................................................] 18575 / 185751307\n",
      "100% [..........................................................] 18663 / 186631308\n",
      "100% [..........................................................] 18599 / 185991309\n",
      "100% [..........................................................] 18542 / 185421310\n",
      "100% [..........................................................] 18553 / 185531311\n",
      "100% [..........................................................] 18594 / 185941312\n",
      "100% [..........................................................] 18542 / 185421313\n",
      "100% [..........................................................] 18542 / 185421314\n",
      "100% [..........................................................] 18591 / 185911315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit ERROR: [18:14:47] Explicit valence for atom # 3 Cl, 3, is greater than permitted\n",
      "[18:14:47] Explicit valence for atom # 3 Cl, 3, is greater than permitted\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "quote_from_bytes() expected bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [89]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mNp_pathways\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Users/mahnoorzulfiqar/OneDriveUNI/MZML\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresultcsv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Users/mahnoorzulfiqar/OneDriveUNI/MZML/MetabolomicsResults/Final_Candidate_List.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [88]\u001b[0m, in \u001b[0;36mNp_pathways\u001b[0;34m(input_dir, resultcsv)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquote_plus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcvv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://npclassifier.ucsd.edu/classify?smiles=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mc\n\u001b[1;32m     14\u001b[0m names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid_X\u001b[39m\u001b[38;5;124m'\u001b[39m][i])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mawRpy/lib/python3.10/urllib/parse.py:886\u001b[0m, in \u001b[0;36mquote_plus\u001b[0;34m(string, safe, encoding, errors)\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    885\u001b[0m     space \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 886\u001b[0m string \u001b[38;5;241m=\u001b[39m \u001b[43mquote\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m string\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m+\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mawRpy/lib/python3.10/urllib/parse.py:870\u001b[0m, in \u001b[0;36mquote\u001b[0;34m(string, safe, encoding, errors)\u001b[0m\n\u001b[1;32m    868\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    869\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquote() doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt support \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merrors\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for bytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 870\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mquote_from_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mawRpy/lib/python3.10/urllib/parse.py:895\u001b[0m, in \u001b[0;36mquote_from_bytes\u001b[0;34m(bs, safe)\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;124;03m\"\"\"Like quote(), but accepts a bytes object rather than a str, and does\u001b[39;00m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;124;03mnot perform string-to-bytes encoding.  It always returns an ASCII string.\u001b[39;00m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;124;03mquote_from_bytes(b'abc def\\x3f') -> 'abc%20def%3f'\u001b[39;00m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(bs, (\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mbytearray\u001b[39m)):\n\u001b[0;32m--> 895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquote_from_bytes() expected bytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    896\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m bs:\n\u001b[1;32m    897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: quote_from_bytes() expected bytes"
     ]
    }
   ],
   "source": [
    "Np_pathways(input_dir = \"/Users/mahnoorzulfiqar/OneDriveUNI/MZML\", \n",
    "            resultcsv = \"/Users/mahnoorzulfiqar/OneDriveUNI/MZML/MetabolomicsResults/Final_Candidate_List.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac455165",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/mahnoorzulfiqar/OneDriveUNI/MZML/MetabolomicsResults/Final_Candidate_List.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8dc2eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc051ea9",
   "metadata": {},
   "source": [
    "## Chemical Similarity MN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "24fa0427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chemMN(input_dir, resultcsv):\n",
    "    #read csv\n",
    "    df = pd.read_csv(resultcsv)\n",
    "    \n",
    "    # define empty variable\n",
    "    dbn= []\n",
    "\n",
    "    # check the result csv\n",
    "    for i, row in df.iterrows():\n",
    "        # to compare each element with each opther element\n",
    "        for j, row in df.iterrows():\n",
    "\n",
    "            # if its not same id\n",
    "            if df['SMILES'][i] != df['SMILES'][j]:\n",
    "\n",
    "                if not isNaN(df['SMILES'][i]):\n",
    "                    if not isNaN(df['SMILES'][j]):\n",
    "\n",
    "                        try:\n",
    "                            ms = [Chem.MolFromSmiles(df['SMILES'][i]), Chem.MolFromSmiles(df['SMILES'][j])]\n",
    "                            fps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in ms]\n",
    "                            tn = DataStructs.FingerprintSimilarity(fps[0],fps[1])\n",
    "                            dbn.append({\n",
    "                                'Name_i':df['id_X'][i],\n",
    "                                'Name_j':df['id_X'][j],\n",
    "                                'i': df['SMILES'][i],\n",
    "                                'j': df['SMILES'][j],\n",
    "                                'Tanimoto': tn\n",
    "                            })\n",
    "                        except Exception as e:\n",
    "                            print(i)\n",
    "                            print(j)\n",
    "                            print(e)\n",
    "    # save chemical similarities                    \n",
    "    db_edgenode = pd.DataFrame(dbn)\n",
    "\n",
    "    dfe = []\n",
    "    heavy_atoms = ['C', 'N', 'P', 'O', 'S']\n",
    "    for i, row in db_edgenode.iterrows():        \n",
    "        if 1.0 > db_edgenode['Tanimoto'][i] >= 0.70:\n",
    "            # list of mol used to calaculate the MCSS\n",
    "            n = [Chem.MolFromSmiles(db_edgenode['i'][i]),Chem.MolFromSmiles(db_edgenode['j'][i])]\n",
    "            res = rdFMCS.FindMCS(n)\n",
    "            sm_res = Chem.MolToSmiles(Chem.MolFromSmarts(res.smartsString))\n",
    "            # Check if the MCSS has one of the heavy atoms and whether they are\n",
    "            # more than 3\n",
    "            elem = [ele for ele in heavy_atoms if(ele in sm_res)]\n",
    "            if elem and len(sm_res)>=3:\n",
    "                MCSS_SMILES = Chem.MolToSmiles(Chem.MolFromSmarts(res.smartsString))\n",
    "\n",
    "            dfe.append({\n",
    "                'Start':db_edgenode['Name_i'][i],\n",
    "                'End':db_edgenode['Name_j'][i],\n",
    "                'Tanimoto':db_edgenode['Tanimoto'][i]\n",
    "                'Start_SMILES':i,\n",
    "                'End_SMILES':j,\n",
    "                'MCSS': MCSS_SMILES\n",
    "            })\n",
    "\n",
    "    df_edge = pd.DataFrame(dfe)\n",
    "    df_edge['Start'] = df_edge['Start'].astype(str)\n",
    "    df_edge['End'] = df_edge['End'].astype(str)\n",
    "    df_edge['sorted_row'] = [sorted([a,b]) for a,b in zip(df_edge.Start,df_edge.End)]\n",
    "    df_edge['sorted_row'] = df_edge['sorted_row'].astype(str)\n",
    "    df_edge.drop_duplicates(subset=['sorted_row'], inplace=True)\n",
    "\n",
    "    nodes= []\n",
    "    for i, row in df.iterrows():\n",
    "        n = df['id_X'][i]\n",
    "        nodes.append({\n",
    "            'nodes':n\n",
    "        })\n",
    "\n",
    "    node= pd.DataFrame(nodes)\n",
    "    \n",
    "    \n",
    "    df_edge.to_csv(input_dir + \"/MetabolomicsResults/ChemMNedges.tsv\", sep='\\t')\n",
    "    node.to_csv(input_dir + \"/MetabolomicsResults/ChemMNnodes.csv\", index = False)\n",
    "\n",
    "    newdf = df_edge\n",
    "    newdf['StartAtt']=np.nan\n",
    "    newdf['EndAtt']=np.nan\n",
    "    for i, row in newdf.iterrows():\n",
    "        for j, row in df.iterrows():\n",
    "            if newdf['Start'][i]==df['id_X'][j]:\n",
    "                newdf.loc[i, 'StartAtt'] = df['class'][j]\n",
    "            if newdf['End'][i]==df['id_X'][j]:\n",
    "                newdf.loc[i, 'EndAtt'] = df['class'][j]\n",
    "    newdf.to_csv(input_dir + \"/MetabolomicsResults/ChemMNcys.tsv\", sep='\\t')\n",
    "    \n",
    "    return(newdf)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a2e8f84c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit ERROR: [18:28:49] Explicit valence for atom # 3 Cl, 3, is greater than permitted\n",
      "[18:28:49] Explicit valence for atom # 3 Cl, 3, is greater than permitted\n"
     ]
    },
    {
     "ename": "ArgumentError",
     "evalue": "Python argument types in\n    rdkit.Chem.rdMolDescriptors.GetMorganFingerprintAsBitVect(NoneType, int)\ndid not match C++ signature:\n    GetMorganFingerprintAsBitVect(RDKit::ROMol mol, unsigned int radius, unsigned int nBits=2048, boost::python::api::object invariants=[], boost::python::api::object fromAtoms=[], bool useChirality=False, bool useBondTypes=True, bool useFeatures=False, boost::python::api::object bitInfo=None, bool includeRedundantEnvironments=False)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [101]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m newdf \u001b[38;5;241m=\u001b[39m \u001b[43mchemMN\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Users/mahnoorzulfiqar/OneDriveUNI/MZML\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresultcsv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Users/mahnoorzulfiqar/OneDriveUNI/MZML/MetabolomicsResults/Final_Candidate_List.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [100]\u001b[0m, in \u001b[0;36mchemMN\u001b[0;34m(input_dir, resultcsv)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m isNaN(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSMILES\u001b[39m\u001b[38;5;124m'\u001b[39m][j]):\n\u001b[1;32m     20\u001b[0m     ms \u001b[38;5;241m=\u001b[39m [Chem\u001b[38;5;241m.\u001b[39mMolFromSmiles(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSMILES\u001b[39m\u001b[38;5;124m'\u001b[39m][i]), Chem\u001b[38;5;241m.\u001b[39mMolFromSmiles(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSMILES\u001b[39m\u001b[38;5;124m'\u001b[39m][j])]\n\u001b[0;32m---> 21\u001b[0m     fps \u001b[38;5;241m=\u001b[39m [AllChem\u001b[38;5;241m.\u001b[39mGetMorganFingerprintAsBitVect(x,\u001b[38;5;241m2\u001b[39m, nBits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m ms]\n\u001b[1;32m     22\u001b[0m     tn \u001b[38;5;241m=\u001b[39m DataStructs\u001b[38;5;241m.\u001b[39mFingerprintSimilarity(fps[\u001b[38;5;241m0\u001b[39m],fps[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     23\u001b[0m     dbn\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mName_i\u001b[39m\u001b[38;5;124m'\u001b[39m:df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid_X\u001b[39m\u001b[38;5;124m'\u001b[39m][i],\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mName_j\u001b[39m\u001b[38;5;124m'\u001b[39m:df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid_X\u001b[39m\u001b[38;5;124m'\u001b[39m][j],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTanimoto\u001b[39m\u001b[38;5;124m'\u001b[39m: tn\n\u001b[1;32m     29\u001b[0m     })\n",
      "Input \u001b[0;32mIn [100]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m isNaN(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSMILES\u001b[39m\u001b[38;5;124m'\u001b[39m][j]):\n\u001b[1;32m     20\u001b[0m     ms \u001b[38;5;241m=\u001b[39m [Chem\u001b[38;5;241m.\u001b[39mMolFromSmiles(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSMILES\u001b[39m\u001b[38;5;124m'\u001b[39m][i]), Chem\u001b[38;5;241m.\u001b[39mMolFromSmiles(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSMILES\u001b[39m\u001b[38;5;124m'\u001b[39m][j])]\n\u001b[0;32m---> 21\u001b[0m     fps \u001b[38;5;241m=\u001b[39m [\u001b[43mAllChem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGetMorganFingerprintAsBitVect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnBits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m ms]\n\u001b[1;32m     22\u001b[0m     tn \u001b[38;5;241m=\u001b[39m DataStructs\u001b[38;5;241m.\u001b[39mFingerprintSimilarity(fps[\u001b[38;5;241m0\u001b[39m],fps[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     23\u001b[0m     dbn\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mName_i\u001b[39m\u001b[38;5;124m'\u001b[39m:df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid_X\u001b[39m\u001b[38;5;124m'\u001b[39m][i],\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mName_j\u001b[39m\u001b[38;5;124m'\u001b[39m:df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid_X\u001b[39m\u001b[38;5;124m'\u001b[39m][j],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTanimoto\u001b[39m\u001b[38;5;124m'\u001b[39m: tn\n\u001b[1;32m     29\u001b[0m     })\n",
      "\u001b[0;31mArgumentError\u001b[0m: Python argument types in\n    rdkit.Chem.rdMolDescriptors.GetMorganFingerprintAsBitVect(NoneType, int)\ndid not match C++ signature:\n    GetMorganFingerprintAsBitVect(RDKit::ROMol mol, unsigned int radius, unsigned int nBits=2048, boost::python::api::object invariants=[], boost::python::api::object fromAtoms=[], bool useChirality=False, bool useBondTypes=True, bool useFeatures=False, boost::python::api::object bitInfo=None, bool includeRedundantEnvironments=False)"
     ]
    }
   ],
   "source": [
    "newdf = chemMN(input_dir = \"/Users/mahnoorzulfiqar/OneDriveUNI/MZML\", \n",
    "            resultcsv = \"/Users/mahnoorzulfiqar/OneDriveUNI/MZML/MetabolomicsResults/Final_Candidate_List.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "43daeb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"/Users/mahnoorzulfiqar/OneDriveUNI/MZML\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2af6da09",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"('M498R160ID10', 'M500R303ID30')\",\n",
       " \"('M498R160ID10', 'M526R133ID2')\",\n",
       " \"('M498R160ID10', 'M542R64ID48')\",\n",
       " \"('M500R303ID30', 'M526R133ID2')\",\n",
       " \"('M500R303ID30', 'M542R64ID48')\",\n",
       " \"('M215R334ID38', 'M362R389ID183')\",\n",
       " \"('M215R334ID38', 'M254R44ID28')\",\n",
       " \"('M215R334ID38', 'M291R248ID65')\",\n",
       " \"('M215R334ID38', 'M282R46ID33')\",\n",
       " \"('M215R334ID38', 'M338R54ID44')\",\n",
       " \"('M261R515ID129', 'M803R189ID25')\",\n",
       " \"('M261R515ID129', 'M757R208ID36')\",\n",
       " \"('M261R515ID129', 'M698R217ID38')\",\n",
       " \"('M261R515ID129', 'M670R263ID69')\",\n",
       " \"('M227R68ID39', 'M261R515ID129')\",\n",
       " \"('M261R515ID129', 'M313R98ID52')\",\n",
       " \"('M261R515ID129', 'M313R387ID69')\",\n",
       " \"('M261R515ID129', 'M289R399ID71')\",\n",
       " \"('M241R30ID1', 'M261R515ID129')\",\n",
       " \"('M261R515ID129', 'M324R82ID44')\",\n",
       " \"('M261R515ID129', 'M331R102ID54')\",\n",
       " \"('M195R351ID68', 'M261R515ID129')\",\n",
       " \"('M226R389ID69', 'M261R515ID129')\",\n",
       " \"('M261R515ID129', 'M396R390ID71')\",\n",
       " \"('M261R515ID129', 'M729R108ID58')\",\n",
       " \"('M111R382ID70', 'M261R515ID129')\",\n",
       " \"('M261R515ID129', 'M779R97ID54')\",\n",
       " \"('M261R515ID129', 'M805R108ID58')\",\n",
       " \"('M553R521ID130', 'M95R335ID67')\",\n",
       " \"('M232R100ID55', 'M553R521ID130')\",\n",
       " \"('M488R68ID50', 'M526R133ID2')\",\n",
       " \"('M526R133ID2', 'M568R64ID47')\",\n",
       " \"('M757R208ID36', 'M803R189ID25')\",\n",
       " \"('M698R217ID38', 'M803R189ID25')\",\n",
       " \"('M731R212ID39', 'M803R189ID25')\",\n",
       " \"('M670R263ID69', 'M803R189ID25')\",\n",
       " \"('M674R283ID80', 'M803R189ID25')\",\n",
       " \"('M676R284ID81', 'M803R189ID25')\",\n",
       " \"('M678R284ID82', 'M803R189ID25')\",\n",
       " \"('M217R80ID42', 'M803R189ID25')\",\n",
       " \"('M289R399ID71', 'M803R189ID25')\",\n",
       " \"('M195R351ID68', 'M803R189ID25')\",\n",
       " \"('M542R64ID48', 'M803R189ID25')\",\n",
       " \"('M488R68ID50', 'M803R189ID25')\",\n",
       " \"('M729R108ID58', 'M803R189ID25')\",\n",
       " \"('M111R382ID70', 'M803R189ID25')\",\n",
       " \"('M779R97ID54', 'M803R189ID25')\",\n",
       " \"('M803R189ID25', 'M805R108ID58')\",\n",
       " \"('M648R61ID47', 'M803R189ID25')\",\n",
       " \"('M568R64ID47', 'M803R189ID25')\",\n",
       " \"('M175R355ID69', 'M314R206ID34')\",\n",
       " \"('M698R217ID38', 'M757R208ID36')\",\n",
       " \"('M731R212ID39', 'M757R208ID36')\",\n",
       " \"('M670R263ID69', 'M757R208ID36')\",\n",
       " \"('M674R283ID80', 'M757R208ID36')\",\n",
       " \"('M676R284ID81', 'M757R208ID36')\",\n",
       " \"('M678R284ID82', 'M757R208ID36')\",\n",
       " \"('M522R298ID90', 'M757R208ID36')\",\n",
       " \"('M544R300ID94', 'M757R208ID36')\",\n",
       " \"('M466R313ID105', 'M757R208ID36')\",\n",
       " \"('M227R68ID39', 'M757R208ID36')\",\n",
       " \"('M217R80ID42', 'M757R208ID36')\",\n",
       " \"('M313R98ID52', 'M757R208ID36')\",\n",
       " \"('M313R387ID69', 'M757R208ID36')\",\n",
       " \"('M289R399ID71', 'M757R208ID36')\",\n",
       " \"('M241R30ID1', 'M757R208ID36')\",\n",
       " \"('M324R82ID44', 'M757R208ID36')\",\n",
       " \"('M195R351ID68', 'M757R208ID36')\",\n",
       " \"('M396R390ID71', 'M757R208ID36')\",\n",
       " \"('M494R66ID49', 'M757R208ID36')\",\n",
       " \"('M111R382ID70', 'M757R208ID36')\",\n",
       " \"('M757R208ID36', 'M779R97ID54')\",\n",
       " \"('M757R208ID36', 'M805R108ID58')\",\n",
       " \"('M648R61ID47', 'M757R208ID36')\",\n",
       " \"('M520R64ID48', 'M757R208ID36')\",\n",
       " \"('M698R217ID38', 'M731R212ID39')\",\n",
       " \"('M670R263ID69', 'M698R217ID38')\",\n",
       " \"('M674R283ID80', 'M698R217ID38')\",\n",
       " \"('M676R284ID81', 'M698R217ID38')\",\n",
       " \"('M678R284ID82', 'M698R217ID38')\",\n",
       " \"('M522R298ID90', 'M698R217ID38')\",\n",
       " \"('M544R300ID94', 'M698R217ID38')\",\n",
       " \"('M466R313ID105', 'M698R217ID38')\",\n",
       " \"('M227R68ID39', 'M698R217ID38')\",\n",
       " \"('M217R80ID42', 'M698R217ID38')\",\n",
       " \"('M313R98ID52', 'M698R217ID38')\",\n",
       " \"('M313R387ID69', 'M698R217ID38')\",\n",
       " \"('M289R399ID71', 'M698R217ID38')\",\n",
       " \"('M241R30ID1', 'M698R217ID38')\",\n",
       " \"('M324R82ID44', 'M698R217ID38')\",\n",
       " \"('M195R351ID68', 'M698R217ID38')\",\n",
       " \"('M396R390ID71', 'M698R217ID38')\",\n",
       " \"('M494R66ID49', 'M698R217ID38')\",\n",
       " \"('M698R217ID38', 'M729R108ID58')\",\n",
       " \"('M111R382ID70', 'M698R217ID38')\",\n",
       " \"('M698R217ID38', 'M779R97ID54')\",\n",
       " \"('M698R217ID38', 'M805R108ID58')\",\n",
       " \"('M648R61ID47', 'M698R217ID38')\",\n",
       " \"('M520R64ID48', 'M698R217ID38')\",\n",
       " \"('M670R263ID69', 'M731R212ID39')\",\n",
       " \"('M674R283ID80', 'M731R212ID39')\",\n",
       " \"('M676R284ID81', 'M731R212ID39')\",\n",
       " \"('M678R284ID82', 'M731R212ID39')\",\n",
       " \"('M522R298ID90', 'M731R212ID39')\",\n",
       " \"('M466R313ID105', 'M731R212ID39')\",\n",
       " \"('M227R68ID39', 'M731R212ID39')\",\n",
       " \"('M217R80ID42', 'M731R212ID39')\",\n",
       " \"('M313R98ID52', 'M731R212ID39')\",\n",
       " \"('M313R387ID69', 'M731R212ID39')\",\n",
       " \"('M289R399ID71', 'M731R212ID39')\",\n",
       " \"('M241R30ID1', 'M731R212ID39')\",\n",
       " \"('M324R82ID44', 'M731R212ID39')\",\n",
       " \"('M195R351ID68', 'M731R212ID39')\",\n",
       " \"('M492R66ID49', 'M731R212ID39')\",\n",
       " \"('M396R390ID71', 'M731R212ID39')\",\n",
       " \"('M494R66ID49', 'M731R212ID39')\",\n",
       " \"('M729R108ID58', 'M731R212ID39')\",\n",
       " \"('M111R382ID70', 'M731R212ID39')\",\n",
       " \"('M731R212ID39', 'M779R97ID54')\",\n",
       " \"('M731R212ID39', 'M805R108ID58')\",\n",
       " \"('M648R61ID47', 'M731R212ID39')\",\n",
       " \"('M520R64ID48', 'M731R212ID39')\",\n",
       " \"('M670R263ID69', 'M674R283ID80')\",\n",
       " \"('M670R263ID69', 'M676R284ID81')\",\n",
       " \"('M670R263ID69', 'M678R284ID82')\",\n",
       " \"('M217R80ID42', 'M670R263ID69')\",\n",
       " \"('M670R263ID69', 'M729R108ID58')\",\n",
       " \"('M670R263ID69', 'M779R97ID54')\",\n",
       " \"('M670R263ID69', 'M805R108ID58')\",\n",
       " \"('M648R61ID47', 'M670R263ID69')\",\n",
       " \"('M130R129ID60', 'M295R286ID78')\",\n",
       " \"('M674R283ID80', 'M676R284ID81')\",\n",
       " \"('M674R283ID80', 'M678R284ID82')\",\n",
       " \"('M522R298ID90', 'M674R283ID80')\",\n",
       " \"('M227R68ID39', 'M674R283ID80')\",\n",
       " \"('M217R80ID42', 'M674R283ID80')\",\n",
       " \"('M313R98ID52', 'M674R283ID80')\",\n",
       " \"('M313R387ID69', 'M674R283ID80')\",\n",
       " \"('M289R399ID71', 'M674R283ID80')\",\n",
       " \"('M241R30ID1', 'M674R283ID80')\",\n",
       " \"('M195R351ID68', 'M674R283ID80')\",\n",
       " \"('M396R390ID71', 'M674R283ID80')\",\n",
       " \"('M494R66ID49', 'M674R283ID80')\",\n",
       " \"('M674R283ID80', 'M729R108ID58')\",\n",
       " \"('M111R382ID70', 'M674R283ID80')\",\n",
       " \"('M674R283ID80', 'M779R97ID54')\",\n",
       " \"('M674R283ID80', 'M805R108ID58')\",\n",
       " \"('M648R61ID47', 'M674R283ID80')\",\n",
       " \"('M520R64ID48', 'M674R283ID80')\",\n",
       " \"('M676R284ID81', 'M678R284ID82')\",\n",
       " \"('M522R298ID90', 'M676R284ID81')\",\n",
       " \"('M227R68ID39', 'M676R284ID81')\",\n",
       " \"('M217R80ID42', 'M676R284ID81')\",\n",
       " \"('M313R98ID52', 'M676R284ID81')\",\n",
       " \"('M313R387ID69', 'M676R284ID81')\",\n",
       " \"('M289R399ID71', 'M676R284ID81')\",\n",
       " \"('M241R30ID1', 'M676R284ID81')\",\n",
       " \"('M195R351ID68', 'M676R284ID81')\",\n",
       " \"('M396R390ID71', 'M676R284ID81')\",\n",
       " \"('M494R66ID49', 'M676R284ID81')\",\n",
       " \"('M676R284ID81', 'M729R108ID58')\",\n",
       " \"('M111R382ID70', 'M676R284ID81')\",\n",
       " \"('M676R284ID81', 'M779R97ID54')\",\n",
       " \"('M676R284ID81', 'M805R108ID58')\",\n",
       " \"('M648R61ID47', 'M676R284ID81')\",\n",
       " \"('M520R64ID48', 'M676R284ID81')\",\n",
       " \"('M313R387ID69', 'M678R284ID82')\",\n",
       " \"('M241R30ID1', 'M678R284ID82')\",\n",
       " \"('M396R390ID71', 'M678R284ID82')\",\n",
       " \"('M678R284ID82', 'M729R108ID58')\",\n",
       " \"('M678R284ID82', 'M779R97ID54')\",\n",
       " \"('M678R284ID82', 'M805R108ID58')\",\n",
       " \"('M648R61ID47', 'M678R284ID82')\",\n",
       " \"('M468R305ID87', 'M522R298ID90')\",\n",
       " \"('M468R305ID87', 'M544R300ID94')\",\n",
       " \"('M466R313ID105', 'M468R305ID87')\",\n",
       " \"('M209R334ID67', 'M468R305ID87')\",\n",
       " \"('M468R305ID87', 'M492R66ID49')\",\n",
       " \"('M468R305ID87', 'M494R66ID49')\",\n",
       " \"('M468R305ID87', 'M516R64ID48')\",\n",
       " \"('M468R305ID87', 'M520R64ID48')\",\n",
       " \"('M522R298ID90', 'M544R300ID94')\",\n",
       " \"('M466R313ID105', 'M522R298ID90')\",\n",
       " \"('M522R298ID90', 'M97R104ID56')\",\n",
       " \"('M481R40ID9', 'M522R298ID90')\",\n",
       " \"('M522R298ID90', 'M542R64ID48')\",\n",
       " \"('M492R66ID49', 'M522R298ID90')\",\n",
       " \"('M522R298ID90', 'M729R108ID58')\",\n",
       " \"('M522R298ID90', 'M648R61ID47')\",\n",
       " \"('M516R64ID48', 'M522R298ID90')\",\n",
       " \"('M520R64ID48', 'M522R298ID90')\",\n",
       " \"('M466R313ID105', 'M544R300ID94')\",\n",
       " \"('M542R64ID48', 'M544R300ID94')\",\n",
       " \"('M492R66ID49', 'M544R300ID94')\",\n",
       " \"('M494R66ID49', 'M544R300ID94')\",\n",
       " \"('M488R68ID50', 'M544R300ID94')\",\n",
       " \"('M544R300ID94', 'M729R108ID58')\",\n",
       " \"('M544R300ID94', 'M779R97ID54')\",\n",
       " \"('M516R64ID48', 'M544R300ID94')\",\n",
       " \"('M544R300ID94', 'M568R64ID47')\",\n",
       " \"('M520R64ID48', 'M544R300ID94')\",\n",
       " \"('M466R313ID105', 'M97R104ID56')\",\n",
       " \"('M466R313ID105', 'M481R40ID9')\",\n",
       " \"('M466R313ID105', 'M542R64ID48')\",\n",
       " \"('M466R313ID105', 'M492R66ID49')\",\n",
       " \"('M466R313ID105', 'M494R66ID49')\",\n",
       " \"('M466R313ID105', 'M729R108ID58')\",\n",
       " \"('M466R313ID105', 'M648R61ID47')\",\n",
       " \"('M466R313ID105', 'M516R64ID48')\",\n",
       " \"('M466R313ID105', 'M520R64ID48')\",\n",
       " \"('M514R316ID108', 'M542R64ID48')\",\n",
       " \"('M488R68ID50', 'M514R316ID108')\",\n",
       " \"('M514R316ID108', 'M568R64ID47')\",\n",
       " \"('M260R340ID130', 'M291R129ID60')\",\n",
       " \"('M229R161ID62', 'M412R358ID141')\",\n",
       " \"('M273R33ID6', 'M412R358ID141')\",\n",
       " \"('M136R388ID170', 'M581R49ID39')\",\n",
       " \"('M150R431ID212', 'M204R149ID62')\",\n",
       " \"('M536R66ID38', 'M538R65ID36')\",\n",
       " \"('M481R40ID9', 'M536R66ID38')\",\n",
       " \"('M791R47ID36', 'M808R42ID14')\",\n",
       " \"('M724R55ID44', 'M808R42ID14')\",\n",
       " \"('M739R45ID31', 'M808R42ID14')\",\n",
       " \"('M227R68ID39', 'M313R98ID52')\",\n",
       " \"('M227R68ID39', 'M313R387ID69')\",\n",
       " \"('M227R68ID39', 'M289R399ID71')\",\n",
       " \"('M227R68ID39', 'M241R30ID1')\",\n",
       " \"('M227R68ID39', 'M255R82ID43')\",\n",
       " \"('M227R68ID39', 'M324R82ID44')\",\n",
       " \"('M227R68ID39', 'M331R102ID54')\",\n",
       " \"('M227R68ID39', 'M97R104ID56')\",\n",
       " \"('M195R351ID68', 'M227R68ID39')\",\n",
       " \"('M226R389ID69', 'M227R68ID39')\",\n",
       " \"('M227R68ID39', 'M396R390ID71')\",\n",
       " \"('M227R68ID39', 'M729R108ID58')\",\n",
       " \"('M111R382ID70', 'M227R68ID39')\",\n",
       " \"('M227R68ID39', 'M779R97ID54')\",\n",
       " \"('M227R68ID39', 'M805R108ID58')\",\n",
       " \"('M227R68ID39', 'M648R61ID47')\",\n",
       " \"('M217R80ID42', 'M729R108ID58')\",\n",
       " \"('M217R80ID42', 'M779R97ID54')\",\n",
       " \"('M217R80ID42', 'M805R108ID58')\",\n",
       " \"('M313R387ID69', 'M313R98ID52')\",\n",
       " \"('M289R399ID71', 'M313R98ID52')\",\n",
       " \"('M241R30ID1', 'M313R98ID52')\",\n",
       " \"('M313R98ID52', 'M324R82ID44')\",\n",
       " \"('M313R98ID52', 'M97R104ID56')\",\n",
       " \"('M195R351ID68', 'M313R98ID52')\",\n",
       " \"('M226R389ID69', 'M313R98ID52')\",\n",
       " \"('M313R98ID52', 'M396R390ID71')\",\n",
       " \"('M313R98ID52', 'M729R108ID58')\",\n",
       " \"('M111R382ID70', 'M313R98ID52')\",\n",
       " \"('M313R98ID52', 'M805R108ID58')\",\n",
       " \"('M289R399ID71', 'M313R387ID69')\",\n",
       " \"('M255R82ID43', 'M313R387ID69')\",\n",
       " \"('M313R387ID69', 'M324R82ID44')\",\n",
       " \"('M313R387ID69', 'M97R104ID56')\",\n",
       " \"('M195R351ID68', 'M313R387ID69')\",\n",
       " \"('M226R389ID69', 'M313R387ID69')\",\n",
       " \"('M313R387ID69', 'M729R108ID58')\",\n",
       " \"('M111R382ID70', 'M313R387ID69')\",\n",
       " \"('M313R387ID69', 'M805R108ID58')\",\n",
       " \"('M313R387ID69', 'M648R61ID47')\",\n",
       " \"('M241R30ID1', 'M289R399ID71')\",\n",
       " \"('M255R82ID43', 'M289R399ID71')\",\n",
       " \"('M289R399ID71', 'M324R82ID44')\",\n",
       " \"('M289R399ID71', 'M331R102ID54')\",\n",
       " \"('M289R399ID71', 'M97R104ID56')\",\n",
       " \"('M226R389ID69', 'M289R399ID71')\",\n",
       " \"('M289R399ID71', 'M396R390ID71')\",\n",
       " \"('M289R399ID71', 'M729R108ID58')\",\n",
       " \"('M289R399ID71', 'M779R97ID54')\",\n",
       " \"('M289R399ID71', 'M805R108ID58')\",\n",
       " \"('M289R399ID71', 'M648R61ID47')\",\n",
       " \"('M241R30ID1', 'M255R82ID43')\",\n",
       " \"('M241R30ID1', 'M324R82ID44')\",\n",
       " \"('M241R30ID1', 'M97R104ID56')\",\n",
       " \"('M195R351ID68', 'M241R30ID1')\",\n",
       " \"('M226R389ID69', 'M241R30ID1')\",\n",
       " \"('M241R30ID1', 'M729R108ID58')\",\n",
       " \"('M111R382ID70', 'M241R30ID1')\",\n",
       " \"('M241R30ID1', 'M805R108ID58')\",\n",
       " \"('M241R30ID1', 'M648R61ID47')\",\n",
       " \"('M229R161ID62', 'M347R43ID17')\",\n",
       " \"('M273R33ID6', 'M347R43ID17')\",\n",
       " \"('M229R161ID62', 'M269R44ID18')\",\n",
       " \"('M269R44ID18', 'M273R33ID6')\",\n",
       " \"('M255R82ID43', 'M324R82ID44')\",\n",
       " \"('M195R351ID68', 'M255R82ID43')\",\n",
       " \"('M226R389ID69', 'M255R82ID43')\",\n",
       " \"('M255R82ID43', 'M396R390ID71')\",\n",
       " \"('M111R382ID70', 'M255R82ID43')\",\n",
       " \"('M324R82ID44', 'M97R104ID56')\",\n",
       " \"('M195R351ID68', 'M324R82ID44')\",\n",
       " \"('M226R389ID69', 'M324R82ID44')\",\n",
       " \"('M324R82ID44', 'M396R390ID71')\",\n",
       " \"('M324R82ID44', 'M729R108ID58')\",\n",
       " \"('M111R382ID70', 'M324R82ID44')\",\n",
       " \"('M324R82ID44', 'M648R61ID47')\",\n",
       " \"('M195R351ID68', 'M331R102ID54')\",\n",
       " \"('M226R389ID69', 'M331R102ID54')\",\n",
       " \"('M111R382ID70', 'M331R102ID54')\",\n",
       " \"('M209R334ID67', 'M97R104ID56')\",\n",
       " \"('M195R351ID68', 'M97R104ID56')\",\n",
       " \"('M492R66ID49', 'M97R104ID56')\",\n",
       " \"('M396R390ID71', 'M97R104ID56')\",\n",
       " \"('M494R66ID49', 'M97R104ID56')\",\n",
       " \"('M111R382ID70', 'M97R104ID56')\",\n",
       " \"('M520R64ID48', 'M97R104ID56')\",\n",
       " \"('M209R334ID67', 'M226R389ID69')\",\n",
       " \"('M135R367ID70', 'M209R334ID67')\",\n",
       " \"('M195R351ID68', 'M226R389ID69')\",\n",
       " \"('M195R351ID68', 'M396R390ID71')\",\n",
       " \"('M195R351ID68', 'M729R108ID58')\",\n",
       " \"('M195R351ID68', 'M779R97ID54')\",\n",
       " \"('M195R351ID68', 'M805R108ID58')\",\n",
       " \"('M195R351ID68', 'M648R61ID47')\",\n",
       " \"('M135R367ID70', 'M226R389ID69')\",\n",
       " \"('M226R389ID69', 'M396R390ID71')\",\n",
       " \"('M111R382ID70', 'M226R389ID69')\",\n",
       " \"('M218R119ID60', 'M95R335ID67')\",\n",
       " \"('M256R401ID75', 'M95R335ID67')\",\n",
       " \"('M761R45ID20', 'M763R44ID19')\",\n",
       " \"('M538R65ID36', 'M564R65ID37')\",\n",
       " \"('M481R40ID9', 'M564R65ID37')\",\n",
       " \"('M130R129ID60', 'M380R103ID55')\",\n",
       " \"('M481R40ID9', 'M494R66ID49')\",\n",
       " \"('M145R51ID40', 'M481R40ID9')\",\n",
       " \"('M481R40ID9', 'M520R64ID48')\",\n",
       " \"('M132R259ID66', 'M315R43ID16')\",\n",
       " \"('M309R44ID29', 'M344R86ID46')\",\n",
       " \"('M344R86ID46', 'M408R46ID33')\",\n",
       " \"('M311R39ID19', 'M313R39ID19')\",\n",
       " \"('M724R55ID44', 'M791R47ID36')\",\n",
       " \"('M739R45ID31', 'M791R47ID36')\",\n",
       " \"('M492R66ID49', 'M542R64ID48')\",\n",
       " \"('M494R66ID49', 'M542R64ID48')\",\n",
       " \"('M488R68ID50', 'M542R64ID48')\",\n",
       " \"('M542R64ID48', 'M779R97ID54')\",\n",
       " \"('M542R64ID48', 'M805R108ID58')\",\n",
       " \"('M516R64ID48', 'M542R64ID48')\",\n",
       " \"('M542R64ID48', 'M568R64ID47')\",\n",
       " \"('M520R64ID48', 'M542R64ID48')\",\n",
       " \"('M492R66ID49', 'M494R66ID49')\",\n",
       " \"('M492R66ID49', 'M516R64ID48')\",\n",
       " \"('M492R66ID49', 'M520R64ID48')\",\n",
       " \"('M218R119ID60', 'M232R100ID55')\",\n",
       " \"('M218R119ID60', 'M246R92ID53')\",\n",
       " \"('M215R123ID60', 'M217R207ID64')\",\n",
       " \"('M156R297ID66', 'M175R355ID69')\",\n",
       " \"('M396R390ID71', 'M729R108ID58')\",\n",
       " \"('M111R382ID70', 'M396R390ID71')\",\n",
       " \"('M396R390ID71', 'M805R108ID58')\",\n",
       " \"('M396R390ID71', 'M648R61ID47')\",\n",
       " \"('M494R66ID49', 'M729R108ID58')\",\n",
       " \"('M494R66ID49', 'M648R61ID47')\",\n",
       " \"('M494R66ID49', 'M516R64ID48')\",\n",
       " \"('M494R66ID49', 'M520R64ID48')\",\n",
       " \"('M488R68ID50', 'M520R64ID48')\",\n",
       " \"('M111R382ID70', 'M729R108ID58')\",\n",
       " \"('M729R108ID58', 'M779R97ID54')\",\n",
       " \"('M729R108ID58', 'M805R108ID58')\",\n",
       " \"('M648R61ID47', 'M729R108ID58')\",\n",
       " \"('M520R64ID48', 'M729R108ID58')\",\n",
       " \"('M111R382ID70', 'M779R97ID54')\",\n",
       " \"('M111R382ID70', 'M805R108ID58')\",\n",
       " \"('M111R382ID70', 'M648R61ID47')\",\n",
       " \"('M246R92ID53', 'M256R401ID75')\",\n",
       " \"('M581R49ID39', 'M597R41ID22')\",\n",
       " \"('M145R51ID40', 'M157R138ID61')\",\n",
       " \"('M779R97ID54', 'M805R108ID58')\",\n",
       " \"('M648R61ID47', 'M779R97ID54')\",\n",
       " \"('M520R64ID48', 'M779R97ID54')\",\n",
       " \"('M648R61ID47', 'M805R108ID58')\",\n",
       " \"('M520R64ID48', 'M805R108ID58')\",\n",
       " \"('M165R324ID67', 'M175R355ID69')\",\n",
       " \"('M595R39ID18', 'M597R39ID19')\",\n",
       " \"('M495R39ID19', 'M549R40ID20')\",\n",
       " \"('M309R40ID20', 'M325R44ID29')\",\n",
       " \"('M724R55ID44', 'M739R45ID31')\",\n",
       " \"('M520R64ID48', 'M648R61ID47')\",\n",
       " \"('M516R64ID48', 'M520R64ID48')\",\n",
       " \"('M520R64ID48', 'M568R64ID47')\"]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_tuples = []\n",
    "my_list = newdf[\"sorted_row\"]\n",
    "for i in my_list:\n",
    "    save_tuples.append(str(i).replace('[','(').replace(']',')'))\n",
    "save_tuples\n",
    "list(save_tuples)\n",
    "G=nx.from_edgelist(save_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede7f7c7",
   "metadata": {},
   "source": [
    "## Molecular Networking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a74e65",
   "metadata": {},
   "source": [
    "## MN with GNPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e1259db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gnpsMNvsgnpsMAW(input_dir):\n",
    "    def isNaN(string):\n",
    "        return string != string\n",
    "    \"\"\"gnpsMNvsgnpsMAW checks with tanimoto similarity score, whether\n",
    "    results from MAW GNPS and GNPS MN Masst results give same candidate\n",
    "\n",
    "    Parameters:\n",
    "    input_dir = input directory where you have stored the cytoscape file \n",
    "    from GNPS MN results and have exported edge and node tables from cytoscape\n",
    "    These two csv egde and node files must have \"edge\" and \"node\" in their name\n",
    "    \n",
    "    Returns:\n",
    "    GNPS results with cluster index named\n",
    "    GNPS MN results with a confirmation column if MAW detected same candidate, \n",
    "    file named: \n",
    "    \n",
    "    Usage: \n",
    "    gnpsMNvsgnpsMAW(input_dir)\n",
    "    \n",
    "    \"\"\"\n",
    "    # extract files with edges from MN results\n",
    "    GMNfile_edge = [f for f in os.listdir(input_dir) if \"edge\" in f]\n",
    "    # extract files with nodes from MN results\n",
    "    GMNfile_node = [f for f in os.listdir(input_dir) if \"node\" in f]\n",
    "    # read the files\n",
    "    GMNdf_node = pd.read_csv(GMNfile_node[0])\n",
    "    GMNdf_edge = pd.read_csv(GMNfile_edge[0])\n",
    "    \n",
    "    # extract only important columns from both csv files\n",
    "    GMNdf_node = GMNdf_node[['precursor mass', 'RTMean', 'UniqueFileSources', \n",
    "                   'charge', 'cluster index', 'componentindex', \n",
    "                   'Compound_Name', 'Smiles', 'SpectrumID']]\n",
    "    GMNdf_edge = GMNdf_edge[['cosine_score', 'EdgeAnnotation', 'node1', 'node2',\n",
    "                     'mass_difference']]\n",
    "    \n",
    "    # rename node1 to cluster index to merge nodes and edges results from MN\n",
    "    GMNdf_edge = GMNdf_edge.rename(columns={'node1': 'cluster index'})\n",
    "    GMNdf = pd.merge(GMNdf_node, GMNdf_edge, on = \"cluster index\")\n",
    "    \n",
    "    # Read results obtained from scoring_spec, named input_dir/MetabolomicsResults/scoredSpecDB.csv\n",
    "    SDB = pd.read_csv(input_dir + \"/MetabolomicsResults/scoredSpecDB.csv\")\n",
    "    # only keep GNPS resulst and remove other columns\n",
    "    only_GNPS = SDB[SDB['annotation'].str.contains('GNPS')]\n",
    "    only_GNPS = only_GNPS[['id_X', 'premz_x', 'rtmean_x', 'GNPSmax_similarity', \n",
    "                       'GNPSSMILES', 'GNPSspectrumID', 'GNPScompound_name', \n",
    "                       'GNPSmirrorSpec']]\n",
    "    \n",
    "    # from GNPS MAW results and GNPS MN results, calculate how many MAW results are same as MN:\n",
    "    for i, row in only_GNPS.iterrows():\n",
    "        for j, row in GMNdf.iterrows():\n",
    "            if not isNaN(only_GNPS[\"GNPSSMILES\"][i]) and not isNaN(GMNdf[\"Smiles\"][j]):\n",
    "                SKms = [Chem.MolFromSmiles(only_GNPS['GNPSSMILES'][i]), Chem.MolFromSmiles(GMNdf['Smiles'][j])]\n",
    "                SKfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in SKms]\n",
    "                SKtn = DataStructs.FingerprintSimilarity(SKfps[0],SKfps[1])\n",
    "                if SKtn == 1.0:\n",
    "                    GMNdf.loc[j, \"gnps_maw\"] = \"confirmed\"\n",
    "                    only_GNPS.loc[i, \"index_MN_nodes\"] = j\n",
    "                elif SKtn < 1.0 and SKtn < 0.75:\n",
    "                    GMNdf.loc[j, \"gnps_maw\"] = \"similar\"\n",
    "                    only_GNPS.loc[i, \"index_MN_nodes\"] = j\n",
    "    only_GNPS.to_csv(input_dir + \"/MetabolomicsResults/only_GNPS.csv\")\n",
    "    GMNdf.to_csv(input_dir + \"/MetabolomicsResults/GMNdf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a1868fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gnpsMNvsgnpsMAW(input_dir = \"/Users/mahnoorzulfiqar/Downloads/MAW-main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8f223c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the compounds in the cluster extracted out also in the same fucntion and store in a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd0d043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the MN vs MCSS results to see any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5338fcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a90d1d3",
   "metadata": {},
   "source": [
    "### MN vs MCSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28b1aabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = pd.read_csv(\"/Users/mahnoorzulfiqar/OneDriveUNI/MZML/MetabolomicsResults/Final_Candidate_List.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c142e64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = final_list.rename(columns = {'SMILES_final':'SMILES'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ec129a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list.to_csv(\"/Users/mahnoorzulfiqar/OneDriveUNI/MZML/MetabolomicsResults/Final_Candidate_List.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d7daf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mawRpy)",
   "language": "python",
   "name": "mawrpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
