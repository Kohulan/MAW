{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d79995f",
   "metadata": {},
   "source": [
    "## SIRIUS_Metfrag_SList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e20dd5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.2\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "611cc70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import pubchempy as pcp\n",
    "import numpy as np\n",
    "def isNaN(string):\n",
    "    return string != string\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "from pybatchclassyfire import *\n",
    "import csv \n",
    "import time\n",
    "import json\n",
    "from pandas import json_normalize\n",
    "\n",
    "\n",
    "import openpyxl\n",
    "import statistics\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "42327aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdFMCS\n",
    "from rdkit.Chem import PandasTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63568ef8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "fd8a7e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure your Smiles entries in the suspect list csv are in a column named \"SMILES\"\n",
    "def slist_metfrag(input_dir, slist_csv, name):\n",
    "    \"\"\"slist_metfrag is used to create a txt file that contains a list of \n",
    "    InChIKeys. This list is later used by MetFrag to use these compounds \n",
    "    as a Suspect List.\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored. For this \n",
    "    function this directory must contain a csv file that has a column \n",
    "    named \"SMILES\".\n",
    "    \n",
    "    slist_csv (str): This is the csv file that contains a column of \n",
    "    \"SMILES\". Additionally this file can contain other information \n",
    "    about the compounds, but for this function, column of \"SMILES\", \n",
    "    named as \"SMILES\" is necessary.\n",
    "\n",
    "    Returns:\n",
    "    list: list of InChIKeys\n",
    "    txt: a txt file of list of InChIKeys, is stored in input_dir\n",
    "    \n",
    "    Usage:\n",
    "    slist_metfrag(input_dir = \"/user/project/\", slist_csv = \n",
    "    \"suspectlist.csv\")\n",
    "    \n",
    "    \"\"\"\n",
    "    sl = pd.read_csv(slist_csv)\n",
    "    sl_mtfrag= []\n",
    "    for i, rows in sl.iterrows():\n",
    "        if i is not None:\n",
    "            mols = Chem.MolFromSmiles(sl['SMILES'][i])\n",
    "            try:\n",
    "                sl.loc[i, 'InChIKey'] = Chem.inchi.MolToInchiKey(mols)\n",
    "                sl_mtfrag.append(sl['InChIKey'][i])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    \n",
    "    with open((input_dir + \"/SL_\"+ name + '.txt'), 'w') as filehandle:\n",
    "        for listitem in sl_mtfrag:\n",
    "            filehandle.write('%s\\n' % listitem)\n",
    "    return(sl_mtfrag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "09b81b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(slist_metfrag.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad461a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3cd169d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slist_sirius(input_dir, slist_csv, substring = None):\n",
    "    \n",
    "    \"\"\"slist_sirius is used to create a tsv file that contains a list of \n",
    "    SMILES. The function also runs the sirius command custom db to create\n",
    "    fingerprints for each SMILES in a folder that we by default name as\n",
    "    SL_Frag/. This fingerprints folder is later used by SIRIUS to use \n",
    "    these compounds as a another small list of compounds to match against\n",
    "    the input spectra fingerprints.\n",
    "    Since SIRIUS doesn't take disconnected structure, Multiply charged, \n",
    "    Incorrect syntax, wild card(*) in smiles; this function removes all\n",
    "    such SMILES from the Suspect List.\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored. For this \n",
    "    function this directory must contain a csv file that has a column \n",
    "    named \"SMILES\".\n",
    "    \n",
    "    slist_csv (str): This is the csv file that contains a column of \n",
    "    \"SMILES\". Additionally this file can contain other information \n",
    "    about the compounds, but for this function, column of \"SMILES\", \n",
    "    named as \"SMILES\" is necessary.\n",
    "    \n",
    "    substring (list): provide a list of strings of SMILES that \n",
    "    shouldn't be considered, provide a list even if there is one string\n",
    "    that shouldnt be considered. e.g: \"[Fe+2]\". \n",
    "\n",
    "    Returns:\n",
    "    tsv: a tsv file of list of SMILES, named as SL_Sirius.tsv, is stored \n",
    "    in input_dir\n",
    "    directory: a directory with compound fragmentations will be created \n",
    "    in a folder named SL_Frag/ within the same input_dir\n",
    "    \n",
    "    \n",
    "    Usage:\n",
    "    slist_sirius(\"/user/project/\", \"suspectlist.csv\", \n",
    "    substring = None)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    sl = pd.read_csv(slist_csv)\n",
    "    \n",
    "    # define function to neutralize the charged SMILES\n",
    "    def neutralize_atoms(mol):\n",
    "        \n",
    "        pattern = Chem.MolFromSmarts(\"[+1!h0!$([*]~[-1,-2,-3,-4]),-1!$([*]~[+1,+2,+3,+4])]\")\n",
    "        at_matches = mol.GetSubstructMatches(pattern)\n",
    "        at_matches_list = [y[0] for y in at_matches]\n",
    "        if len(at_matches_list) > 0:\n",
    "            for at_idx in at_matches_list:\n",
    "                atom = mol.GetAtomWithIdx(at_idx)\n",
    "                chg = atom.GetFormalCharge()\n",
    "                hcount = atom.GetTotalNumHs()\n",
    "                atom.SetFormalCharge(0)\n",
    "                atom.SetNumExplicitHs(hcount - chg)\n",
    "                atom.UpdatePropertyCache()\n",
    "        return mol\n",
    "    \n",
    "\n",
    "    for i, row in sl.iterrows():\n",
    "        # remove SMILES with wild card\n",
    "        if \"*\" in sl[\"SMILES\"][i]:\n",
    "            sl = sl.drop(labels = i, axis = 0) \n",
    "    for i, row in sl.iterrows():\n",
    "        # remove SMILES with any string present in the substring\n",
    "        if substring:\n",
    "            if bool([ele for ele in substring if(ele in sl[\"SMILES\"][i])]):\n",
    "                sl = sl.drop(labels = i, axis = 0)\n",
    "    for i, row in sl.iterrows():\n",
    "        if \".\" in sl[\"SMILES\"][i]:\n",
    "            sl.loc[i, \"SMILES\"] = sl[\"SMILES\"][i].split('.')[0]\n",
    "    # Neutralize the charged SMILES\n",
    "    for i, row in sl.iterrows():\n",
    "        if \"+\" in sl[\"SMILES\"][i] or \"-\" in sl[\"SMILES\"][i]:\n",
    "            mol = Chem.MolFromSmiles(sl[\"SMILES\"][i])\n",
    "            neutralize_atoms(mol)\n",
    "            sl.loc[i, \"SMILES\"] = Chem.MolToSmiles(mol)\n",
    "            \n",
    "            # Remove multiple charged SMILES\n",
    "            if \"+\" in sl[\"SMILES\"][i] or \"-\" in sl[\"SMILES\"][i]:\n",
    "                pos = sl[\"SMILES\"][i].count('+')\n",
    "                neg = sl[\"SMILES\"][i].count('-')\n",
    "                charge = pos + neg \n",
    "                if charge > 1:\n",
    "                    sl = sl.drop(labels = i, axis = 0) \n",
    "                    \n",
    "    slsirius = pd.DataFrame({'smiles':sl[\"SMILES\"]})\n",
    "    slsirius.to_csv(input_dir+ \"SL_Sirius.tsv\", sep = \"\\t\", header = False, index = False)\n",
    "    os.system(\"sirius --input \" + input_dir + \"SL_Sirius.tsv custom-db --name=SL_Frag --output \"+ input_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5ef42aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(slist_sirius.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc7a19e",
   "metadata": {},
   "source": [
    "### SIRIUS post processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e00a88",
   "metadata": {},
   "source": [
    "### SIRIUS Result Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e047124f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8b65479b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sirius_postProc2.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ecf65a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sirius_postProc2(input_dir, input_tablecsv):\n",
    "    \n",
    "    def isNaN(string):\n",
    "        return string != string\n",
    "    \"\"\"sirius_postProc2 is the second part of the function \n",
    "    sirius_postProc defined in R part of the workflow. This function\n",
    "    re-checks the Suspect list, if present or given as a parameter, \n",
    "    whether the candidates have a high similarity with compounds in\n",
    "    Suspect List. It also calculates the Maximum Common Substructure\n",
    "    (MCSS)\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored. For this \n",
    "    function this directory must contain a csv file that has a column \n",
    "    named \"SMILES\".\n",
    "    \n",
    "    input_tablecsv (str): This is the table in csv format (defined in R), \n",
    "    which stores a csv table containing columns \"mzml_files\", which \n",
    "    contains liat of all input files with their relative paths, second\n",
    "    column is \"ResultFileName\" which is a list of the corresponding\n",
    "    result relative directories to each mzml files. Lastly, \"file_id\", \n",
    "    contains a file directory. This table will be used to read the \n",
    "    SIRIUS json files\n",
    "    \n",
    "\n",
    "    Returns:\n",
    "    csv: a result file with additional columns such as those for suspect\n",
    "    list if one is used. It also adds columns on MCSS., named as \n",
    "    \"input_dir/ResultFileName/insilico/SiriusResults.csv\"\n",
    "    \n",
    "    \n",
    "    Usage:\n",
    "    sirius_postProc2(input_dir = \"/user/project/\", \n",
    "    input_table = \"/user/project/suspectlist.csv\")\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Describe the heavy atoms to be considered for MCSS\n",
    "    heavy_atoms = ['C', 'N', 'P', 'O', 'S']\n",
    "    \n",
    "    input_table = pd.read_csv(input_tablecsv)\n",
    "    \n",
    "    for m, row in input_table.iterrows():\n",
    "        \n",
    "        # Read the file result_dir/insilico/MS1DATAsirius.csv. \n",
    "        # This file has been produced in R workflow and contains \n",
    "        # SIRIUS results.\n",
    "\n",
    "        file1 = pd.read_csv(input_dir + (input_table['ResultFileNames'][m] + '/insilico/MS1DATAsirius.csv').replace(\"./\", \"\"))\n",
    "        \n",
    "        for i, row in file1.iterrows():\n",
    "            \n",
    "            # if the entry has SMILES extracted for MCSS calculation\n",
    "            if not isNaN(file1['SMILESforMCSS'][i]):\n",
    "                \n",
    "                # split the SMILES using |\n",
    "                top_smiles = file1['SMILESforMCSS'][i].split(\"|\")\n",
    "                \n",
    "                # if there are more than 1 smiles in the top smiles, \n",
    "                if len(top_smiles) > 1:\n",
    "                    mol = []\n",
    "                    for j in top_smiles:\n",
    "                        n = Chem.MolFromSmiles(j)\n",
    "                        mol.append(n)\n",
    "                    # list of mol used to calaculate the MCSS\n",
    "                    res = rdFMCS.FindMCS(mol)\n",
    "                    sm_res = Chem.MolToSmiles(Chem.MolFromSmarts(res.smartsString))\n",
    "                    # Check if the MCSS has one of the heavy atoms and whether they are\n",
    "                    # more than 3\n",
    "                    elem = [ele for ele in heavy_atoms if(ele in sm_res)]\n",
    "                    if elem and len(sm_res)>=3:\n",
    "                        file1.loc[i, 'MCSSstring'] = res.smartsString\n",
    "                        file1.loc[i, 'MCSS_SMILES'] = Chem.MolToSmiles(Chem.MolFromSmarts(res.smartsString))\n",
    "                        \n",
    "                        \n",
    "            if file1[\"FormulaRank\"][i] == 1.0:\n",
    "                sep = 'json/'\n",
    "                strpd = file1[\"dir\"][i].split(sep, 1)[0] +\"json/canopus_summary.tsv\"\n",
    "                if os.path.isfile(strpd):\n",
    "\n",
    "                    canopus = pd.read_csv(strpd, sep='\\t')\n",
    "                    if len(canopus) > 0:\n",
    "                        #file1.loc[i, 'most_specific_class'] = canopus[\"most specific class\"][0]\n",
    "                        #file1.loc[i, 'level _5'] = canopus[\"level 5\"][0]\n",
    "                        file1.loc[i, 'subclass'] = canopus[\"subclass\"][0]\n",
    "                        file1.loc[i, 'class'] = canopus[\"class\"][0]\n",
    "                        file1.loc[i, 'superclass'] = canopus[\"superclass\"][0]\n",
    "                        #file1.loc[i, 'all_classifications'] = canopus[\"all classifications\"][0]\n",
    "                        file1.loc[i, 'Classification_Source'] = 'CANOPUS'\n",
    "                    \n",
    "        \n",
    "        file1.to_csv(input_dir + (input_table['ResultFileNames'][m] + '/insilico/SiriusResults.csv').replace(\"./\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c8451182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for one file\n",
    "#sirius_postProc2(input_dir= \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/\", \n",
    "#                 input_tablecsv = \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/input_table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7d0fed58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for multiple file\n",
    "#sirius_postProc2(input_dir= \"/Users/mahnoorzulfiqar/Downloads/MAW-main/\", \n",
    "#                 input_tablecsv = \"/Users/mahnoorzulfiqar/Downloads/MAW-main/input_table.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653e9b79",
   "metadata": {},
   "source": [
    "### MetFrag Result Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a01c9220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metfrag_postproc(input_dir, input_tablecsv, sl= True):\n",
    "    \n",
    "    \n",
    "    \"\"\"metfrag_postproc function re-checks the Suspect list, if present \n",
    "    or given as a parameter, whether the candidates have a high \n",
    "    similarity with compounds in Suspect List. It also calculates the \n",
    "    Maximum Common Substructure (MCSS). This function adds top candidates\n",
    "    from PubChem and KEGG as these two databases are used with MetFrag\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored. For this \n",
    "    function this directory must contain a csv file that has a column \n",
    "    named \"SMILES\".\n",
    "    \n",
    "    input_tablecsv (str): This is the table in csv format (defined in R), \n",
    "    which stores a csv table containing columns \"mzml_files\", which \n",
    "    contains liat of all input files with their relative paths, second\n",
    "    column is \"ResultFileName\" which is a list of the corresponding\n",
    "    result relative directories to each mzml files. Lastly, \"file_id\", \n",
    "    contains a file directory. This table will be used to read the \n",
    "    MetFrag csv files\n",
    "\n",
    "    Returns:\n",
    "    csv: a result file with additional columns such as those for suspect\n",
    "    list if one is used. It also adds columns on MCSS., named as \n",
    "    \"input_dir/ResultFileName/insilico/MetFragResults.csv\". It \n",
    "    contains columns for KEGG and PubChem\n",
    "    \n",
    "    \n",
    "    Usage:\n",
    "    metfrag_postproc(input_dir = \"/user/project/\", \n",
    "    input_table = \"/user/project/suspectlist.csv\", sl = True, slistcsv)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Describe the heavy atoms to be considered for MCSS\n",
    "    heavy_atoms = ['C', 'N', 'P', 'O', 'S']\n",
    "    \n",
    "    input_table = pd.read_csv(input_tablecsv)\n",
    "    \n",
    "    for m, row in input_table.iterrows():\n",
    "        \n",
    "        # read SIRIUS results:\n",
    "        \n",
    "        #siriusResults = pd.read_csv(input_dir + (input_table['ResultFileNames'][m] + '/insilico/SiriusResults.csv'))\n",
    "    \n",
    "        # Result directory\n",
    "        result = input_dir + (input_table['ResultFileNames'][m] + \n",
    "                                 '/insilico/MetFrag').replace(\"./\", \"\")\n",
    "\n",
    "        # list of all the csv files in the result directory result_dir/inislico/MetFrag/\n",
    "        files_met = (glob.glob(result+'/*.csv'))\n",
    "\n",
    "        # read the csv file that contains all the features from the input .mzml file\n",
    "        file1  = pd.read_csv(input_dir + (input_table['ResultFileNames'][m] + '/insilico/MS1DATA.csv').replace(\"./\", \"\"))\n",
    "    \n",
    "        # for each feature in the MS1DATA.csv file\n",
    "        for i, row in file1.iterrows():\n",
    "        \n",
    "            # take id as a pattern to differentiate between different ids\n",
    "            pattern = file1.loc[i, \"id_X\"]\n",
    "        \n",
    "            #check which of the csv result files have the same pattern in their names\n",
    "            results = [i for i in files_met if pattern in i]\n",
    "        \n",
    "            # find which of the files with that id have KEGG in their names,\n",
    "            KEGG = [i for i in results if \"KEGG\" in i]\n",
    "        \n",
    "            # if kegg present in the name\n",
    "            if KEGG:\n",
    "            \n",
    "                # read the KEGG csv file for that feature\n",
    "                KEGG_file = pd.read_csv((KEGG)[0])\n",
    "            \n",
    "                # if the KEGG file isn't empty\n",
    "                if len(KEGG_file)>0:\n",
    "                \n",
    "                    # extract only the columns with >0.75 score\n",
    "                    KEGG_file = KEGG_file.drop(KEGG_file[KEGG_file.Score < 0.98].index)\n",
    "                    \n",
    "                    #s_best_kg = []\n",
    "                    #for kg, rows in KEGG_file.iterrows():\n",
    "                        #kg_smiles = Chem.MolToSmiles(Chem.MolFromInchi(KEGG_file[\"InChI\"][kg]))\n",
    "                        #SSmsk = [Chem.MolFromSmiles(kg_smiles), Chem.MolFromSmiles(siriusResults[\"SMILES\"][0])]\n",
    "                        #SSfpsk = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in SSmsk]\n",
    "                        #SStn2k = DataStructs.FingerprintSimilarity(SSfpsk[0],SSfpsk[1])\n",
    "                        #s_best_kg.append(SStn2k)\n",
    "                    #index_kg = np.argmax(s_best_kg)\n",
    "                        \n",
    "                    # add the relevavnt information to the original MS1DATA csv\n",
    "                    file1.loc[i, 'KG_ID'] = KEGG_file.loc[0, 'Identifier']\n",
    "                    file1.loc[i, 'KG_Name'] = KEGG_file.loc[0, 'CompoundName']\n",
    "                    file1.loc[i, 'KG_Formula'] = KEGG_file.loc[0, 'MolecularFormula']\n",
    "                    file1.loc[i, 'KG_expPeaks'] = KEGG_file.loc[0, 'NoExplPeaks']\n",
    "                    file1.loc[i, 'KG_SMILES'] = Chem.MolToSmiles(Chem.MolFromInchi(KEGG_file[\"InChI\"][0]))\n",
    "                    file1.loc[i, 'KG_Score'] = KEGG_file.loc[0, 'Score']\n",
    "                    if sl:\n",
    "                        file1.loc[i, 'KGSL_Score'] = KEGG_file.loc[0, 'SuspectListScore']\n",
    "                    file1.loc[i, 'KG_file'] = KEGG[0]\n",
    "                \n",
    "                    #create empty list of KEGG top smiles\n",
    "                    Kegg_smiles = []\n",
    "                \n",
    "                    # extract only the InChI of the top 5\n",
    "                    for j in KEGG_file[\"InChI\"][0:5].tolist():\n",
    "                        # convert the InChI to SMILES\n",
    "                        mol = Chem.MolToSmiles(Chem.MolFromInchi(j))\n",
    "                        mol2 = Chem.MolFromSmiles(mol)\n",
    "                        Kegg_smiles.append(mol2)\n",
    "                    # if there are more than 1 top smiles\n",
    "                    if len(Kegg_smiles) > 1:\n",
    "                        #calculate the MCSS\n",
    "                        res = rdFMCS.FindMCS(Kegg_smiles)\n",
    "                        sm_res = Chem.MolToSmiles(Chem.MolFromSmarts(res.smartsString))\n",
    "                        # if there are atleast 3 heavy atoms in the MCSS, then add it to the result file\n",
    "                        elem = [ele for ele in heavy_atoms if(ele in sm_res)]\n",
    "                        if elem and len(sm_res)>=3:\n",
    "                            file1.loc[i, 'KG_MCSSstring'] = res.smartsString\n",
    "                            file1.loc[i, 'KG_MCSS_SMILES'] = Chem.MolToSmiles(Chem.MolFromSmarts(res.smartsString))\n",
    "                            \n",
    "            #start here for PubChem; find which of the files with that id have PubChem in their names,\n",
    "            PubChem = [i for i in results if \"PubChem\" in i]\n",
    "            \n",
    "            if PubChem:\n",
    "\n",
    "                PubChem_file = pd.read_csv(PubChem[0])\n",
    "                \n",
    "                # if more candidates\n",
    "                if len(PubChem_file)>0:\n",
    "                    \n",
    "                    # take the ones with more than 0.80 score\n",
    "                    PubChem_file = PubChem_file.drop(PubChem_file[PubChem_file.Score < 0.80].index)\n",
    "                    #s_best_pb = []\n",
    "                    #for pb, rows in PubChem_file.iterrows():\n",
    "                        #SSmsp = [Chem.MolFromSmiles(PubChem_file[\"SMILES\"][pb]), Chem.MolFromSmiles(siriusResults[\"SMILES\"][0])]\n",
    "                        #SSfpsp = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in SSmsp]\n",
    "                        #SStn2p = DataStructs.FingerprintSimilarity(SSfpsp[0],SSfpsp[1])\n",
    "                        #s_best_pb.append(SStn2p)\n",
    "                    #index_pb = np.argmax(s_best_pb)\n",
    "                    # add the relavnt information to the original MS1DATA csv\n",
    "                    file1.loc[i, 'PC_ID'] = PubChem_file.loc[0, 'Identifier']\n",
    "                    file1.loc[i, 'PC_Name'] = PubChem_file.loc[0, 'IUPACName']\n",
    "                    file1.loc[i, 'PC_Formula'] = PubChem_file.loc[0, 'MolecularFormula']\n",
    "                    file1.loc[i, 'PC_expPeaks'] = PubChem_file.loc[0, 'NoExplPeaks']\n",
    "                    file1.loc[i, 'PC_SMILES'] = PubChem_file[\"SMILES\"][0]\n",
    "                    file1.loc[i, 'PC_Score'] = PubChem_file[\"Score\"][0]\n",
    "                    if sl:\n",
    "                        file1.loc[i, 'PCSL_Score'] = PubChem_file.loc[0, 'SuspectListScore']\n",
    "                    file1.loc[i, 'PC_file'] = PubChem[0]\n",
    "                    \n",
    "                    # empty object\n",
    "                    Pubchem_smiles = []\n",
    "                    \n",
    "                    # extract only the SMILES of the top 5\n",
    "                    for j in PubChem_file[\"SMILES\"][0:5].tolist():\n",
    "                        \n",
    "                        # Concert smiles to mol\n",
    "                        sm2 = Chem.MolFromSmiles(j)\n",
    "                        # store mol in Pubchem_smiles\n",
    "                        Pubchem_smiles.append(sm2)\n",
    "                    \n",
    "                    if len(Pubchem_smiles) > 1:\n",
    "                        # calculate MCSS\n",
    "                        res2 = rdFMCS.FindMCS(Pubchem_smiles)\n",
    "                        sm_res = Chem.MolToSmiles(Chem.MolFromSmarts(res2.smartsString))\n",
    "                        # If atleast 3 heavy atoms present\n",
    "                        elem = [ele for ele in heavy_atoms if(ele in sm_res)]\n",
    "                        if elem and len(sm_res)>=3:\n",
    "                            file1.loc[i, 'PC_MCSSstring']= res2.smartsString\n",
    "                            file1.loc[i, 'PC_MCSS_SMILES'] = Chem.MolToSmiles(Chem.MolFromSmarts(res2.smartsString))\n",
    "        file1.to_csv(input_dir + (input_table['ResultFileNames'][m] + '/insilico/MetFragResults.csv').replace(\"./\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f5de3b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for multiple file\n",
    "#metfrag_postproc(input_dir= \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/\", \n",
    "#                 input_tablecsv = \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/input_table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c352c1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for multiple file\n",
    "#metfrag_postproc(input_dir= \"/Users/mahnoorzulfiqar/Downloads/MAW-main/\", \n",
    "#                 input_tablecsv = \"/Users/mahnoorzulfiqar/Downloads/MAW-main/input_table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "7312134b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(metfrag_postproc.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80fae2f",
   "metadata": {},
   "source": [
    "### COMBINE IN SILICO -All files with SIRIUS results separate and with MetFragresults separate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "6a14bbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_insilico(input_dir, input_tablecsv, Source = \"all_insilico\"):\n",
    "    \n",
    "    \"\"\"combine_insilico function combines the Sirius results from all\n",
    "    result directories for each input mzml file. It does same for \n",
    "    Metfrag.\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "    \n",
    "    input_table (str): This is the table in csv format (defined in R), \n",
    "    which stores a csv table containing columns \"mzml_files\", which \n",
    "    contains liat of all input files with their relative paths, second\n",
    "    column is \"ResultFileName\" which is a list of the corresponding\n",
    "    result relative directories to each mzml files. Lastly, \"file_id\", \n",
    "    contains a file directory. This table will be used to read the \n",
    "    Sirius and MetFrag result csv files\n",
    "    \n",
    "    Source (str): either \"SIRIUS\" or \"MetFrag\"\n",
    "\n",
    "    Returns:\n",
    "    \n",
    "    dataframe: of combined SIRIUS/MetFrag results\n",
    "    \n",
    "    csv: stores the dataframe in a csv, named as \n",
    "    \"input_dir/ResultFileName/MetabolomicsResults/SIRIUS_combined.csv\" \n",
    "    OR/AND \n",
    "    \"input_dir/ResultFileName/MetabolomicsResults/MetFrag_combined.csv\"\n",
    "    \n",
    "    \n",
    "    Usage:\n",
    "    combine_insilico(input_dir = \"/user/project/\", \n",
    "    input_table = \"/user/project/suspectlist.csv\", Source = \"SIRIUS\")\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    input_table = pd.read_csv(input_tablecsv)\n",
    "    # create a new directory to store all results /MetabolomicsResults/\n",
    "    path = os.path.join(input_dir, \"MetabolomicsResults\")\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)    \n",
    "    # if Sirius results are to be combined\n",
    "    if Source == \"all_insilico\" or Source == \"SIRIUS\":\n",
    "        \n",
    "        # store all files paths here\n",
    "        all_files = []\n",
    "        for n, row in input_table.iterrows():\n",
    "            all_files.append(input_dir + input_table['ResultFileNames'][n].replace(\"./\", \"\") + '/insilico/SiriusResults.csv')\n",
    "        \n",
    "        # store all dataframes of the results here\n",
    "        li = []\n",
    "    \n",
    "        for filename in all_files:\n",
    "            df = pd.read_csv(filename, index_col=None, header=0)\n",
    "            df[\"ResultFileNames\"] = filename\n",
    "            li.append(df)\n",
    "            \n",
    "        # join all resulst dataframe\n",
    "        frame = pd.concat(li, axis=0, ignore_index=True)\n",
    "        frame.to_csv(input_dir + '/MetabolomicsResults/SIRIUS_combined.csv')       \n",
    "    \n",
    "    # if MetFrag results are to be combined\n",
    "    if Source == \"all_insilico\" or Source == \"MetFrag\":\n",
    "        \n",
    "        # store all files paths here\n",
    "        all_files = []\n",
    "        for m, row in input_table.iterrows():\n",
    "            all_files.append(input_dir + input_table['ResultFileNames'][m].replace(\"./\", \"\") + '/insilico/MetFragResults.csv')\n",
    "        li = []\n",
    "\n",
    "        for filename in all_files:\n",
    "            df = pd.read_csv(filename, index_col=None, header=0)\n",
    "            df[\"result_dir\"] = filename\n",
    "            li.append(df)\n",
    "\n",
    "        frame = pd.concat(li, axis=0, ignore_index=True)\n",
    "        frame.to_csv(input_dir+'MetabolomicsResults/MetFrag_combined.csv')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "26b6ec8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for one file\n",
    "#combine_insilico(input_dir= \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/\", \n",
    " #                input_tablecsv = \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/input_table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "14179601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for multiple file\n",
    "##combine_insilico(input_dir= \"/Users/mahnoorzulfiqar/Downloads/MAW-main/\", \n",
    "#                 input_tablecsv = \"/Users/mahnoorzulfiqar/Downloads/MAW-main/input_table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "16c4edfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(combine_insilico.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aef0fd6",
   "metadata": {},
   "source": [
    "# Spectral DB Dereplication Results Post Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf3d8c1",
   "metadata": {},
   "source": [
    "### GNPS, MassBank and HMDB Results post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "51160acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spec_postproc(input_dir, Source = \"all\"):\n",
    "    \n",
    "    \"\"\"spec_postproc function processes the resulst from dereplication \n",
    "    using different spectral DBs. \n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "    \n",
    "    Source (str): either \"mbank\" or \"hmdb\" or \"gnps\", or \"all\"\n",
    "\n",
    "    Returns:\n",
    "    \n",
    "    dataframe: of the paths of the processed DB results\n",
    "    \n",
    "    \n",
    "    Usage:\n",
    "    spec_postproc(input_dir = \"/user/project/\", Source = \"all\")\n",
    "\n",
    "    \"\"\"\n",
    "    def isNaN(string):\n",
    "        return string != string\n",
    "\n",
    "    # empty lists of csv files paths for each database\n",
    "    GNPScsvfiles = []\n",
    "    HMDBcsvfiles = []\n",
    "    MassBankcsvfiles = []\n",
    "    \n",
    "    #list all files and directories\n",
    "    for entry in os.listdir(input_dir):\n",
    "        if os.path.isdir(os.path.join(input_dir, entry)):\n",
    "            \n",
    "            # enter the directory with /spectral_dereplication/ results\n",
    "            sub_dir = input_dir + entry + '/spectral_dereplication'\n",
    "            if os.path.exists(sub_dir):\n",
    "                files = (glob.glob(sub_dir+'/*.csv'))\n",
    "\n",
    "                for f in files:\n",
    "                    if 'gnps.' in f: \n",
    "                        GNPScsvfiles.append(f)\n",
    "                    if 'hmdb.' in f: \n",
    "                        HMDBcsvfiles.append(f)\n",
    "                    if 'mbank.' in f: \n",
    "                        MassBankcsvfiles.append(f)\n",
    "                            \n",
    "    \n",
    "    if Source == \"hmdb\" or Source == \"all\":\n",
    "\n",
    "        if not os.path.exists(input_dir+\"structures.sdf\"):\n",
    "            #download SDF structures\n",
    "            os.system(\"wget -P \" + input_dir + \" https://hmdb.ca/system/downloads/current/structures.zip\")\n",
    "            os.system(\"unzip \"+ input_dir + \"structures.zip\" + \" -d \" + input_dir)\n",
    "            \n",
    "        # Load the sdf\n",
    "        dframe = PandasTools.LoadSDF((input_dir+\"structures.sdf\"),\n",
    "                                     idName='HMDB_ID',smilesName='SMILES',\n",
    "                                     molColName='Molecule', includeFingerprints=False)\n",
    "        \n",
    "        #### read sdf file from HMDB to collect names and smiles ####\n",
    "    \n",
    "        #HMDB CSV Result file pre_processing\n",
    "        \n",
    "        #open another csv path holding empty list, which will be filled \n",
    "        #with post processed csv results\n",
    "        HMDBcsvfiles2 = []\n",
    "        \n",
    "        for k in HMDBcsvfiles:\n",
    "            \n",
    "            # read the csv files\n",
    "            hmdb_df = pd.read_csv(k)\n",
    "            \n",
    "            # merge on basis of id, frame and hmdb result files\n",
    "            SmilesHM = pd.merge(hmdb_df, dframe, left_on=hmdb_df.HMDBcompoundID, right_on=dframe.DATABASE_ID)\n",
    "            \n",
    "            \n",
    "            for i, row in hmdb_df.iterrows():\n",
    "                \n",
    "                for j, row in SmilesHM.iterrows():\n",
    "                    \n",
    "                    # where index for both match, add the name and SMILES\n",
    "                    if hmdb_df['id_X'][i]== SmilesHM['id_X'][j]:\n",
    "                        hmdb_df.loc[i, 'HMDBSMILES'] = SmilesHM['SMILES'][j]#add SMILES\n",
    "                        hmdb_df.loc[i, 'HMDBcompound_name'] = SmilesHM[\"GENERIC_NAME\"][j]#add name\n",
    "                        hmdb_df.loc[i, 'HMDBformula'] = SmilesHM[\"FORMULA\"][j]#add formula\n",
    "                \n",
    "            csvname = (os.path.splitext(k)[0])+\"proc\"+\".csv\" # name for writing it in a new file\n",
    "            hmdb_df.to_csv(csvname) #write\n",
    "            HMDBcsvfiles2.append(csvname)# add to a list\n",
    "            dict1 = {'HMDBr': HMDBcsvfiles2} \n",
    "            df = pd.DataFrame(dict1)\n",
    "        \n",
    "    #MassBank CSV Result file pre_processing\n",
    "    \n",
    "    if Source == \"mbank\" or Source == \"all\":\n",
    "        \n",
    "        #open another csv path holding empty list, which will be filled \n",
    "        #with post processed csv results\n",
    "        MassBankcsvfiles2 = []\n",
    "        \n",
    "        for l in MassBankcsvfiles:\n",
    "            \n",
    "            # read mbank csv file\n",
    "            mbank_df = pd.read_csv(l)\n",
    "            \n",
    "            for i, row in mbank_df.iterrows():\n",
    "                \n",
    "                inchiK = str(mbank_df[\"MBinchiKEY\"][i])\n",
    "                \n",
    "                #extract inchikeys\n",
    "                y = pcp.get_compounds(inchiK, 'inchikey')#compound based on inchikey\n",
    "                \n",
    "                for compound in y:\n",
    "                    \n",
    "                    #add smiles\n",
    "                    smles = compound.isomeric_smiles   \n",
    "                    mbank_df.loc[i, 'MBSMILES'] = smles\n",
    "                    \n",
    "            csvname = (os.path.splitext(l)[0])+\"proc\"+\".csv\"\n",
    "            mbank_df.to_csv(csvname)\n",
    "            MassBankcsvfiles2.append(csvname)\n",
    "            \n",
    "            dict1 = {'MBr': MassBankcsvfiles2} \n",
    "            df = pd.DataFrame(dict1)\n",
    "    \n",
    "    # GNPS CSV Result file pre_processing\n",
    "    if Source == \"gnps\" or Source == \"all\":\n",
    "        #open another csv path holding empty list, which will be filled \n",
    "        #with post processed csv results\n",
    "        GNPScsvfiles2 = []\n",
    "        #currently only these subsets are removed from the names from GNPS\n",
    "        matches = [\"M+\",\"[M\", \"M-\", \"2M\", \"M*\" \"20.0\", \"50.0\", \"30.0\", \"40.0\", \"60.0\", \"70.0\", \"eV\", \"Massbank\"\n",
    "               , \"Spectral\", \"Match\", \"to\", \"from\", \"NIST14\", \"MoNA\", '[IIN-based:',  '[IIN-based', 'on:', 'CCMSLIB00003136269]']\n",
    "\n",
    "        for l in GNPScsvfiles:\n",
    "            gnps_df = pd.read_csv(l)\n",
    "\n",
    "            for i, row in gnps_df.iterrows():\n",
    "                # if compound name is present\n",
    "                if not isNaN(gnps_df['GNPScompound_name'][i]):\n",
    "                    # split if there is a gap in the names\n",
    "                    string_chng = (gnps_df['GNPScompound_name'][i].split(\" \"))\n",
    "\n",
    "                    # create an empty list\n",
    "                    newstr = []\n",
    "\n",
    "                    # for each part of the string in the names\n",
    "                    chng = []\n",
    "\n",
    "                    for j in range(len(string_chng)):\n",
    "\n",
    "                        # check if the substrings are present in the matches and no - is present\n",
    "                        if not any(x in string_chng[j] for x in matches): #and not '-' == string_chng[j]:\n",
    "\n",
    "                            # IF | and ! not in the substring\n",
    "                            if '|' not in string_chng[j] or '!' not in string_chng[j]:\n",
    "                                newstr.append(string_chng[j])\n",
    "\n",
    "                            # if | present in the substring   \n",
    "                            elif '|' in string_chng[j]:\n",
    "\n",
    "                                #split the string\n",
    "                                jlen = string_chng[j].split(\"|\")\n",
    "                                #how many substrings are left now\n",
    "                                lst = len(jlen)-1\n",
    "                                #append this to chng\n",
    "                                chng.append(jlen[lst])\n",
    "                                break\n",
    "\n",
    "                    # now append chng to newstr            \n",
    "                    chng.append(' '.join(newstr))\n",
    "                    #save this as the correct name\n",
    "                    gnps_df.loc[i, \"corr_names\"] = chng[0]\n",
    "                    if not isNaN(gnps_df['GNPSSMILES'][i]):\n",
    "                        if chng == '':\n",
    "                            break\n",
    "                        elif gnps_df['GNPSSMILES'][i].isalpha():\n",
    "                            s = pcp.get_compounds(chng[0], 'name')\n",
    "                            if s:\n",
    "                                for comp in s:\n",
    "                                    gnps_df[\"GNPSSMILES\"][i] = comp.isomeric_smiles\n",
    "                            else:\n",
    "                                gnps_df[\"GNPSSMILES\"][i] = ''\n",
    "                else:\n",
    "                    gnps_df[\"GNPSSMILES\"][i] = ''\n",
    "\n",
    "            for i, row in gnps_df.iterrows():\n",
    "                if isNaN(gnps_df['GNPSSMILES'][i]):\n",
    "                    if \"[\" in gnps_df['GNPScompound_name'][i].split(\" \")[-1]:\n",
    "                        string_chng = (gnps_df['GNPScompound_name'][i].split(\"[\"))\n",
    "                        #print(gnps_df['GNPScompound_name'][i])\n",
    "                        keep_names = []\n",
    "                        for j in range(len(string_chng)-1):\n",
    "                            gnps_df.loc[i, \"corr_names\"] == string_chng[j]\n",
    "                            s = pcp.get_compounds(string_chng[j], 'name')\n",
    "\n",
    "                            if s:\n",
    "                                for comp in s:\n",
    "                                    gnps_df[\"GNPSSMILES\"][i] = comp.isomeric_smiles\n",
    "                            else:\n",
    "                                gnps_df[\"GNPSSMILES\"][i] = ''\n",
    "                if not isNaN(gnps_df['GNPSSMILES'][i]):\n",
    "                    try:\n",
    "                        sx = pcp.get_compounds(gnps_df['GNPSSMILES'][i], 'smiles')\n",
    "                        if sx:\n",
    "                            sx = str(sx)\n",
    "                            comp = pcp.Compound.from_cid([int(x) for x in re.findall(r'\\b\\d+\\b', sx)])\n",
    "                            gnps_df.loc[i, 'GNPSformula'] = comp.molecular_formula\n",
    "                    except:\n",
    "                        gnps_df.loc[i, 'GNPSformula'] = ''\n",
    "\n",
    "            csvname = (os.path.splitext(l)[0])+\"proc\"+\".csv\"\n",
    "            gnps_df.to_csv(csvname)\n",
    "            GNPScsvfiles2.append(csvname)\n",
    "            dict1 = {'GNPSr': GNPScsvfiles2} \n",
    "            df = pd.DataFrame(dict1)\n",
    "        \n",
    "\n",
    "    if Source == \"all\":\n",
    "        \n",
    "        dict1 = {'GNPSr': GNPScsvfiles2, 'HMDBr': HMDBcsvfiles2, 'MBr': MassBankcsvfiles2} \n",
    "        df = pd.DataFrame(dict1)\n",
    "\n",
    "        return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5137fcd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "89471167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for one file\n",
    "#spec_postproc(input_dir= \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/\",\n",
    "#             Source = \"gnps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b793f0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for multiple file\n",
    "#spec_postproc(input_dir= \"/Users/mahnoorzulfiqar/Downloads/MAW-main/\", \n",
    "#             Source = \"gnps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "8889acf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(spec_postproc.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6e281b",
   "metadata": {},
   "source": [
    "### Combine_all Spectral DBs for one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "60009629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_specdb(input_dir):\n",
    "    \n",
    "    \"\"\"combine_specdb function combines all results from different\n",
    "    spectral dbs. Can only be used if more than one db is used \n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "\n",
    "    Returns:\n",
    "    dataframe: of the paths of the merged results\n",
    "    \n",
    "    \n",
    "    Usage:\n",
    "    combine_specdb(input_dir)\n",
    "\n",
    "    \"\"\"\n",
    "    def isNaN(string):\n",
    "        return string != string\n",
    "\n",
    "    \n",
    "    # empty lists of csv files paths for each database\n",
    "    GNPScsvfiles2 = []\n",
    "    HMDBcsvfiles2 = []\n",
    "    MassBankcsvfiles2 = []\n",
    "    \n",
    "    #list all files and directories\n",
    "    for entry in os.listdir(input_dir):\n",
    "        if os.path.isdir(os.path.join(input_dir, entry)):\n",
    "            \n",
    "            # enter the directory with /spectral_dereplication/ results\n",
    "            sub_dir = input_dir + entry + '/spectral_dereplication'\n",
    "            if os.path.exists(sub_dir):\n",
    "                files = (glob.glob(sub_dir+'/*.csv'))\n",
    "\n",
    "                for f in files:\n",
    "                    if 'gnpsproc.' in f: \n",
    "                        GNPScsvfiles2.append(f)\n",
    "                    if 'hmdbproc.' in f: \n",
    "                        HMDBcsvfiles2.append(f)\n",
    "                    if 'mbankproc.' in f: \n",
    "                        MassBankcsvfiles2.append(f)\n",
    "   \n",
    "    # if all results present\n",
    "    if len(GNPScsvfiles2)>0 and len(HMDBcsvfiles2)>0 and len(MassBankcsvfiles2)>0:\n",
    "        \n",
    "        dict1 = {'GNPSr': GNPScsvfiles2, 'HMDBr': HMDBcsvfiles2, 'MBr': MassBankcsvfiles2} \n",
    "        df = pd.DataFrame(dict1)\n",
    "    \n",
    "        Merged_Result_df = []\n",
    "        for i, row in df.iterrows():\n",
    "            CSVfileG = pd.read_csv(df[\"GNPSr\"][i])\n",
    "            CSVfileH = pd.read_csv(df[\"HMDBr\"][i])\n",
    "            CSVfileM = pd.read_csv(df[\"MBr\"][i])\n",
    "            if os.path.exists(df[\"MBr\"][i]) and os.path.exists(df[\"HMDBr\"][i]) and os.path.exists(df[\"GNPSr\"][i]):\n",
    "                # merge on the basis of Idx\n",
    "                MergedRE = CSVfileG.merge(CSVfileH,on='id_X').merge(CSVfileM,on='id_X')\n",
    "                csvname = (df[\"GNPSr\"][i]).replace(\"gnpsproc\", \"mergedR\")\n",
    "                MergedRE.to_csv(csvname)\n",
    "                Merged_Result_df.append(csvname)\n",
    "                \n",
    "                \n",
    "    # if only GNPS and MassBank           \n",
    "    if len(GNPScsvfiles2)>0 and len(HMDBcsvfiles2)==0 and len(MassBankcsvfiles2)>0:\n",
    "            dict1 = {'GNPSr': GNPScsvfiles2, 'MBr': MassBankcsvfiles2} \n",
    "            df = pd.DataFrame(dict1)\n",
    "            Merged_Result_df = []\n",
    "            for i, row in df.iterrows():\n",
    "                CSVfileG = pd.read_csv(df[\"GNPSr\"][i])\n",
    "                CSVfileM = pd.read_csv(df[\"MBr\"][i])\n",
    "                if os.path.exists(df[\"MBr\"][i]) and os.path.exists(df[\"GNPSr\"][i]):\n",
    "                    # merge on the basis of Idx\n",
    "                    MergedRE = CSVfileG.merge(CSVfileM,on='id_X')\n",
    "                    csvname = (df[\"MBr\"][i]).replace(\"mbankproc\", \"mergedR\")\n",
    "                    MergedRE.to_csv(csvname)\n",
    "                    Merged_Result_df.append(csvname)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    # if only GNPS and Hmdb\n",
    "    if not isNaN(GNPScsvfiles2) and not isNaN(HMDBcsvfiles2) and isNaN(MassBankcsvfiles2):\n",
    "            dict1 = {'GNPSr': GNPScsvfiles2, 'HMDBr': MassBankcsvfiles2} \n",
    "            df = pd.DataFrame(dict1)\n",
    "            Merged_Result_df = []\n",
    "            for i, row in df.iterrows():\n",
    "                CSVfileG = pd.read_csv(df[\"GNPSr\"][i])\n",
    "                CSVfileH = pd.read_csv(df[\"HMDBr\"][i])\n",
    "                if os.path.exists(df[\"HMDBr\"][i]) and os.path.exists(df[\"GNPSr\"][i]):\n",
    "                    # merge on the basis of Idx\n",
    "                    MergedRE = CSVfileG.merge(CSVfileH,on='id_X')\n",
    "                    csvname = (df[\"GNPSr\"][i]).replace(\"gnpsproc\", \"mergedR\")\n",
    "                    MergedRE.to_csv(csvname)\n",
    "                    Merged_Result_df.append(csvname)\n",
    "                \n",
    "                \n",
    "                \n",
    "    # if only MBANK and Hmdb\n",
    "    if not isNaN(GNPScsvfiles2) and isNaN(HMDBcsvfiles2) and isNaN(MassBankcsvfiles2):\n",
    "            dict1 = {'GNPSr': GNPScsvfiles2, 'HMDBr': MassBankcsvfiles2} \n",
    "            df = pd.DataFrame(dict1)   \n",
    "            dict1 = {'GNPSr': GNPScsvfiles2, 'HMDBr': MassBankcsvfiles2} \n",
    "            df = pd.DataFrame(dict1)\n",
    "            Merged_Result_df = []\n",
    "            for i, row in df.iterrows():\n",
    "                CSVfileG = pd.read_csv(df[\"MBr\"][i])\n",
    "                CSVfileH = pd.read_csv(df[\"HMDBr\"][i])\n",
    "                if os.path.exists(df[\"MBr\"][i]) and os.path.exists(df[\"HMDBr\"][i]):\n",
    "                    # merge on the basis of Idx\n",
    "                    MergedRE = CSVfileM.merge(CSVfileH,on='id_X')\n",
    "                    csvname = (df[\"MBr\"][i]).replace(\"mbankproc\", \"mergedR\")\n",
    "                    MergedRE.to_csv(csvname)\n",
    "                    Merged_Result_df.append(csvname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c1754a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for multiple file\n",
    "#combine_specdb(input_dir= \"/Users/mahnoorzulfiqar/Downloads/MAW-main/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "db3b91f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for one file\n",
    "#combine_specdb(input_dir= \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8e489d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "7174f3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(combine_specdb.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e951a6a3",
   "metadata": {},
   "source": [
    "### Combine all files for spectral db dereplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d6dbeda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_allspec(input_dir):\n",
    "    \n",
    "    \"\"\"combine_allspec function combines all results from different\n",
    "    spectral dbs. Can only be used if more than one db is used \n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "    df (dataframe): dataframe from combine_specdb\n",
    "    \n",
    "    Returns:\n",
    "    dataframe: of the paths of the merged results from all files\n",
    "    \n",
    "    Usage:\n",
    "    combine_allspec(input_dir = \"usr/project/\", comb_df)\n",
    "\n",
    "    \"\"\"\n",
    "    def isNaN(string):\n",
    "        return string != string\n",
    "    # create a new directory to store all results /MetabolomicsResults/\n",
    "    path = os.path.join(input_dir, \"MetabolomicsResults\")\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "        \n",
    "        \n",
    "    Mergedcsvfiles = []\n",
    "    single_file = []\n",
    "    \n",
    "    #list all files and directories\n",
    "    for entry in os.listdir(input_dir):\n",
    "        if os.path.isdir(os.path.join(input_dir, entry)):\n",
    "            \n",
    "            # enter the directory with /spectral_dereplication/ results\n",
    "            sub_dir = input_dir + entry + '/spectral_dereplication'\n",
    "            if os.path.exists(sub_dir):\n",
    "                files = (glob.glob(sub_dir+'/*.csv'))\n",
    "\n",
    "                for f in files:\n",
    "                    if 'mergedR.csv' in f: \n",
    "                        Mergedcsvfiles.append(f)\n",
    "                    else:\n",
    "                        single_file.append(f)\n",
    "    \n",
    "    if len(Mergedcsvfiles)>0:\n",
    "        combined_csv = pd.concat([pd.read_csv(l) for l in Mergedcsvfiles], ignore_index=True)\n",
    "        combined_csv.to_csv(input_dir + 'MetabolomicsResults/SD_post_processed_combined_results.csv')\n",
    "        return(combined_csv)\n",
    "    else:\n",
    "        single_csv = pd.read_csv(single_file[0])\n",
    "        single_csv.to_csv(input_dir + 'MetabolomicsResults/SD_post_processed_combined_results.csv')\n",
    "        return(single_csv)\n",
    "    \n",
    "    #for i, row in combined_csv.iterrows():\n",
    "        #if combined_csv['GNPSSMILES'][i] == ' ' or isNaN(combined_csv['GNPSSMILES'][i]):\n",
    "            #combined_csv['GNPSSMILES'][i] = ''\n",
    "            \n",
    "    #for i, row in combined_csv.iterrows():\n",
    "        #if not isNaN(combined_csv['MBinchiKEY'][i]):\n",
    "            #try:\n",
    "                #y = pcp.get_compounds(combined_csv['MBinchiKEY'][i], 'inchikey')\n",
    "                #if len(y)>1:\n",
    "                    #combined_csv['MBSMILES'][i] = y[0].isomeric_smiles\n",
    "            #except:\n",
    "                #pass\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "3e236b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for one file\n",
    "#combine_allspec(input_dir= \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c4c5d8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for multiple file\n",
    "#combine_allspec(input_dir= \"/Users/mahnoorzulfiqar/Downloads/MAW-main/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "56418fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(combine_allspec.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0558d699",
   "metadata": {},
   "source": [
    "### Scoring Scheme for Spectral DB Dereplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cfc095",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "5f0f28c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring_spec(input_dir, spec_file):\n",
    "    \n",
    "    \"\"\"scoring_spec extracts the candidates with high scores from\n",
    "    the results from combine_allspec function \n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "    combined (dataframe): dataframe from combine_allspec\n",
    "    \n",
    "    Returns:\n",
    "    dataframe: of the all features and their results\n",
    "    csv: CSV reuslt file named MetabolomicsResults/combinedSpecDB.csv\n",
    "    which contains all the features and their Spec DB annotations\n",
    "    \n",
    "    Usage:\n",
    "    scoring_spec(input_dir = \"usr/project/\", combined)\n",
    "\n",
    "    \"\"\"\n",
    "    def isNaN(string):\n",
    "        return string != string\n",
    "    # the scoring highly depends on the following information:\n",
    "    # similarity scores should be higher than 0.75\n",
    "    # intScore >=0.50\n",
    "    # mzScore >= 0.50\n",
    "    # ratio of the matchingpeaks by the totalpeaks in the query >= 0.50\n",
    "    \n",
    "    combined = pd.read_csv(spec_file)\n",
    "    \n",
    "    def HMDB_Scoring(db, i):\n",
    "        if db['HMDBmax_similarity'][i] >= 0.75 and db['HMDBintScore'][i] >= 0.50 and db['HMDBmzScore'][i] >= 0.50 and db['HQMatchingPeaks'][i]/db['hQueryTotalPeaks'][i] >= 0.50:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    \n",
    "    def GNPS_Scoring(db, i):\n",
    "        if db['GNPSmax_similarity'][i] >= 0.90 and db['GNPSintScore'][i] >= 0.50 and db['GNPSmzScore'][i] >= 0.50 and db['GQMatchingPeaks'][i]/db['gQueryTotalPeaks'][i] >= 0.50:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    \n",
    "    def MB_Scoring(db, i):\n",
    "        if db['MBmax_similarity'][i] >= 0.50 and db['MBintScore'][i] >= 0.50 and db['MBmzScore'][i] >= 0.50 and db['MQMatchingPeaks'][i]/db['mQueryTotalPeaks'][i] >= 0.50:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    \n",
    "    for i, row in combined.iterrows():\n",
    "        \n",
    "        \n",
    "        if 'HMDBSMILES' in combined.columns and 'MBSMILES' in combined.columns and 'GNPSSMILES' in combined.columns:\n",
    "            \n",
    "            # if all DBs show good candidates accorindg to the scoring\n",
    "            if HMDB_Scoring(combined, i) and GNPS_Scoring(combined, i) and MB_Scoring(combined, i) and not isNaN(combined['GNPSSMILES'][i]) and not isNaN(combined['MBSMILES'][i]) and not isNaN(combined['HMDBSMILES'][i]):\n",
    "            \n",
    "                # calulate the tanimoto similarity between the candidates from three DBs\n",
    "            \n",
    "                # hmdb and gnps\n",
    "                HGms = [Chem.MolFromSmiles(combined['HMDBSMILES'][i]), Chem.MolFromSmiles(combined['GNPSSMILES'][i])]\n",
    "                HGfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in HGms]\n",
    "                HGtn = DataStructs.FingerprintSimilarity(HGfps[0],HGfps[1])\n",
    "            \n",
    "                # gnps and mbank\n",
    "                GMms = [Chem.MolFromSmiles(combined['GNPSSMILES'][i]), Chem.MolFromSmiles(combined['MBSMILES'][i])]\n",
    "                GMfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in GMms]\n",
    "                GMtn = DataStructs.FingerprintSimilarity(GMfps[0],GMfps[1])\n",
    "            \n",
    "                # mbank and hmdb\n",
    "                HMms = [Chem.MolFromSmiles(combined['HMDBSMILES'][i]), Chem.MolFromSmiles(combined['MBSMILES'][i])]\n",
    "                HMfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in HMms]\n",
    "                HMtn = DataStructs.FingerprintSimilarity(HMfps[0],HMfps[1])\n",
    "            \n",
    "                # add the following columns\n",
    "                combined.loc[i, 'annotation'] = 'HMDB, GNPS, MassBank'\n",
    "                combined.loc[i, 'tanimotoHG'] = HGtn\n",
    "                combined.loc[i, 'tanimotoGM'] = GMtn\n",
    "                combined.loc[i, 'tanimotoHM'] = HMtn\n",
    "                combined.loc[i, 'occurence'] = 3\n",
    "        \n",
    "            # if HMDB and GNPS show good candidates accorindg to the scoring\n",
    "            if HMDB_Scoring(combined, i) and GNPS_Scoring(combined, i) and not MB_Scoring(combined, i) and not isNaN(combined['GNPSSMILES'][i]) and not isNaN(combined['HMDBSMILES'][i]):\n",
    "                HGms = [Chem.MolFromSmiles(combined['HMDBSMILES'][i]), Chem.MolFromSmiles(combined['GNPSSMILES'][i])]\n",
    "                HGfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in HGms]\n",
    "                HGtn = DataStructs.FingerprintSimilarity(HGfps[0],HGfps[1])\n",
    "        \n",
    "                combined.loc[i, 'annotation'] = 'HMDB, GNPS'\n",
    "                combined.loc[i, 'tanimotoHG'] = HGtn\n",
    "                combined.loc[i, 'tanimotoGM'] = np.nan\n",
    "                combined.loc[i, 'tanimotoHM'] = np.nan\n",
    "                combined.loc[i, 'occurence'] = 2\n",
    "        \n",
    "            # if MassBank and GNPS show good candidates accorindg to the scoring\n",
    "            if not HMDB_Scoring(combined, i) and GNPS_Scoring(combined, i) and MB_Scoring(combined, i) and not isNaN(combined['MBSMILES'][i]) and not isNaN(combined['GNPSSMILES'][i]):\n",
    "                GMms = [Chem.MolFromSmiles(combined['GNPSSMILES'][i]), Chem.MolFromSmiles(combined['MBSMILES'][i])]\n",
    "                GMfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in GMms]\n",
    "                GMtn = DataStructs.FingerprintSimilarity(GMfps[0],GMfps[1])\n",
    "        \n",
    "                combined.loc[i, 'annotation'] = 'GNPS, MassBank'\n",
    "                combined.loc[i, 'tanimotoHG'] = np.nan\n",
    "                combined.loc[i, 'tanimotoGM'] = GMtn\n",
    "                combined.loc[i, 'tanimotoHM'] = np.nan\n",
    "                combined.loc[i, 'occurence'] = 2\n",
    "        \n",
    "            # if MassBank and HMDB show good candidates accorindg to the scoring\n",
    "            if HMDB_Scoring(combined, i) and not GNPS_Scoring(combined, i) and MB_Scoring(combined, i) and not isNaN(combined['MBSMILES'][i]) and not isNaN(combined['HMDBSMILES'][i]):\n",
    "                HMms = [Chem.MolFromSmiles(combined['HMDBSMILES'][i]), Chem.MolFromSmiles(combined['MBSMILES'][i])]\n",
    "                HMfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in HMms]\n",
    "                HMtn = DataStructs.FingerprintSimilarity(HMfps[0],HMfps[1])\n",
    "        \n",
    "                combined.loc[i, 'annotation'] = 'HMDB, MassBank'\n",
    "                combined.loc[i, 'tanimotoHG'] = np.nan\n",
    "                combined.loc[i, 'tanimotoGM'] = np.nan\n",
    "                combined.loc[i, 'tanimotoHM'] = HMtn\n",
    "                combined.loc[i, 'occurence'] = 2\n",
    "        \n",
    "            # only HMDB\n",
    "            if HMDB_Scoring(combined, i) and not GNPS_Scoring(combined, i) and not MB_Scoring(combined, i):\n",
    "        \n",
    "                combined.loc[i, 'annotation'] = 'HMDB'\n",
    "                combined.loc[i, 'tanimotoHG'] = np.nan\n",
    "                combined.loc[i, 'tanimotoGM'] = np.nan\n",
    "                combined.loc[i, 'tanimotoHM'] = np.nan\n",
    "                combined.loc[i, 'occurence'] = 1\n",
    "            \n",
    "            # only GNPS\n",
    "            if not HMDB_Scoring(combined, i) and GNPS_Scoring(combined, i) and not MB_Scoring(combined, i):\n",
    "        \n",
    "                combined.loc[i, 'annotation'] = 'GNPS'\n",
    "                combined.loc[i, 'tanimotoHG'] = np.nan\n",
    "                combined.loc[i, 'tanimotoGM'] = np.nan\n",
    "                combined.loc[i, 'tanimotoHM'] = np.nan\n",
    "                combined.loc[i, 'occurence'] = 1\n",
    "        \n",
    "            # only MassBank\n",
    "            if not HMDB_Scoring(combined, i) and not GNPS_Scoring(combined, i) and MB_Scoring(combined, i):\n",
    "        \n",
    "                combined.loc[i, 'annotation'] = 'MassBank'\n",
    "                combined.loc[i, 'tanimotoHG'] = np.nan\n",
    "                combined.loc[i, 'tanimotoGM'] = np.nan\n",
    "                combined.loc[i, 'tanimotoHM'] = np.nan\n",
    "                combined.loc[i, 'occurence'] = 1\n",
    "        \n",
    "            # none\n",
    "            if not HMDB_Scoring(combined, i) and not GNPS_Scoring(combined, i) and not MB_Scoring(combined, i):\n",
    "                combined.loc[i, 'annotation'] = 'none'\n",
    "                combined.loc[i, 'tanimotoHG'] = np.nan\n",
    "                combined.loc[i, 'tanimotoGM'] = np.nan\n",
    "                combined.loc[i, 'tanimotoHM'] = np.nan\n",
    "                combined.loc[i, 'occurence'] = 0\n",
    "        \n",
    "        if 'HMDBSMILES' not in combined.columns and 'MBSMILES' in combined.columns and 'GNPSSMILES' in combined.columns:\n",
    "\n",
    "            # if MassBank and GNPS show good candidates accorindg to the scoring\n",
    "            if GNPS_Scoring(combined, i) and MB_Scoring(combined, i) and not isNaN(combined['MBSMILES'][i]) and not isNaN(combined['GNPSSMILES'][i]):\n",
    "                GMms = [Chem.MolFromSmiles(combined['GNPSSMILES'][i]), Chem.MolFromSmiles(combined['MBSMILES'][i])]\n",
    "                GMfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in GMms]\n",
    "                GMtn = DataStructs.FingerprintSimilarity(GMfps[0],GMfps[1])\n",
    "        \n",
    "                combined.loc[i, 'annotation'] = 'GNPS, MassBank'\n",
    "                combined.loc[i, 'tanimotoGM'] = GMtn\n",
    "                combined.loc[i, 'occurence'] = 2\n",
    "            # only GNPS\n",
    "            if GNPS_Scoring(combined, i) and not MB_Scoring(combined, i):\n",
    "        \n",
    "                combined.loc[i, 'annotation'] = 'GNPS'\n",
    "                combined.loc[i, 'tanimotoGM'] = np.nan\n",
    "                combined.loc[i, 'occurence'] = 1\n",
    "        \n",
    "            # only MassBank\n",
    "            if not GNPS_Scoring(combined, i) and MB_Scoring(combined, i):\n",
    "        \n",
    "                combined.loc[i, 'annotation'] = 'MassBank'\n",
    "                combined.loc[i, 'tanimotoGM'] = np.nan\n",
    "                combined.loc[i, 'occurence'] = 1\n",
    "                \n",
    "            # none\n",
    "            if not GNPS_Scoring(combined, i) and not MB_Scoring(combined, i):\n",
    "                combined.loc[i, 'annotation'] = 'none'\n",
    "                combined.loc[i, 'tanimotoGM'] = np.nan\n",
    "                combined.loc[i, 'occurence'] = 0\n",
    "                \n",
    "                \n",
    "                \n",
    "        if 'HMDBSMILES' in combined.columns and 'MBSMILES' not in combined.columns and 'GNPSSMILES' in combined.columns:\n",
    "            # if HMDB and GNPS show good candidates accorindg to the scoring\n",
    "            if HMDB_Scoring(combined, i) and GNPS_Scoring(combined, i) and not isNaN(combined['GNPSSMILES'][i]) and not isNaN(combined['HMDBSMILES'][i]):\n",
    "                HGms = [Chem.MolFromSmiles(combined['HMDBSMILES'][i]), Chem.MolFromSmiles(combined['GNPSSMILES'][i])]\n",
    "                HGfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in HGms]\n",
    "                HGtn = DataStructs.FingerprintSimilarity(HGfps[0],HGfps[1])\n",
    "        \n",
    "                combined.loc[i, 'annotation'] = 'HMDB, GNPS'\n",
    "                combined.loc[i, 'tanimotoHG'] = HGtn\n",
    "                combined.loc[i, 'occurence'] = 2\n",
    "        \n",
    "            # only HMDB\n",
    "            if HMDB_Scoring(combined, i) and not GNPS_Scoring(combined, i):\n",
    "        \n",
    "                combined.loc[i, 'annotation'] = 'HMDB'\n",
    "                combined.loc[i, 'tanimotoHG'] = np.nan\n",
    "                combined.loc[i, 'occurence'] = 1\n",
    "            \n",
    "            # only GNPS\n",
    "            if not HMDB_Scoring(combined, i) and GNPS_Scoring(combined, i):\n",
    "        \n",
    "                combined.loc[i, 'annotation'] = 'GNPS'\n",
    "                combined.loc[i, 'tanimotoHG'] = np.nan\n",
    "                combined.loc[i, 'occurence'] = 1\n",
    "            # none\n",
    "            if not HMDB_Scoring(combined, i) and not GNPS_Scoring(combined, i):\n",
    "                combined.loc[i, 'annotation'] = 'none'\n",
    "                combined.loc[i, 'tanimotoHG'] = np.nan\n",
    "                combined.loc[i, 'occurence'] = 0\n",
    "    \n",
    "        if 'HMDBSMILES' in combined.columns and 'MBSMILES' in combined.columns and 'GNPSSMILES' not in combined.columns:\n",
    "            \n",
    "            # if MassBank and HMDB show good candidates accorindg to the scoring\n",
    "            if HMDB_Scoring(combined, i) and MB_Scoring(combined, i) and not isNaN(combined['MBSMILES'][i]) and not isNaN(combined['HMDBSMILES'][i]):\n",
    "                HMms = [Chem.MolFromSmiles(combined['HMDBSMILES'][i]), Chem.MolFromSmiles(combined['MBSMILES'][i])]\n",
    "                HMfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in HMms]\n",
    "                HMtn = DataStructs.FingerprintSimilarity(HMfps[0],HMfps[1])\n",
    "        \n",
    "                combined.loc[i, 'annotation'] = 'HMDB, MassBank'\n",
    "                combined.loc[i, 'tanimotoHM'] = HMtn\n",
    "                combined.loc[i, 'occurence'] = 2\n",
    "                \n",
    "            # only HMDB\n",
    "            if HMDB_Scoring(combined, i) and not MB_Scoring(combined, i):\n",
    "        \n",
    "                combined.loc[i, 'annotation'] = 'HMDB'\n",
    "                combined.loc[i, 'tanimotoHM'] = np.nan\n",
    "                combined.loc[i, 'occurence'] = 1\n",
    "            \n",
    "            # only MassBank\n",
    "            if not HMDB_Scoring(combined, i) and MB_Scoring(combined, i):\n",
    "        \n",
    "                combined.loc[i, 'annotation'] = 'MassBank'\n",
    "                combined.loc[i, 'tanimotoHM'] = np.nan\n",
    "                combined.loc[i, 'occurence'] = 1\n",
    "        \n",
    "            # none\n",
    "            if not HMDB_Scoring(combined, i) and not MB_Scoring(combined, i):\n",
    "                combined.loc[i, 'annotation'] = 'none'\n",
    "                combined.loc[i, 'tanimotoHM'] = np.nan\n",
    "                combined.loc[i, 'occurence'] = 0\n",
    "        \n",
    "        \n",
    "        #If only HMDB was used\n",
    "        \n",
    "        if 'HMDBSMILES' in combined.columns and 'MBSMILES' not in combined.columns and 'GNPSSMILES' not in combined.columns:\n",
    "            # only HMDB\n",
    "            if HMDB_Scoring(combined, i):\n",
    "        \n",
    "                combined.loc[i, 'annotation'] = 'HMDB'\n",
    "                combined.loc[i, 'occurence'] = 1\n",
    "            \n",
    "            # none\n",
    "            if not HMDB_Scoring(combined, i):\n",
    "                combined.loc[i, 'annotation'] = 'none'\n",
    "                combined.loc[i, 'occurence'] = 0\n",
    "                \n",
    "                \n",
    "        #If only MassBank was used      \n",
    "                \n",
    "        if 'HMDBSMILES' not in combined.columns and 'MBSMILES' in combined.columns and 'GNPSSMILES' not in combined.columns:\n",
    "            # only MassBank\n",
    "            if MB_Scoring(combined, i):\n",
    "        \n",
    "                combined.loc[i, 'annotation'] = 'MassBank'\n",
    "                combined.loc[i, 'occurence'] = 1\n",
    "            \n",
    "            # none\n",
    "            if not MB_Scoring(combined, i):\n",
    "                combined.loc[i, 'annotation'] = 'none'\n",
    "                combined.loc[i, 'occurence'] = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "        #If only GNPS was used\n",
    "        \n",
    "        if 'HMDBSMILES' not in combined.columns and 'MBSMILES' not in combined.columns and 'GNPSSMILES' in combined.columns:\n",
    "            # only GNPS\n",
    "            if GNPS_Scoring(combined, i):\n",
    "        \n",
    "                combined.loc[i, 'annotation'] = 'GNPS'\n",
    "                combined.loc[i, 'occurence'] = 1\n",
    "            \n",
    "            # none\n",
    "            if not GNPS_Scoring(combined, i):\n",
    "                combined.loc[i, 'annotation'] = 'none'\n",
    "                combined.loc[i, 'occurence'] = 0\n",
    "                \n",
    "                \n",
    "    combined.to_csv(input_dir + \"MetabolomicsResults/scoredSpecDB.csv\")\n",
    "    return(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "aba5416d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scoring_spec(input_dir = \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/\",\n",
    "#             spec_file = \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/MetabolomicsResults/SD_post_processed_combined_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "188fe433",
   "metadata": {},
   "outputs": [],
   "source": [
    "##scoring_spec(input_dir = \"/Users/mahnoorzulfiqar/Downloads/MAW-main/\",\n",
    " #            spec_file = \"/Users/mahnoorzulfiqar/Downloads/MAW-main/MetabolomicsResults/SD_post_processed_combined_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "406fd6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(scoring_spec.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adc81a7",
   "metadata": {},
   "source": [
    "### Suspect List Screening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "ff1df5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suspectListScreening(input_dir, slistcsv, SpectralDB_Results, db = \"all\"):\n",
    "    \n",
    "    \"\"\"suspectListScreening runs tanoimoto similarity score to between\n",
    "    compounds from the results from spectral DBs and suspect list\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "    slistcsv (str): path to suspect list\n",
    "    SpectralDB_Results (dataframe): dataframe from scoring_spec\n",
    "    db(str): can be all, gnps, mbank, hmdb, gm, hg, hm\n",
    "    \n",
    "    Returns:\n",
    "    dataframe: all features and specDB reults and suspect list screening \n",
    "    results\n",
    "    csv: CSV reuslt file named MetabolomicsResults/SpecDBvsSL.csv\n",
    "    which contains all the features and their Spec DB annotations\n",
    "    and suspect list occurences if any\n",
    "    \n",
    "    Usage:\n",
    "    suspectListScreening(input_dir = \"usr/project/\",\n",
    "    slistcsv = \"usr/project/suspect_list.csv\", \n",
    "    SpectralDB_Results)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    SpectralDB_Results = pd.read_csv(SpectralDB_Results)\n",
    "    Suspect_list = pd.read_csv(slistcsv)\n",
    "    \n",
    "    def isNaN(string):\n",
    "        return string != string\n",
    "    if db == \"hmdb\" or db == \"hm\" or db == \"hg\" or db == \"all\":\n",
    "        \n",
    "        # add columns to the result from scoring_spec\n",
    "        # these columns are for high similiarity canidtes between the databases and suspect list\n",
    "        SpectralDB_Results['HLsmiles'] = np.nan\n",
    "        SpectralDB_Results['HLname'] = np.nan\n",
    "\n",
    "        for i, row in SpectralDB_Results.iterrows():\n",
    "            if not isNaN(SpectralDB_Results['HMDBSMILES'][i]) and SpectralDB_Results['HMDBSMILES'][i] != \" \":\n",
    "                for j, row in Suspect_list.iterrows():\n",
    "                    LHms2 = [Chem.MolFromSmiles(SpectralDB_Results['HMDBSMILES'][i]), Chem.MolFromSmiles(Suspect_list['SMILES'][j])]\n",
    "                    LHfps2 = [AllChem.GetMorganFingerprintAsBitVect(x2,2, nBits=2048) for x2 in LHms2]\n",
    "                    LHtn2 = DataStructs.FingerprintSimilarity(LHfps2[0],LHfps2[1])\n",
    "                    if LHtn2 >= 0.9:\n",
    "                        SpectralDB_Results.loc[i, 'HLsmiles'] = Suspect_list['SMILES'][j]\n",
    "                        SpectralDB_Results.loc[i, 'HLname'] = Suspect_list['Name'][j]\n",
    "\n",
    "        # add annotations and occurences\n",
    "        for i, row in SpectralDB_Results.iterrows():\n",
    "            if not isNaN(SpectralDB_Results['HLname'][i]):\n",
    "                SpectralDB_Results['occurence'][i] = SpectralDB_Results['occurence'][i] + 1\n",
    "                if SpectralDB_Results['annotation'][i] == \"none\":\n",
    "                    SpectralDB_Results['annotation'][i] = 'Suspect_List'\n",
    "                else:\n",
    "                    SpectralDB_Results['annotation'][i] = SpectralDB_Results['annotation'][i] + ', Suspect_List'\n",
    "    \n",
    "    if db == \"gnps\" or db == \"gm\" or db == \"hg\" or db == \"all\":\n",
    "\n",
    "        # add columns to the result from scoring_spec\n",
    "        # these columns are for high similiarity canidtes between the databases and suspect list\n",
    "        SpectralDB_Results['GLsmiles'] = np.nan\n",
    "        SpectralDB_Results['GLname'] = np.nan\n",
    "\n",
    "\n",
    "        for i, row in SpectralDB_Results.iterrows():\n",
    "\n",
    "            if not isNaN(SpectralDB_Results['GNPSSMILES'][i]) and SpectralDB_Results['GNPSSMILES'][i] != \" \":\n",
    "                for k, row in Suspect_list.iterrows():\n",
    "                    LGms2 = [Chem.MolFromSmiles(SpectralDB_Results['GNPSSMILES'][i]), Chem.MolFromSmiles(Suspect_list['SMILES'][k])]\n",
    "                    LGfps2 = [AllChem.GetMorganFingerprintAsBitVect(x2,2, nBits=2048) for x2 in LGms2]\n",
    "                    LGtn2 = DataStructs.FingerprintSimilarity(LGfps2[0],LGfps2[1])\n",
    "                    if LGtn2 >= 0.9:\n",
    "                        SpectralDB_Results.loc[i, 'GLsmiles'] = Suspect_list['SMILES'][k]\n",
    "                        SpectralDB_Results.loc[i, 'GLname'] = Suspect_list['Name'][k]\n",
    "        # add annotations and occurences\n",
    "        for i, row in SpectralDB_Results.iterrows():\n",
    "            if not isNaN(SpectralDB_Results['GLname'][i]):\n",
    "                SpectralDB_Results['occurence'][i] = SpectralDB_Results['occurence'][i] + 1\n",
    "                if SpectralDB_Results['annotation'][i] == \"none\":\n",
    "                    SpectralDB_Results['annotation'][i] = 'Suspect_List'\n",
    "                else:\n",
    "                    SpectralDB_Results['annotation'][i] = SpectralDB_Results['annotation'][i] + ', Suspect_List'\n",
    "    \n",
    "    if db == \"mbank\" or db == \"gm\" or db == \"hm\" or db == \"all\":\n",
    "\n",
    "        # add columns to the result from scoring_spec\n",
    "        # these columns are for high similiarity canidtes between the databases and suspect list\n",
    "        SpectralDB_Results['MLsmiles'] = np.nan\n",
    "        SpectralDB_Results['MLname'] = np.nan\n",
    "\n",
    "\n",
    "        for i, row in SpectralDB_Results.iterrows():\n",
    "            if not isNaN(SpectralDB_Results['MBSMILES'][i]) and SpectralDB_Results['MBSMILES'][i] != \" \":\n",
    "                for l, row in Suspect_list.iterrows():\n",
    "                    LMms2 = [Chem.MolFromSmiles(SpectralDB_Results['MBSMILES'][i]), Chem.MolFromSmiles(Suspect_list['SMILES'][l])]\n",
    "                    LMfps2 = [AllChem.GetMorganFingerprintAsBitVect(x2,2, nBits=2048) for x2 in LMms2]\n",
    "                    LMtn2 = DataStructs.FingerprintSimilarity(LMfps2[0],LMfps2[1])\n",
    "                    if LMtn2 >= 0.9:\n",
    "                        SpectralDB_Results.loc[i, 'MLsmiles'] = Suspect_list['SMILES'][l]\n",
    "                        SpectralDB_Results.loc[i, 'MLname'] = Suspect_list['Name'][l]\n",
    "\n",
    "        # add annotations and occurences\n",
    "        for i, row in SpectralDB_Results.iterrows():\n",
    "            if not isNaN(SpectralDB_Results['MLname'][i]):\n",
    "                SpectralDB_Results['occurence'][i] = SpectralDB_Results['occurence'][i] + 1\n",
    "                if SpectralDB_Results['annotation'][i] == \"none\":\n",
    "                    SpectralDB_Results['annotation'][i] = 'Suspect_List'\n",
    "                else:\n",
    "                    SpectralDB_Results['annotation'][i] = SpectralDB_Results['annotation'][i] + ', Suspect_List'\n",
    "                \n",
    "    SpectralDB_Results.to_csv(input_dir + \"MetabolomicsResults/SpecDBvsSL.csv\")\n",
    "    return(SpectralDB_Results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "764e8b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#suspectListScreening(input_dir = \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/\", \n",
    "                     #slistcsv = \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/SkeletonemaSuspectListV1.csv\", \n",
    "                     #SpectralDB_Results = \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/MetabolomicsResults/scoredSpecDB.csv\", \n",
    "                     #db = \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "2cb634f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#suspectListScreening(input_dir = \"/Users/mahnoorzulfiqar/Downloads/MAW-main/\", \n",
    "                     #slistcsv = \"/Users/mahnoorzulfiqar/Downloads/MAW-main/SkeletonemaSuspectListV1.csv\", \n",
    "                     ##SpectralDB_Results = \"/Users/mahnoorzulfiqar/Downloads/MAW-main/MetabolomicsResults/scoredSpecDB.csv\", \n",
    "                     #db = \"gm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eb9290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "a4a29d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(suspectListScreening.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82dbf13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce884025",
   "metadata": {},
   "source": [
    "# Final Candidate List Curation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e45b773",
   "metadata": {},
   "source": [
    "## MetFrag Curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc97b626",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "e0e33527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metfrag_curation(input_dir, metfragcsv, sl = True):\n",
    "    def isNaN(string):\n",
    "        return string != string\n",
    "    \n",
    "    \"\"\"metfrag_curation checks which database produced results. If both \n",
    "    did, it checks whether it was the same compound as candidate, if not,\n",
    "    add PubChem or any of the two databases with similarity to Suspect\n",
    "    list\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "    metfragcsv (str): path to combined metfrag results:\n",
    "    MetabolomicsResults/MetFrag_combined.csv\n",
    "    \n",
    "    Returns:\n",
    "    dataframe: dataframe of curated metfrag results\n",
    "    csv: MetabolomicsResults/metfrag_curated.csv\n",
    "    \n",
    "    Usage:\n",
    "    metfrag_curation(input_dir = \"usr/project/\", \n",
    "    metfragcsv = \"usr/project/MetabolomicsResults/MetFrag_combined.csv\")\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    metfrag = pd.read_csv(metfragcsv)\n",
    "    for i, row in metfrag.iterrows():\n",
    "        \n",
    "        \n",
    "        # If only KEGG\n",
    "        if not isNaN(metfrag['KG_SMILES'][i]) and isNaN(metfrag['PC_SMILES'][i]):\n",
    "            metfrag.loc[i, 'Annotation_M'] = 'KEGG'\n",
    "            if sl:\n",
    "                if metfrag['KGSL_Score'][i]>=0.9:\n",
    "                    metfrag.loc[i, 'Annotation_M'] = 'KEGG, SuspectList'\n",
    "                else:\n",
    "                    metfrag.loc[i, 'Annotation_M'] = 'KEGG'\n",
    "    \n",
    "        # If only Pubchem\n",
    "        if not isNaN(metfrag['PC_SMILES'][i]) and isNaN(metfrag['KG_SMILES'][i]):\n",
    "            metfrag.loc[i, 'Annotation_M'] = 'PubChem'\n",
    "            if sl:\n",
    "                if metfrag['PCSL_Score'][i]>=0.9:\n",
    "                    metfrag.loc[i, 'Annotation_M'] = 'PubChem, SuspectList'\n",
    "                else:\n",
    "                    metfrag.loc[i, 'Annotation_M'] = 'PubChem'           \n",
    "        \n",
    "    \n",
    "        # If both, calculate the similarity\n",
    "        if not isNaN(metfrag['PC_SMILES'][i]) and not isNaN(metfrag['KG_SMILES'][i]):\n",
    "        \n",
    "            PKms = [Chem.MolFromSmiles(metfrag['KG_SMILES'][i]), Chem.MolFromSmiles(metfrag['PC_SMILES'][i])]\n",
    "            PKfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in PKms]\n",
    "            PKtn = DataStructs.FingerprintSimilarity(PKfps[0],PKfps[1])\n",
    "        \n",
    "            # if both are similar, add both\n",
    "            if PKtn == 1:\n",
    "                metfrag.loc[i, 'Annotation_M'] = 'KEGG, PubChem'\n",
    "                if sl:\n",
    "                    if metfrag['KGSL_Score'][i]>=0.9 and metfrag['PCSL_Score'][i]>=0.9:\n",
    "                        metfrag.loc[i, 'Annotation_M'] = metfrag['Annotation_M'][i] + \", SuspectList\"\n",
    "        \n",
    "            # if not similar:\n",
    "            # check Suspect list score and Fragmenter Score\n",
    "            \n",
    "            else:\n",
    "                if not isNaN(metfrag[\"KG_Score\"][i]):\n",
    "                    metfrag.loc[i, 'Annotation_M'] = 'KEGG'\n",
    "                else:\n",
    "                    metfrag.loc[i, 'Annotation_M'] = 'PubChem'\n",
    "                    \n",
    "                                \n",
    "    metfrag.to_csv(input_dir + \"MetabolomicsResults/metfrag_curated.csv\")  \n",
    "    return(metfrag)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "aa2dca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(metfrag_curation.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "e1baff43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#metfrag_curation(input_dir = \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/\",\n",
    "                 ##metfragcsv = \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/MetabolomicsResults/MetFrag_combined.csv\", \n",
    "                 #sl = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bb7a26",
   "metadata": {},
   "source": [
    "## SIRIUS Results Curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "b4f07a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#metfrag_curation(input_dir = \"/Users/mahnoorzulfiqar/Downloads/MAW-main/\", \n",
    "##                 metfragcsv = \"/Users/mahnoorzulfiqar/Downloads/MAW-main/MetabolomicsResults/MetFrag_combined.csv\", \n",
    "#                 sl = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "73654ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sirius_curation(input_dir, siriuscsv, sl = True):\n",
    "    def isNaN(string):\n",
    "        return string != string\n",
    "    \"\"\"sirius_curation checks if candidate selected has a good score for \n",
    "    explained intensity. It also checks if there was any similarity to\n",
    "    a compound from Suspect list\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "    siriuscsv (str): path to combined metfrag results:\n",
    "    MetabolomicsResults/Sirius_combined.csv\n",
    "    \n",
    "    Returns:\n",
    "    dataframe: dataframe of curated sirius results\n",
    "    csv: MetabolomicsResults/sirius_curated.csv\n",
    "    \n",
    "    Usage:\n",
    "    sirius_curation(input_dir = \"usr/project/\", \n",
    "    siriuscsv = \"usr/project/MetabolomicsResults/Sirius_combined.csv\")\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    sirius = pd.read_csv(siriuscsv)\n",
    "    for i, row in sirius.iterrows():\n",
    "    \n",
    "        # If the explained intensity is greater than 0.70 and there is no suspect list entry\n",
    "        if sirius['exp_int'][i] >= 0.70 and \"SIRIUS_SL\" not in sirius['Result'][i]:\n",
    "            sirius.loc[i, 'Annotation_S'] = 'SIRIUS'\n",
    "            #sirius.loc[i, 'SMILES_final'] = sirius['SMILES'][i]\n",
    "        else:\n",
    "            if sl:\n",
    "                \n",
    "                #If the explained intensity is greater than 0.70 and there is an entry from suspect list\n",
    "                if sirius['exp_int'][i] >= 0.70 and \"SIRIUS_SL\" in sirius['Result'][i]:\n",
    "                    sirius.loc[i, 'Annotation_S'] = 'SIRIUS, SuspectList'\n",
    "                    #sirius.loc[i, 'SMILES_final'] = sirius['SMILES'][i]\n",
    "    \n",
    "                # if the intensity is less thna 0.70 but it still is similar to an entry in Suspect list,\n",
    "                elif sirius['exp_int'][i] < 0.70 and \"SIRIUS_SL\" in sirius['Result'][i]:\n",
    "                    sirius.loc[i, 'Annotation_S'] = 'SIRIUS, SuspectList'\n",
    "                    #sirius.loc[i, 'SMILES_final'] = sirius['SMILES'][i]\n",
    "        \n",
    "    sirius.to_csv(input_dir + \"MetabolomicsResults/sirius_curated.csv\")\n",
    "    return(sirius)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "80945399",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sirius_curation.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e3454398",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sirius_curation(input_dir = \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/\",\n",
    " #                siriuscsv = \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/MetabolomicsResults/SIRIUS_combined.csv\", \n",
    " #                sl = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "b7d4cfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sirius_curation(input_dir = \"/Users/mahnoorzulfiqar/Downloads/MAW-main/\", \n",
    "              #   siriuscsv = \"/Users/mahnoorzulfiqar/Downloads/MAW-main/MetabolomicsResults/SIRIUS_combined.csv\", \n",
    "              #   sl = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2ff69c",
   "metadata": {},
   "source": [
    "## combine curated S and M results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "8ac2db42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineSM(input_dir, metfragcsv, siriuscsv):\n",
    "    \n",
    "    \"\"\"combineSM prioritizes Sirius and Suspect list over PubChem and\n",
    "    KEGG\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "    sirius (dataframe): result of sirius_curation\n",
    "    metfrag (dataframe): result of metfrag_curation\n",
    "    \n",
    "    Returns:\n",
    "    dataframe: dataframe of combined curated sirius and metfrag results\n",
    "    csv: \"MetabolomicsResults/combinedSM.csv\"\n",
    "    \n",
    "    Usage:\n",
    "    combineSM(input_dir = \"usr/project/\", metfrag, sirius)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def isNaN(string):\n",
    "        return string != string\n",
    "    \n",
    "    metfrag = pd.read_csv(metfragcsv)\n",
    "    sirius = pd.read_csv(siriuscsv)\n",
    "    S_M_CSV = pd.concat([sirius, metfrag], axis = 1, levels = [\"id_X\"])\n",
    "    \n",
    "    for i, rows in S_M_CSV.iterrows():\n",
    "        # if results has Sirius Structure annotation, and the explained inetnsity is >= 0.70, keep the annotation as is.\n",
    "        if S_M_CSV[\"Result\"][i] == \"SIRIUS_STR\" and S_M_CSV['exp_int'][i] >= 0.70:\n",
    "            S_M_CSV.loc[i, 'Annotation_C'] = S_M_CSV['Annotation_S'][i]\n",
    "            \n",
    "            # to add to that annotation\n",
    "            if not isNaN(S_M_CSV[\"Annotation_M\"][i]):\n",
    "                # if annotation has PubChem, by default add SIRIUS\n",
    "                if S_M_CSV[\"Annotation_M\"][i] == \"KEGG\":\n",
    "                    SKms = [Chem.MolFromSmiles(S_M_CSV['SMILES'][i]), Chem.MolFromSmiles(S_M_CSV['KG_SMILES'][i])]\n",
    "                    SKfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in SKms]\n",
    "                    SKtn = DataStructs.FingerprintSimilarity(SKfps[0],SKfps[1])\n",
    "\n",
    "                    if SKtn >= 0.75:\n",
    "\n",
    "                        S_M_CSV.loc[i, 'Annotation_C'] = S_M_CSV['Annotation_S'][i] +', KEGG'\n",
    "\n",
    "                    else:\n",
    "                        S_M_CSV.loc[i, 'Annotation_C'] = S_M_CSV['Annotation_S'][i]\n",
    "                        \n",
    "                # if annotation has PubChem, by default add SIRIUS\n",
    "                if S_M_CSV[\"Annotation_M\"][i] == \"PubChem\":\n",
    "                    PSms = [Chem.MolFromSmiles(S_M_CSV['SMILES'][i]), Chem.MolFromSmiles(S_M_CSV['PC_SMILES'][i])]\n",
    "                    PSfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in PSms]\n",
    "                    PStn = DataStructs.FingerprintSimilarity(PSfps[0],PSfps[1])\n",
    "\n",
    "                    # if similar strcutres, then add Pubchme and sirius\n",
    "                    if PStn >= 0.7:\n",
    "\n",
    "                        S_M_CSV.loc[i, 'Annotation_C'] = S_M_CSV['Annotation_S'][i] + ', PubChem'\n",
    "\n",
    "                    # if not then just keep sirius\n",
    "                    else:\n",
    "                        S_M_CSV.loc[i, 'Annotation_C'] = S_M_CSV['Annotation_S'][i]\n",
    "                        \n",
    "                        \n",
    "                if S_M_CSV[\"Annotation_M\"][i] == \"KEGG, PubChem\":\n",
    "                    SKms = [Chem.MolFromSmiles(S_M_CSV['SMILES'][i]), Chem.MolFromSmiles(S_M_CSV['KG_SMILES'][i])]\n",
    "                    SKfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in SKms]\n",
    "                    SKtn = DataStructs.FingerprintSimilarity(SKfps[0],SKfps[1])\n",
    "                    if SKtn >= 0.7:\n",
    "\n",
    "                        S_M_CSV.loc[i, 'Annotation_C'] = S_M_CSV['Annotation_S'][i] +', KEGG, PubChem'\n",
    "\n",
    "                    else:\n",
    "                        S_M_CSV.loc[i, 'Annotation_C'] = S_M_CSV['Annotation_S'][i]\n",
    "    S_M_CSV.to_csv(input_dir + \"MetabolomicsResults/combinedSM.csv\")\n",
    "    return(S_M_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "3c04409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(combineSM.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "db5d8624",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combineSM(input_dir = \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/\",\n",
    "#          metfragcsv = \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/MetabolomicsResults/metfrag_curated.csv\", \n",
    " #         siriuscsv = \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/MetabolomicsResults/sirius_curated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "9d60be8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#combineSM(input_dir = \"/Users/mahnoorzulfiqar/Downloads/MAW-main/\",\n",
    " #         metfragcsv = \"/Users/mahnoorzulfiqar/Downloads/MAW-main/MetabolomicsResults/metfrag_curated.csv\", \n",
    " #         siriuscsv = \"/Users/mahnoorzulfiqar/Downloads/MAW-main/MetabolomicsResults/sirius_curated.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28372129",
   "metadata": {},
   "source": [
    "## Spec DB Curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "b43fb2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def specDB_Curation(input_dir, combinedx, sl = True, db = \"all\"):\n",
    "    \n",
    "    \"\"\"specDB_Curation prioritizes in the following manner: gnps>\n",
    "    mbank>suspectlist>hmdb\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "    \n",
    "    combined: dataframe from either suspectListScreening function if\n",
    "    sl = True OR from scoring_spec if sl = False\n",
    "    \n",
    "    Returns:\n",
    "    dataframe: with curated Spectral DB results\n",
    "    csv: \"MetabolomicsResults/curatedSDB.csv\"\n",
    "    \n",
    "    Usage:\n",
    "    specDB_Curation(input_dir = \"usr/project/\",combinedx, sl = True)\n",
    "\n",
    "    \"\"\"\n",
    "    def isNaN(string):\n",
    "        return string != string\n",
    "    def HMDB_Scoring(db, i):\n",
    "        if db['HMDBmax_similarity'][i] >= 0.75 and db['HMDBintScore'][i] >= 0.50 and db['HMDBmzScore'][i] >= 0.50 and db['HQMatchingPeaks'][i]/db['hQueryTotalPeaks'][i] >= 0.50:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def GNPS_Scoring(db, i):\n",
    "        if db['GNPSmax_similarity'][i] >= 0.90 and db['GNPSintScore'][i] >= 0.50 and db['GNPSmzScore'][i] >= 0.50 and db['GQMatchingPeaks'][i]/db['gQueryTotalPeaks'][i] >= 0.50:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    \n",
    "    def MB_Scoring(db, i):\n",
    "        if db['MBmax_similarity'][i] >= 0.50 and db['MBintScore'][i] >= 0.50 and db['MBmzScore'][i] >= 0.50 and db['MQMatchingPeaks'][i]/db['mQueryTotalPeaks'][i] >= 0.50:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    combined = pd.read_csv(combinedx)\n",
    "    \n",
    "    \n",
    "    # remove the similarity scores from low scoring candidates\n",
    "    for i, row in combined.iterrows():\n",
    "        if db == \"all\" or db == \"hg\" or db == \"hm\" or db == \"hmdb\":\n",
    "            if not HMDB_Scoring(combined, i):\n",
    "                combined['HMDBcompoundID'][i] = np.nan\n",
    "        if db == \"all\" or db == \"hg\" or db == \"gm\" or db == \"gnps\":\n",
    "            if not GNPS_Scoring(combined, i):\n",
    "                combined['GNPSspectrumID'][i] = np.nan\n",
    "        if db == \"all\" or db == \"gm\" or db == \"hm\" or db == \"mbank\":\n",
    "            if not MB_Scoring(combined, i):\n",
    "                combined['MBspectrumID'][i] = np.nan\n",
    "    \n",
    "    # if sl = True\n",
    "    if sl:\n",
    "        for i, row in combined.iterrows():\n",
    "            # if all databases are used to generate results\n",
    "            if db == \"all\":\n",
    "                \n",
    "                # if all dbs have results\n",
    "                if not isNaN(combined['GNPSspectrumID'][i]) and not isNaN(combined['MBspectrumID'][i]) and not isNaN(combined['HMDBcompoundID'][i]):\n",
    "        \n",
    "                    # entries with same candidate from all Spectral DBs\n",
    "                    if combined['tanimotoHG'][i] == 1.0 and combined['tanimotoGM'][i] == 1.0 and combined['tanimotoHM'][i] == 1.0:\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS, HMDB, MassBank'\n",
    "                        #entries with same candidate in suspect list, as in all Spectral DBs\n",
    "                        if combined['GLname'][i] == combined['HLname'][i]== combined['MLname'][i]:\n",
    "                            combined.loc[i, 'Annotation'] = 'GNPS, HMDB, MassBank, SuspectList'\n",
    "                \n",
    "                    # same candidate from GNPS and HMDB        \n",
    "                    if combined['tanimotoHG'][i] == 1.0 and combined['tanimotoGM'][i] != 1.0 and combined['tanimotoHM'][i] != 1.0:\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS, HMDB'\n",
    "                        # if its present in Suspect List\n",
    "                        if combined['GLname'][i] == combined['HLname'][i]:\n",
    "                            combined.loc[i, 'Annotation'] = 'GNPS, HMDB, SuspectList'\n",
    "        \n",
    "                    # same candidate from GNPS and MassBank        \n",
    "                    if combined['tanimotoHG'][i] != 1.0 and combined['tanimotoGM'][i] == 1.0 and combined['tanimotoHM'][i] != 1.0:\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS, MassBank'\n",
    "                        # if its present in Suspect List\n",
    "                        if combined['GLname'][i] == combined['MLname'][i]:\n",
    "                            combined.loc[i, 'Annotation'] = 'GNPS, MassBank, SuspectList'\n",
    "                \n",
    "                    # same candidate from MassBank and HMDB        \n",
    "                    if combined['tanimotoHG'][i] != 1.0 and combined['tanimotoGM'][i] != 1.0 and combined['tanimotoHM'][i] == 1.0:\n",
    "                        combined.loc[i, 'Annotation'] = 'MassBank, HMDB'\n",
    "                        # if its present in Suspect List\n",
    "                        if combined['MLname'][i] == combined['HLname'][i]:\n",
    "                            combined.loc[i, 'Annotation'] = 'HMDB, MassBank, SuspectList'\n",
    "                    \n",
    "                    # only one database must be selected based on SuspectList annotation\n",
    "                    if combined['tanimotoHG'][i] != 1.0 and combined['tanimotoGM'][i] != 1.0 and combined['tanimotoHM'][i] != 1.0:\n",
    "            \n",
    "                        # only GNPS has SuspectList annotation\n",
    "                        if not isNaN(combined['GLname'][i]):\n",
    "\n",
    "                            combined.loc[i, 'Annotation'] = 'GNPS, SuspectList'\n",
    "            \n",
    "            \n",
    "                        # only MassBank has SuspectList annotation\n",
    "                        elif not isNaN(combined['MLname'][i]):\n",
    "                            combined.loc[i, 'Annotation'] = 'MassBank, SuspectList'\n",
    "            \n",
    "            \n",
    "                        # only HMDB has SuspectList annotation\n",
    "                        #elif not isNaN(combined['HLname'][i]):\n",
    "                            #combined.loc[i, 'Annotation'] = 'HMDB, SuspectList'\n",
    "            \n",
    "        \n",
    "                        # all different annotations, take GNPS\n",
    "                        else:\n",
    "                            if not isNaN(combined['GNPSSMILES'][i]):\n",
    "                                combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "                            else:\n",
    "                                combined.loc[i, 'Annotation'] = 'MassBank'\n",
    "    \n",
    "                #### When there is an annotation from two DBs #####\n",
    "\n",
    "                # only GNPS and HMDB\n",
    "                if not isNaN(combined['GNPSspectrumID'][i]) and isNaN(combined['MBspectrumID'][i]) and not isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    if combined['tanimotoHG'][i] == 1.0:\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS, HMDB'\n",
    "                        if not isNaN(combined['GLname'][i]) and not isNaN(combined['HLname'][i]):\n",
    "                            if combined['GLname'][i] == combined['HLname'][i]:\n",
    "                                combined.loc[i, 'Annotation'] = 'GNPS, HMDB, SuspectList'\n",
    "                    else:\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "                        if not isNaN(combined['GLname'][i]):\n",
    "                            combined.loc[i, 'Annotation'] = 'GNPS, SuspectList'\n",
    "                        #elif not isNaN(combined['HLname'][i]):\n",
    "                            #combined.loc[i, 'Annotation'] = 'HMDB, SuspectList'\n",
    "\n",
    "\n",
    "                # only GNPS and MassBank\n",
    "                if not isNaN(combined['GNPSspectrumID'][i]) and not isNaN(combined['MBspectrumID'][i]) and isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    if combined['tanimotoGM'][i] == 1.0:\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS, MassBank'\n",
    "                        if not isNaN(combined['GLname'][i]) and not isNaN(combined['MLname'][i]):\n",
    "                            if combined['GLname'][i] == combined['MLname'][i]:\n",
    "                                combined.loc[i, 'Annotation'] = 'GNPS, MassBank, SuspectList'\n",
    "                                \n",
    "                    else:\n",
    "                        if not isNaN(combined['GLname'][i]):\n",
    "                            combined.loc[i, 'Annotation'] = 'GNPS, SuspectList'\n",
    "                        elif not isNaN(combined['MLname'][i]):\n",
    "                            combined.loc[i, 'Annotation'] = 'MassBank, SuspectList'\n",
    "                        elif not isNaN(combined['GNPSSMILES'][i]):\n",
    "                            combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "                        else:\n",
    "                            combined.loc[i, 'Annotation'] = 'MassBank'\n",
    "\n",
    "                # only MassBank and HMDB\n",
    "\n",
    "                if isNaN(combined['GNPSspectrumID'][i]) and not isNaN(combined['MBspectrumID'][i]) and not isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    if combined['tanimotoHM'][i] == 1.0:\n",
    "                        combined.loc[i, 'Annotation'] = 'HMDB, MassBank'\n",
    "                        if not isNaN(combined['HLname'][i]) and not isNaN(combined['MLname'][i]):\n",
    "                            if combined['HLname'][i] == combined['MLname'][i]:\n",
    "                                combined.loc[i, 'Annotation'] = 'HMDB, MassBank, SuspectList'\n",
    "                                \n",
    "                    else:\n",
    "                        if not isNaN(combined['MLname'][i]):\n",
    "                            combined.loc[i, 'Annotation'] = 'MassBank, SuspectList'\n",
    "                        #elif not isNaN(combined['MLname'][i]):\n",
    "                            #combined.loc[i, 'Annotation'] = 'MassBank, SuspectList'\n",
    "                        #elif not isNaN(combined['GNPSSMILES'][i]):\n",
    "                            #combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "                        else:\n",
    "                            combined.loc[i, 'Annotation'] = 'MassBank'\n",
    "\n",
    "\n",
    "\n",
    "                ##### When there is an annotation from one DBs #####\n",
    "\n",
    "\n",
    "                # only GNPS\n",
    "                if not isNaN(combined['GNPSspectrumID'][i]) and isNaN(combined['MBspectrumID'][i]) and isNaN(combined['HMDBcompoundID'][i]):\n",
    "\n",
    "                    #If also SuspectList\n",
    "                    if not isNaN(combined['GLname'][i]):\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS, SuspectList'\n",
    "                    elif not isNaN(combined['GNPSSMILES'][i]):\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "\n",
    "                # only MassBank\n",
    "                if isNaN(combined['GNPSspectrumID'][i]) and not isNaN(combined['MBspectrumID'][i]) and isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    combined.loc[i, 'Annotation'] = 'MassBank'\n",
    "                    #If also SuspectList\n",
    "                    if not isNaN(combined['MLname'][i]):\n",
    "                        combined.loc[i, 'Annotation'] = 'MassBank, SuspectList'\n",
    "\n",
    "                # only HMDB\n",
    "                #if isNaN(combined['GNPSspectrumID'][i]) and isNaN(combined['MBspectrumID'][i]) and not isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    #combined.loc[i, 'Annotation'] = 'HMDB'\n",
    "                    #If also SuspectList\n",
    "                    #if not isNaN(combined['HLname'][i]):\n",
    "                        #combined.loc[i, 'Annotation'] = 'HMDB, SuspectList'\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "            \n",
    "            \n",
    "            # if GNPS AND MassBank databases are used to generate results\n",
    "            if db == \"gm\":\n",
    "                \n",
    "                # only GNPS and MassBank\n",
    "                if not isNaN(combined['GNPSspectrumID'][i]) and not isNaN(combined['MBspectrumID'][i]):\n",
    "                    if combined['tanimotoGM'][i] == 1.0:\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS, MassBank'\n",
    "                        if not isNaN(combined['GLname'][i]) and not isNaN(combined['MLname'][i]):\n",
    "                            if combined['GLname'][i] == combined['MLname'][i]:\n",
    "                                combined.loc[i, 'Annotation'] = 'GNPS, MassBank, SuspectList'\n",
    "                                \n",
    "                    else:\n",
    "                        if not isNaN(combined['GLname'][i]):\n",
    "                            combined.loc[i, 'Annotation'] = 'GNPS, SuspectList'\n",
    "                        elif not isNaN(combined['MLname'][i]):\n",
    "                            combined.loc[i, 'Annotation'] = 'MassBank, SuspectList'\n",
    "                        elif not isNaN(combined['GNPSSMILES'][i]):\n",
    "                            combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "                        else:\n",
    "                            combined.loc[i, 'Annotation'] = 'MassBank'\n",
    "\n",
    "                \n",
    "                \n",
    "                # only GNPS\n",
    "                if not isNaN(combined['GNPSspectrumID'][i]) and isNaN(combined['MBspectrumID'][i]):\n",
    "\n",
    "                    #If also SuspectList\n",
    "                    if not isNaN(combined['GLname'][i]):\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS, SuspectList'\n",
    "                    elif not isNaN(combined['GNPSSMILES'][i]):\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "\n",
    "                # only MassBank\n",
    "                if isNaN(combined['GNPSspectrumID'][i]) and not isNaN(combined['MBspectrumID'][i]):\n",
    "                    combined.loc[i, 'Annotation'] = 'MassBank'\n",
    "                    #If also SuspectList\n",
    "                    if not isNaN(combined['MLname'][i]):\n",
    "                        combined.loc[i, 'Annotation'] = 'MassBank, SuspectList'\n",
    "            \n",
    "            \n",
    "            \n",
    "            # if GNPS AND HMDB databases are used to generate results\n",
    "            if db == \"hg\":\n",
    "                \n",
    "                # only GNPS and HMDB\n",
    "                if not isNaN(combined['GNPSspectrumID'][i]) and not isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    if combined['tanimotoHG'][i] == 1.0:\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS, HMDB'\n",
    "                        if not isNaN(combined['GLname'][i]) and not isNaN(combined['HLname'][i]):\n",
    "                            if combined['GLname'][i] == combined['HLname'][i]:\n",
    "                                combined.loc[i, 'Annotation'] = 'GNPS, HMDB, SuspectList'\n",
    "                    else:\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "                        if not isNaN(combined['GLname'][i]):\n",
    "                            combined.loc[i, 'Annotation'] = 'GNPS, SuspectList'\n",
    "                        #elif not isNaN(combined['HLname'][i]):\n",
    "                            #combined.loc[i, 'Annotation'] = 'HMDB, SuspectList'\n",
    "\n",
    "                # only GNPS\n",
    "                if not isNaN(combined['GNPSspectrumID'][i]) and isNaN(combined['HMDBcompoundID'][i]):\n",
    "\n",
    "                    #If also SuspectList\n",
    "                    if not isNaN(combined['GLname'][i]):\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS, SuspectList'\n",
    "                    elif not isNaN(combined['GNPSSMILES'][i]):\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "                # only HMDB\n",
    "                #if isNaN(combined['GNPSspectrumID'][i]) and not isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    #combined.loc[i, 'Annotation'] = 'HMDB'\n",
    "                    #If also SuspectList\n",
    "                    #if not isNaN(combined['HLname'][i]):\n",
    "                        #combined.loc[i, 'Annotation'] = 'HMDB, SuspectList'\n",
    "            \n",
    "            # if MassBank AND HMDB databases are used to generate results\n",
    "            if db == \"hm\":\n",
    "                \n",
    "                # only MassBank and HMDB\n",
    "\n",
    "                if not isNaN(combined['MBspectrumID'][i]) and not isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    if combined['tanimotoHM'][i] == 1.0:\n",
    "                        combined.loc[i, 'Annotation'] = 'HMDB, MassBank'\n",
    "                        if not isNaN(combined['HLname'][i]) and not isNaN(combined['MLname'][i]):\n",
    "                            if combined['HLname'][i] == combined['MLname'][i]:\n",
    "                                combined.loc[i, 'Annotation'] = 'HMDB, MassBank, SuspectList'\n",
    "                                \n",
    "                    else:\n",
    "                        if not isNaN(combined['MLname'][i]):\n",
    "                            combined.loc[i, 'Annotation'] = 'MassBank, SuspectList'\n",
    "                        else:\n",
    "                            combined.loc[i, 'Annotation'] = 'MassBank'\n",
    "                \n",
    "                \n",
    "                \n",
    "                # only MassBank\n",
    "                if not isNaN(combined['MBspectrumID'][i]) and isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    combined.loc[i, 'Annotation'] = 'MassBank'\n",
    "                    #If also SuspectList\n",
    "                    if not isNaN(combined['MLname'][i]):\n",
    "                        combined.loc[i, 'Annotation'] = 'MassBank, SuspectList'\n",
    "\n",
    "                # only HMDB\n",
    "                #if isNaN(combined['MBspectrumID'][i]) and not isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    #combined.loc[i, 'Annotation'] = 'HMDB'\n",
    "                    #If also SuspectList\n",
    "                    #if not isNaN(combined['HLname'][i]):\n",
    "                        #combined.loc[i, 'Annotation'] = 'HMDB, SuspectList'\n",
    "            if db == \"gnps\":\n",
    "                if not isNaN(combined['GNPSspectrumID'][i]):\n",
    "\n",
    "                    #If also SuspectList\n",
    "                    if not isNaN(combined['GLname'][i]):\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS, SuspectList'\n",
    "                    elif not isNaN(combined['GNPSSMILES'][i]):\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "            if db == \"mbank\":\n",
    "                # only MassBank\n",
    "                if not isNaN(combined['MBspectrumID'][i]):\n",
    "                    combined.loc[i, 'Annotation'] = 'MassBank'\n",
    "                    #If also SuspectList\n",
    "                    if not isNaN(combined['MLname'][i]):\n",
    "                        combined.loc[i, 'Annotation'] = 'MassBank, SuspectList'\n",
    "            #if db == \"hmdb\":\n",
    "                # only HMDB\n",
    "                #if not isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    #combined.loc[i, 'Annotation'] = 'HMDB'\n",
    "                    #If also SuspectList\n",
    "                    #if not isNaN(combined['HLname'][i]):\n",
    "                        #combined.loc[i, 'Annotation'] = 'HMDB, SuspectList'\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "    else:\n",
    "        for i, row in combined.iterrows():\n",
    "            #if all databases were used\n",
    "            if db == \"all\":\n",
    "                ##### When there is an annotaion from all DBs #####\n",
    "                #all entries with a high scoring annotation in all DBs,\n",
    "                if not isNaN(combined['GNPSspectrumID'][i]) and not isNaN(combined['MBspectrumID'][i]) and not isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    # entries with same candidate from all Spectral DBs\n",
    "                    if combined['tanimotoHG'][i] == 1.0 and combined['tanimotoGM'][i] == 1.0 and combined['tanimotoHM'][i] == 1.0:\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS, HMDB, MassBank'\n",
    "                \n",
    "                    # same candidate from GNPS and HMDB        \n",
    "                    if combined['tanimotoHG'][i] == 1.0 and combined['tanimotoGM'][i] != 1.0 and combined['tanimotoHM'][i] != 1.0:\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS, HMDB'\n",
    "        \n",
    "                    # same candidate from GNPS and MassBank        \n",
    "                    if combined['tanimotoHG'][i] != 1.0 and combined['tanimotoGM'][i] == 1.0 and combined['tanimotoHM'][i] != 1.0:\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS, MassBank'\n",
    "                \n",
    "                    # same candidate from MassBank and HMDB        \n",
    "                    if combined['tanimotoHG'][i] != 1.0 and combined['tanimotoGM'][i] != 1.0 and combined['tanimotoHM'][i] == 1.0:\n",
    "                        combined.loc[i, 'Annotation'] = 'MassBank, HMDB'\n",
    "                \n",
    "                    # all different annotations, take GNPS\n",
    "                    else:\n",
    "                        if not isNaN(combined['GNPSSMILES'][i]):\n",
    "                            combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "                        else:\n",
    "                            combined.loc[i, 'Annotation'] = 'MassBank'\n",
    "                ##### When there is an annotation from two DBs #####\n",
    "    \n",
    "    \n",
    "                # only GNPS and HMDB\n",
    "                if not isNaN(combined['GNPSspectrumID'][i]) and isNaN(combined['MBspectrumID'][i]) and not isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    if combined['tanimotoHG'][i] == 1.0:\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS, HMDB'\n",
    "                    else:\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "                    \n",
    "                    \n",
    "                # only GNPS and MassBank\n",
    "                if not isNaN(combined['GNPSspectrumID'][i]) and not isNaN(combined['MBspectrumID'][i]) and isNaN(combined['HMDBcompoundID'][i]):\n",
    "\n",
    "                    if combined['tanimotoGM'][i] == 1.0:\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS, MassBank'\n",
    "                    else:\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "    \n",
    "                # only MassBank and HMDB\n",
    "                if isNaN(combined['GNPSspectrumID'][i]) and not isNaN(combined['MBspectrumID'][i]) and not isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    if combined['tanimotoHM'][i] == 1.0:\n",
    "                        combined.loc[i, 'Annotation'] = 'HMDB, MassBank'\n",
    "                    else:\n",
    "                        combined.loc[i, 'Annotation'] = 'MassBank'\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                ##### When there is an annotation from one DBs #####\n",
    "    \n",
    "    \n",
    "                # only GNPS\n",
    "                if not isNaN(combined['GNPSspectrumID'][i]) and isNaN(combined['MBspectrumID'][i]) and isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    if not isNaN(combined['GNPSSMILES'][i]):\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "        \n",
    "                # only MassBank\n",
    "                if isNaN(combined['GNPSspectrumID'][i]) and not isNaN(combined['MBspectrumID'][i]) and isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    combined.loc[i, 'Annotation'] = 'MassBank'\n",
    "    \n",
    "                # only HMDB\n",
    "                    #if isNaN(combined['GNPSspectrumID'][i]) and isNaN(combined['MBspectrumID'][i]) and not isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    #combined.loc[i, 'Annotation'] = 'HMDB'\n",
    "                \n",
    "            \n",
    "            #if GNPS and MassBank databases were used\n",
    "            if db == \"gm\":\n",
    "                # only GNPS and MassBank\n",
    "                if not isNaN(combined['GNPSspectrumID'][i]) and not isNaN(combined['MBspectrumID'][i]):\n",
    "                    if combined['tanimotoGM'][i] == 1.0:\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS, MassBank'\n",
    "                    else:\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "                    \n",
    "                \n",
    "                ##### When there is an annotation from one DBs #####\n",
    "                # only GNPS\n",
    "                if not isNaN(combined['GNPSspectrumID'][i]) and isNaN(combined['MBspectrumID'][i]):\n",
    "                    if not isNaN(combined['GNPSSMILES'][i]):\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "        \n",
    "                # only MassBank\n",
    "                if isNaN(combined['GNPSspectrumID'][i]) and not isNaN(combined['MBspectrumID'][i]):\n",
    "                    combined.loc[i, 'Annotation'] = 'MassBank'\n",
    "                    \n",
    "                    \n",
    "            # only GNPS and HMDB   \n",
    "            if db == \"hg\":\n",
    "                ##### When there is an annotation from two DBs #####\n",
    "    \n",
    "    \n",
    "                # only GNPS and HMDB\n",
    "                if not isNaN(combined['GNPSspectrumID'][i]) and not isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    if combined['tanimotoHG'][i] == 1.0:\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS, HMDB'\n",
    "                    else:\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "                \n",
    "                \n",
    "                ##### When there is an annotation from one DBs #####\n",
    "    \n",
    "                # only GNPS\n",
    "                if not isNaN(combined['GNPSspectrumID'][i]) and isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    if not isNaN(combined['GNPSSMILES'][i]):\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "                # only HMDB\n",
    "                    #if isNaN(combined['GNPSspectrumID'][i]) and not isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    #combined.loc[i, 'Annotation'] = 'HMDB'\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "            # only MassBank and HMDB        \n",
    "            if db == \"hm\":\n",
    "                # only MassBank and HMDB\n",
    "                if not isNaN(combined['MBspectrumID'][i]) and not isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    if combined['tanimotoHM'][i] == 1.0:\n",
    "                        combined.loc[i, 'Annotation'] = 'HMDB, MassBank'\n",
    "                    else:\n",
    "                        combined.loc[i, 'Annotation'] = 'MassBank'\n",
    "                \n",
    "                \n",
    "                \n",
    "                # only MassBank\n",
    "                if not isNaN(combined['MBspectrumID'][i]) and isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    combined.loc[i, 'Annotation'] = 'MassBank'\n",
    "    \n",
    "                # only HMDB\n",
    "                    #if isNaN(combined['MBspectrumID'][i]) and not isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    #combined.loc[i, 'Annotation'] = 'HMDB'\n",
    "                \n",
    "            if db == \"gnps\":\n",
    "                # only GNPS\n",
    "                if not isNaN(combined['GNPSspectrumID'][i]):\n",
    "                    if not isNaN(combined['GNPSSMILES'][i]):\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "            if db == \"mbank\":\n",
    "                # only MassBank\n",
    "                if not isNaN(combined['MBspectrumID'][i]):\n",
    "                    combined.loc[i, 'Annotation'] = 'MassBank'\n",
    "            #if db == \"hmdb\":\n",
    "                # only HMDB\n",
    "                #if not isNaN(combined['HMDBcompoundID'][i]):\n",
    "                    #combined.loc[i, 'Annotation'] = 'HMDB'\n",
    "    combined.to_csv(input_dir + \"MetabolomicsResults/curatedSDB.csv\")\n",
    "    return(combined)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "d5d9adc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(specDB_Curation.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "576e9f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specDB_Curation(input_dir = \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/\", \n",
    " #               combinedx = \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/MetabolomicsResults/SpecDBvsSL.csv\",\n",
    "   #             sl = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "2ebd2016",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specDB_Curation(input_dir = \"/Users/mahnoorzulfiqar/Downloads/MAW-main/\",\n",
    " #               combinedx = \"/Users/mahnoorzulfiqar/Downloads/MAW-main/MetabolomicsResults/SpecDBvsSL.csv\",\n",
    " #               sl = True,\n",
    "   #            db = \"gm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd0d1cb",
   "metadata": {},
   "source": [
    "# combine curated SDB and CDB (S+M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "900ba52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_CuratedR(input_dir, combinedSDBs, combinedSMs, data_type = \"standards\"):\n",
    "    \n",
    "    \"\"\"combine_CuratedR prioritizes in the following manner: gnps>\n",
    "    mbank>suspectlist>sirius>hmdb>metfrag\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "    \n",
    "    curatedSDB: df from specDB_Curation\n",
    "    combinedSM: df from combineSM\n",
    "    \n",
    "    Returns:\n",
    "    dataframe: with curated Spectral DB results and CDB (S+M) results\n",
    "    csv: \"MetabolomicsResults/final_curation_without_classes.csv\"\n",
    "    \n",
    "    Usage:\n",
    "    combine_CuratedR(input_dir = \"usr/project/\", curatedSDB, combinedSM)\n",
    "\n",
    "    \"\"\"\n",
    "    def isNaN(string):\n",
    "        return string != string\n",
    "\n",
    "    combinedSDB = pd.read_csv(combinedSDBs)\n",
    "    combinedSM = pd.read_csv(combinedSMs)\n",
    "    mega = pd.concat([combinedSM, combinedSDB], axis = 1, levels = [\"id_X\"])\n",
    "    \n",
    "    for i, row in mega.iterrows():\n",
    "    \n",
    "        #if only compound database results\n",
    "        if isNaN(mega['Annotation'][i]) and not isNaN(mega['Annotation_C'][i]):\n",
    "            mega.loc[i, \"Annotation_Source\"] = mega['Annotation_C'][i]\n",
    "        \n",
    "        # if only spectral db results\n",
    "        if not isNaN(mega['Annotation'][i]) and isNaN(mega['Annotation_C'][i]):\n",
    "            mega.loc[i, \"Annotation_Source\"] = mega['Annotation'][i]\n",
    "            \n",
    "        # if both have results\n",
    "        if not isNaN(mega['Annotation'][i]) and not isNaN(mega['Annotation_C'][i]):\n",
    "            ########THREE OR FOUR SDB SOURCES########\n",
    "        \n",
    "            #if three sdb sources or more\n",
    "            # prioritize Spectral DBs\n",
    "            if len(mega['Annotation'][i].split()) >= 3 and 'SIRIUS' in mega['Annotation_C'][i]:\n",
    "                if 'MassBank' in mega['Annotation'][i]:\n",
    "                    SKms = [Chem.MolFromSmiles(mega['MBSMILES'][i]), Chem.MolFromSmiles(mega['SMILES'][i])]\n",
    "                    SKfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in SKms]\n",
    "                    SKtn = DataStructs.FingerprintSimilarity(SKfps[0],SKfps[1])\n",
    "                    if SKtn == 1.0:\n",
    "                        print(SKtn)\n",
    "                        mega.loc[i, \"Annotation_Source\"] = mega['Annotation'][i] + ', ' + mega['Annotation_C'][i]\n",
    "                    else:\n",
    "                        mega.loc[i, \"Annotation_Source\"] = mega['Annotation'][i]\n",
    "            \n",
    "                elif 'HMDB' in mega['Annotation'][i]:\n",
    "                    SKms = [Chem.MolFromSmiles(mega['HMDBSMILES'][i]), Chem.MolFromSmiles(mega['SMILES'][i])]\n",
    "                    SKfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in SKms]\n",
    "                    SKtn = DataStructs.FingerprintSimilarity(SKfps[0],SKfps[1])\n",
    "                    if SKtn == 1.0:\n",
    "                        mega.loc[i, \"Annotation_Source\"] = mega['Annotation'][i] + ', ' + mega['Annotation_C'][i]\n",
    "                    else:\n",
    "                        mega.loc[i, \"Annotation_Source\"] = mega['Annotation'][i]\n",
    "            elif len(mega['Annotation'][i].split()) >= 3 and 'SIRIUS' not in mega['Annotation_C'][i]:\n",
    "                if 'KEGG' in mega['Annotation_C'][i]:\n",
    "                    if 'MassBank' in mega['Annotation'][i]:\n",
    "                        SKms = [Chem.MolFromSmiles(mega['MBSMILES'][i]), Chem.MolFromSmiles(mega['KG_SMILES'][i])]\n",
    "                        SKfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in SKms]\n",
    "                        SKtn = DataStructs.FingerprintSimilarity(SKfps[0],SKfps[1])\n",
    "                        if SKtn == 1.0:\n",
    "                            mega.loc[i, \"Annotation_Source\"] = mega['Annotation'][i] + ', ' + mega['Annotation_C'][i]\n",
    "                        else:\n",
    "                            mega.loc[i, \"Annotation_Source\"] = mega['Annotation'][i]\n",
    "            \n",
    "                    elif 'HMDB' in mega['Annotation'][i]:\n",
    "                        SKms = [Chem.MolFromSmiles(mega['HMDBSMILES'][i]), Chem.MolFromSmiles(mega['KG_SMILES'][i])]\n",
    "                        SKfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in SKms]\n",
    "                        SKtn = DataStructs.FingerprintSimilarity(SKfps[0],SKfps[1])\n",
    "                        if SKtn == 1.0:\n",
    "                            mega.loc[i, \"Annotation_Source\"] = mega['Annotation'][i] + ', ' + mega['Annotation_C'][i]\n",
    "                        else:\n",
    "                            mega.loc[i, \"Annotation_Source\"] = mega['Annotation'][i]\n",
    "                else:\n",
    "                    mega.loc[i, \"Annotation_Source\"] = mega['Annotation'][i]\n",
    "            \n",
    "            \n",
    "            \n",
    "            #######TWO OR ONE SDB SOURCE#########\n",
    "                \n",
    "            #if both 2 SDBs and results from insilico tools\n",
    "            elif len(mega['Annotation'][i].split()) <= 2:\n",
    "                mega.loc[i, \"Annotation_Source\"] = mega['Annotation_C'][i]\n",
    "                \n",
    "                \n",
    "        # if no results from any databases\n",
    "        if isNaN(mega['Annotation'][i]) and isNaN(mega['Annotation_C'][i]) and not isNaN(mega['Formula'][i]):\n",
    "            mega.loc[i, \"Annotation_Source\"] = 'SIRIUS_Formula'\n",
    "        \n",
    "    bef_mega = mega.loc[:,~mega.columns.duplicated()]\n",
    "    for i, row in bef_mega.iterrows():\n",
    "        if not isNaN(bef_mega['Annotation_Source'][i]):\n",
    "            # check if SIRIUS is in the annotation source but keep in mind it shouldnt be SIRIUS_Formula\n",
    "            if 'SIRIUS' in bef_mega['Annotation_Source'][i] and 'SIRIUS_Formula' not in bef_mega['Annotation_Source'][i]:\n",
    "                bef_mega.loc[i, 'SMILES_final'] = bef_mega['SMILES'][i]\n",
    "                bef_mega.loc[i,\"CompoundNames\"] = bef_mega['name'][i]\n",
    "                bef_mega['PC_MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['KG_MCSS_SMILES'][i] = np.nan\n",
    "            elif 'KEGG' in bef_mega['Annotation_Source'][i]:\n",
    "                bef_mega.loc[i, 'SMILES_final'] = bef_mega['KG_SMILES'][i]\n",
    "                bef_mega.loc[i, 'CompoundNames'] = bef_mega['KG_Name'][i]\n",
    "                #bef_mega['most_specific_class'][i] = np.nan\n",
    "                #bef_mega['level _5'][i] = np.nan\n",
    "                bef_mega['subclass'][i] = np.nan\n",
    "                bef_mega['class'][i] = np.nan\n",
    "                bef_mega['superclass'][i] = np.nan\n",
    "                #bef_mega['all_classifications'][i] = np.nan\n",
    "                bef_mega['Classification_Source'][i] = np.nan\n",
    "                bef_mega['MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['PC_MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['Formula'][i] = np.nan\n",
    "            \n",
    "            elif 'GNPS, SuspectList' in bef_mega['Annotation_Source'][i]:\n",
    "                bef_mega.loc[i,'SMILES_final'] = bef_mega['GLsmiles'][i]\n",
    "                bef_mega.loc[i, 'CompoundNames'] = bef_mega['GLname'][i]\n",
    "                bef_mega.loc[i, 'CompoundNames']\n",
    "                #bef_mega['most_specific_class'][i] = np.nan\n",
    "                #bef_mega['level _5'][i] = np.nan\n",
    "                bef_mega['subclass'][i] = np.nan\n",
    "                bef_mega['class'][i] = np.nan\n",
    "                bef_mega['superclass'][i] = np.nan\n",
    "                #bef_mega['all_classifications'][i] = np.nan\n",
    "                bef_mega['MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['Classification_Source'][i] = np.nan\n",
    "                bef_mega['PC_MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['KG_MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['Formula'][i] = np.nan\n",
    "        \n",
    "            elif 'GNPS' in bef_mega['Annotation_Source'][i]:\n",
    "                bef_mega.loc[i,'SMILES_final'] = bef_mega['GNPSSMILES'][i]\n",
    "                bef_mega.loc[i, 'CompoundNames'] = bef_mega['GNPScompound_name'][i]\n",
    "                #bef_mega['most_specific_class'][i] = np.nan\n",
    "                #bef_mega['level _5'][i] = np.nan\n",
    "                bef_mega['subclass'][i] = np.nan\n",
    "                bef_mega['class'][i] = np.nan\n",
    "                bef_mega['superclass'][i] = np.nan\n",
    "                #bef_mega['all_classifications'][i] = np.nan\n",
    "                bef_mega['MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['Classification_Source'][i] = np.nan\n",
    "                bef_mega['PC_MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['KG_MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['Formula'][i] = np.nan\n",
    "            elif 'MassBank' in bef_mega['Annotation_Source'][i]:\n",
    "                bef_mega.loc[i, 'SMILES_final'] = bef_mega['MBSMILES'][i]\n",
    "                bef_mega.loc[i, 'CompoundNames'] = bef_mega['MBcompound_name'][i]\n",
    "                #bef_mega['most_specific_class'][i] = np.nan\n",
    "                #bef_mega['level _5'][i] = np.nan\n",
    "                bef_mega['subclass'][i] = np.nan\n",
    "                bef_mega['class'][i] = np.nan\n",
    "                bef_mega['superclass'][i] = np.nan\n",
    "                #bef_mega['all_classifications'][i] = np.nan\n",
    "                bef_mega['Classification_Source'][i] = np.nan\n",
    "                bef_mega['MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['PC_MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['KG_MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['Formula'][i] = np.nan\n",
    "                \n",
    "                \n",
    "            elif 'PubChem' in bef_mega['Annotation_Source'][i]:\n",
    "                bef_mega.loc[i, 'SMILES_final'] = bef_mega['PC_SMILES'][i]\n",
    "                bef_mega.loc[i, 'CompoundNames'] = bef_mega['PC_Name'][i]\n",
    "                #bef_mega['most_specific_class'][i] = np.nan\n",
    "                #bef_mega['level _5'][i] = np.nan\n",
    "                bef_mega['subclass'][i] = np.nan\n",
    "                bef_mega['class'][i] = np.nan\n",
    "                bef_mega['superclass'][i] = np.nan\n",
    "                #bef_mega['all_classifications'][i] = np.nan\n",
    "                bef_mega['Classification_Source'][i] = np.nan\n",
    "                bef_mega['MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['KG_MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['Formula'][i] = np.nan\n",
    "            \n",
    "            elif 'HMDB' in bef_mega['Annotation_Source'][i]:\n",
    "                bef_mega.loc[i, 'SMILES_final'] = bef_mega['HMDBSMILES'][i]\n",
    "                bef_mega.loc[i, 'CompoundNames'] = bef_mega['HMDBcompound_name'][i]\n",
    "                #bef_mega['most_specific_class'][i] = np.nan\n",
    "                #bef_mega['level _5'][i] = np.nan\n",
    "                bef_mega['subclass'][i] = np.nan\n",
    "                bef_mega['class'][i] = np.nan\n",
    "                bef_mega['superclass'][i] = np.nan\n",
    "                #bef_mega['all_classifications'][i] = np.nan\n",
    "                bef_mega['Classification_Source'][i] = np.nan\n",
    "                bef_mega['MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['PC_MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['KG_MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['Formula'][i] = np.nan\n",
    "                \n",
    "                \n",
    "            elif 'SIRIUS_Formula' in bef_mega['Annotation_Source'][i]:\n",
    "                bef_mega['PC_MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['KG_MCSS_SMILES'][i] = np.nan\n",
    "                \n",
    "                \n",
    "    bef_megaA = bef_mega[['id_X', \n",
    "                          'premz', \n",
    "                          'rtmed', \n",
    "                          'rtmean',\n",
    "                          'int', \n",
    "                          'col_eng', \n",
    "                          'pol', \n",
    "                          'SMILES_final', \n",
    "                          'CompoundNames', \n",
    "                          'MCSS_SMILES', \n",
    "                          'PC_MCSS_SMILES', \n",
    "                          'KG_MCSS_SMILES', \n",
    "                          'subclass', \n",
    "                          'class', \n",
    "                          'superclass', \n",
    "                          'Classification_Source', \n",
    "                          'Annotation_Source'\n",
    "                         ]]\n",
    "            \n",
    "    bef_megaA.rename(columns = {'SMILES_final':'SMILES'}, inplace = True)\n",
    "    \n",
    "    \n",
    "    Standards = ['Experimental']\n",
    "    SpectralDB = ['GNPS', 'HMDB', 'MassBank']\n",
    "    CompoundDB = ['SuspectList', 'SIRIUS', 'KEGG', 'PubChem']\n",
    "    Formula = ['SIRIUS_Formula']\n",
    "\n",
    "    \n",
    "    #bef_megaA['MSI_Level'] = np.nan\n",
    "    for i, rows in bef_megaA.iterrows():\n",
    "        \n",
    "        \n",
    "        if not isNaN(bef_megaA['Annotation_Source'][i]):\n",
    "            \n",
    "            if data_type == \"standards\":\n",
    "                bef_megaA.loc[i, 'Annotation_Source'] = bef_megaA['Annotation_Source'][i] + ', Experimental'\n",
    "\n",
    "                if any(x in bef_megaA['Annotation_Source'][i] for x in SpectralDB):\n",
    "                    bef_megaA.loc[i, 'MSI_Level'] = 'Level_1'\n",
    "                    \n",
    "                elif any(x in bef_megaA['Annotation_Source'][i] for x in CompoundDB) and not any(x in bef_megaA['Annotation_Source'][i] for x in Formula):\n",
    "                    bef_megaA.loc[i, 'MSI_Level'] = 'Level_2/Level_3'\n",
    "                    \n",
    "                elif any(x in bef_megaA['Annotation_Source'][i] for x in Formula):\n",
    "                    bef_megaA.loc[i, 'MSI_Level'] = 'Level_4'\n",
    "                    \n",
    "            else:\n",
    "\n",
    "                if any(x in bef_megaA['Annotation_Source'][i] for x in SpectralDB):\n",
    "                    bef_megaA.loc[i, 'MSI_Level'] = 'Level_2'\n",
    "                    \n",
    "                elif any(x in bef_megaA['Annotation_Source'][i] for x in CompoundDB) and not any(x in bef_megaA['Annotation_Source'][i] for x in Formula):\n",
    "                    bef_megaA.loc[i, 'MSI_Level'] = 'Level_3'\n",
    "                    \n",
    "                elif any(x in bef_megaA['Annotation_Source'][i] for x in Formula):\n",
    "                    bef_megaA.loc[i, 'MSI_Level'] = 'Level_4'\n",
    "                \n",
    "        else:\n",
    "            bef_megaA.loc[i, 'MSI_Level'] = 'Level_5'\n",
    "            \n",
    "                \n",
    "    \n",
    "            \n",
    "    bef_megaA.to_csv(input_dir + \"MetabolomicsResults/final_curation_without_classes.csv\")\n",
    "    return(bef_megaA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6e1e4ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(combine_CuratedR.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "95d4c26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine_CuratedR(input_dir = \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/\", \n",
    "                 #combinedSDBs = \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/MetabolomicsResults/curatedSDB.csv\", \n",
    "                 #combinedSMs = \"/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/MetabolomicsResults/combinedSM.csv\",\n",
    "                #data_type = \"standards\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "9aa52d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine_CuratedR(input_dir = \"/Users/mahnoorzulfiqar/Downloads/MAW-main/\", \n",
    "                 #combinedSDBs = \"/Users/mahnoorzulfiqar/Downloads/MAW-main/MetabolomicsResults/curatedSDB.csv\", \n",
    "                 #combinedSMs =\"/Users/mahnoorzulfiqar/Downloads/MAW-main/MetabolomicsResults/combinedSM.csv\",\n",
    "                #data_type = \"standards\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "82f0a82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkSMILES_validity(input_dir, resultcsv):\n",
    "    def isNaN(string):\n",
    "        return string != string\n",
    "    \"\"\"checkSMILES_validity does exactly as the name says, using\n",
    "    RDKit, whether the SMILES are invalid or have invalid \n",
    "    chemistry\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "    \n",
    "    results: df from combine_CuratedR\n",
    "    \n",
    "    Returns:\n",
    "    dataframe: with valid SMILES\n",
    "    csv: \"MetabolomicsResults/final_curation_with_validSMILES.csv\"\n",
    "    \n",
    "    Usage:\n",
    "    checkSMILES_validity(input_dir = \"usr/project/\", results)\n",
    "\n",
    "    \"\"\"\n",
    "    results = pd.read_csv(resultcsv)\n",
    "    # check validity of SMILES\n",
    "    for i, row in results.iterrows():\n",
    "        if not isNaN(results['SMILES'][i]):\n",
    "            m = Chem.MolFromSmiles(results['SMILES'][i] ,sanitize=False)\n",
    "            if m is None:\n",
    "                results['SMILES_final'][i] = 'invalid_SMILES'\n",
    "            else:\n",
    "                try:\n",
    "                    Chem.SanitizeMol(m)\n",
    "                except:\n",
    "                    results['SMILES_final'][i] = 'invalid_chemistry'\n",
    "    results.to_csv(input_dir + \"MetabolomicsResults/final_curation_with_validSMILES.csv\")\n",
    "    return(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "76c18e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(checkSMILES_validity.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ec74f9c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id_X</th>\n",
       "      <th>premz</th>\n",
       "      <th>rtmed</th>\n",
       "      <th>rtmean</th>\n",
       "      <th>int</th>\n",
       "      <th>col_eng</th>\n",
       "      <th>pol</th>\n",
       "      <th>SMILES</th>\n",
       "      <th>CompoundNames</th>\n",
       "      <th>MCSS_SMILES</th>\n",
       "      <th>PC_MCSS_SMILES</th>\n",
       "      <th>KG_MCSS_SMILES</th>\n",
       "      <th>subclass</th>\n",
       "      <th>class</th>\n",
       "      <th>superclass</th>\n",
       "      <th>Classification_Source</th>\n",
       "      <th>Annotation_Source</th>\n",
       "      <th>MSI_Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>file_8M130R129ID1</td>\n",
       "      <td>130.086105</td>\n",
       "      <td>129.480048</td>\n",
       "      <td>129.576581</td>\n",
       "      <td>2505025024</td>\n",
       "      <td>30</td>\n",
       "      <td>pos</td>\n",
       "      <td>C1CCNC(C1)C(=O)O</td>\n",
       "      <td>l-homopro</td>\n",
       "      <td>CCCC(N)C(=O)O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Amino acids, peptides, and analogues</td>\n",
       "      <td>Carboxylic acids and derivatives</td>\n",
       "      <td>Organic acids and derivatives</td>\n",
       "      <td>CANOPUS</td>\n",
       "      <td>SIRIUS, Experimental</td>\n",
       "      <td>Level_2/Level_3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0               id_X       premz       rtmed      rtmean  \\\n",
       "0           0  file_8M130R129ID1  130.086105  129.480048  129.576581   \n",
       "\n",
       "          int  col_eng  pol            SMILES CompoundNames    MCSS_SMILES  \\\n",
       "0  2505025024       30  pos  C1CCNC(C1)C(=O)O     l-homopro  CCCC(N)C(=O)O   \n",
       "\n",
       "   PC_MCSS_SMILES  KG_MCSS_SMILES                              subclass  \\\n",
       "0             NaN             NaN  Amino acids, peptides, and analogues   \n",
       "\n",
       "                              class                     superclass  \\\n",
       "0  Carboxylic acids and derivatives  Organic acids and derivatives   \n",
       "\n",
       "  Classification_Source     Annotation_Source        MSI_Level  \n",
       "0               CANOPUS  SIRIUS, Experimental  Level_2/Level_3  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checkSMILES_validity(input_dir = '/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/',\n",
    "  #                   resultcsv = '/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/MetabolomicsResults/final_curation_without_classes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "b11606a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkSMILES_validity(input_dir = '/Users/mahnoorzulfiqar/Downloads/MAW-main/', \n",
    "      #               resultcsv = '/Users/mahnoorzulfiqar/Downloads/MAW-main/MetabolomicsResults/final_curation_without_classes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbace30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "6e99e6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(input_dir, resultcsv):\n",
    "    def isNaN(string):\n",
    "        return string != string\n",
    "    \"\"\"classification function uses ClassyFire ChemONT\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "    \n",
    "    resultcsv: csv of df from combine_CuratedR or checkSMILES_validity\n",
    "    \n",
    "    Returns:\n",
    "    dataframe: with classification\n",
    "    csv: \"MetabolomicsResults/final_curationList.csv\"\n",
    "    \n",
    "    Usage:\n",
    "    checkSMILES_validity(input_dir = \"usr/project/\", frame)\n",
    "\n",
    "    \"\"\"\n",
    "    frame = pd.read_csv(resultcsv)\n",
    "    inchis = []\n",
    "    for i, row in frame.iterrows():\n",
    "        if not isNaN(frame['SMILES'][i]) and isNaN(frame['Classification_Source'][i]):\n",
    "            try:\n",
    "                InChI = Chem.MolToInchi(Chem.MolFromSmiles(frame[\"SMILES\"][i]))\n",
    "                InChIKey = Chem.inchi.InchiToInchiKey(InChI)\n",
    "                inchis.append({\n",
    "                    'index': i,\n",
    "                    'smiles':frame[\"SMILES\"][i],\n",
    "                    'inchi': InChI,\n",
    "                    'inchikey': InChIKey\n",
    "                })\n",
    "            except:\n",
    "                pass\n",
    "    inchis = pd.DataFrame(inchis)\n",
    "    if len(inchis):\n",
    "        inchis = inchis.loc[-isNaN(inchis['inchikey'])]\n",
    "        ## Retrieve ClassyFire classifications ##\n",
    "\n",
    "        # This first step is done using inchikey and interrogation of the gnps classified structures\n",
    "        gnps_proxy = True \n",
    "        url = \"http://classyfire.wishartlab.com\"\n",
    "        proxy_url =  \"https://gnps-classyfire.ucsd.edu\"\n",
    "        chunk_size = 1000\n",
    "        sleep_interval = 12\n",
    "\n",
    "        all_inchi_keys = list(inchis['inchikey'].drop_duplicates())\n",
    "\n",
    "        resolved_ik_number_list = [0, 0]\n",
    "        total_inchikey_number = len(all_inchi_keys)\n",
    "\n",
    "        while True:\n",
    "\n",
    "            #start_time = time.time()\n",
    "\n",
    "            #print('%s inchikey to resolve' % total_inchikey_number )\n",
    "            get_classifications_cf_mod(all_inchi_keys, par_level = 6)\n",
    "\n",
    "            cleanse('all_json.json', 'all_json.json')\n",
    "\n",
    "            with open(\"all_json.json\") as tweetfile:\n",
    "                jsondic = json.loads(tweetfile.read())\n",
    "\n",
    "            df = json_normalize(jsondic)\n",
    "            df = df.drop_duplicates( 'inchikey' )\n",
    "            resolved_ik_number = len( df.drop_duplicates('inchikey').inchikey )\n",
    "            resolved_ik_number_list.append( resolved_ik_number )\n",
    "            #print('%s resolved inchikeys' % resolved_ik_number )\n",
    "            #print(\"done in --- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "            if resolved_ik_number_list[-1] < resolved_ik_number_list[-2] or resolved_ik_number_list[-1] == resolved_ik_number_list[-3]:\n",
    "                break\n",
    "            cleanse('all_json.json', 'all_json_cleaned.json')\n",
    "\n",
    "            with open(\"all_json_cleaned.json\") as tweetfile:\n",
    "                jsondic = json.loads(tweetfile.read())\n",
    "\n",
    "        flattened_classified_json = json_normalize(jsondic)\n",
    "        flattened_df = flattened_classified_json.drop_duplicates('inchikey')\n",
    "        flattened_df['inchikey'] = flattened_df['inchikey'].str.replace(r'InChIKey=', '')\n",
    "        df_merged = pd.merge(inchis, flattened_df, left_on='inchikey', right_on='inchikey', how='left')\n",
    "\n",
    "        for p, rowp in df_merged.iterrows():\n",
    "            for q, rowq in frame.iterrows():\n",
    "                if df_merged[\"smiles_x\"][p] is frame[\"SMILES\"][q]:\n",
    "                    frame.loc[q, 'subclass'] = df_merged[\"subclass.name\"][p]\n",
    "                    frame.loc[q, 'class'] = df_merged[\"class.name\"][p]\n",
    "                    frame.loc[q, 'superclass'] = df_merged[\"superclass.name\"][p]\n",
    "                    frame.loc[q, 'Classification_Source'] = \"ClassyFire\"\n",
    "\n",
    "\n",
    "\n",
    "        frame.to_csv(input_dir + \"MetabolomicsResults/final_curationList.csv\")\n",
    "        return(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d0561a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(classification.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc515e92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4939d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "882d4477",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#classification(input_dir = '/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/',\n",
    "              # resultcsv = '/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/MetabolomicsResults/final_curation_with_validSMILES.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "60f80bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification(input_dir = '/Users/mahnoorzulfiqar/Downloads/MAW-main/',\n",
    "               #resultcsv = '/Users/mahnoorzulfiqar/Downloads/MAW-main/MetabolomicsResults/final_curation_with_validSMILES.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5e1ccf",
   "metadata": {},
   "source": [
    "# Comparison with a list of SMILES from any Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ba5db8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cd = pd.read_csv('/Users/mahnoorzulfiqar/OneDriveUNI/MZML/CD/CD_Results.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "dc19f8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def create_SMILES_list(input_dir, compcsv)\n",
    "    #CDCSV = list(cd[-isNaN(cd['SMILES'])]['SMILES'])\n",
    "    #f = open(\"/Users/mahnoorzulfiqar/OneDriveUNI/MZML/CD/CDCSV.txt\", \"w\")\n",
    "    #for item in CDCSV:\n",
    "        #f.write(item + \"\\n\")\n",
    "    #f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "77bdce92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMILESscreening(input_dir, resultcsv, complist, listname):\n",
    "    def isNaN(string):\n",
    "        return string != string\n",
    "    \"\"\"SMILESscreening takes a list of SMILES\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "    \n",
    "    resultcsv: df from combine_CuratedR or checkSMILES_validity or classification\n",
    "    complist: list of /n separated txt file conyaining smiles on each line\n",
    "    listname: name of the list of compounds\n",
    "    \n",
    "    Returns:\n",
    "    dataframe: comparison with another list of compounds\n",
    "    csv: \"MetabolomicsResults/final_curation_with_validSMILES.csv\"\n",
    "    \n",
    "    Usage:\n",
    "    checkSMILES_validity(input_dir = \"usr/project/\", results)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    results = pd.read_csv(resultcsv)\n",
    "    with open(complist, \"r\") as text_file:\n",
    "        cd = text_file.read().split('\\n')\n",
    "    \n",
    "    for i, row in results.iterrows():\n",
    "        if not isNaN(results['SMILES'][i]):\n",
    "            if 'invalid_SMILES' not in results['SMILES'][i] and 'invalid_chemistry' not in results['SMILES'][i]:\n",
    "                for j in cd:\n",
    "                    if not isNaN(j):\n",
    "                        CGms = [Chem.MolFromSmiles(results['SMILES'][i]), Chem.MolFromSmiles(j)]\n",
    "                        CGfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=1024) for x in CGms]\n",
    "                        CGtn = DataStructs.FingerprintSimilarity(CGfps[0],CGfps[1])\n",
    "                        if CGtn == 1 and listname not in results['Annotation_Source'][i]:\n",
    "                            results['Annotation_Source'][i] = results['Annotation_Source'][i] + ', ' + listname\n",
    "    \n",
    "\n",
    "    frame.to_csv(input_dir + \"MetabolomicsResults/final_curationListVS\"+listname+\".csv\")\n",
    "    return(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "82049e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(SMILESscreening.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968d2ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "4ba72546",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMILESscreening(input_dir = '/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/',\n",
    "                #resultcsv = '/Users/mahnoorzulfiqar/OneDriveUNI/CheckDocker/MetabolomicsResults/final_curation_with_validSMILES.csv',\n",
    "                #complist = '/Users/mahnoorzulfiqar/OneDriveUNI/MZML/CD/CDCSV.txt',\n",
    "                #listname = 'CompoundDiscoverer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "0d1e4041",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMILESscreening(input_dir = '/Users/mahnoorzulfiqar/Downloads/MAW-main/', \n",
    "                #resultcsv = '/Users/mahnoorzulfiqar/Downloads/MAW-main/MetabolomicsResults/final_curationList.csv', \n",
    "                #complist = '/Users/mahnoorzulfiqar/OneDriveUNI/MZML/CD/CDCSV.txt', \n",
    "                #listname = 'CompoundDiscoverer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9fa40f",
   "metadata": {},
   "source": [
    "## Molecular Networking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b50f033",
   "metadata": {},
   "source": [
    "## MN with GNPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fede0d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mawRpy)",
   "language": "python",
   "name": "mawrpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
