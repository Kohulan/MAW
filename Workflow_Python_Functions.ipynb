{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e20dd5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "\n",
    "#print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "611cc70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import pubchempy as pcp\n",
    "import numpy as np\n",
    "def isNaN(string):\n",
    "    return string != string\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "from pybatchclassyfire import *\n",
    "import csv \n",
    "import time\n",
    "import json\n",
    "from pandas import json_normalize\n",
    "import wget\n",
    "import string\n",
    "import urllib.parse\n",
    "import openpyxl\n",
    "import statistics\n",
    "import sys\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42327aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:rdkit:Enabling RDKit 2021.09.4 jupyter extensions\n"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdFMCS\n",
    "from rdkit.Chem import PandasTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd90da13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/mahnoorzulfiqar/OneDriveUNI/MAW-data/StandardSMarinoi_Data'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dir = \"/Users/mahnoorzulfiqar/OneDriveUNI/MAW-data/StandardSMarinoi_Data\"\n",
    "input_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1815cd",
   "metadata": {},
   "source": [
    "# Suspect List for SIRIUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cd169d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slist_sirius(input_dir, slist_csv, substring = None):\n",
    "    \n",
    "    \"\"\"slist_sirius is used to create a tsv file that contains a list of \n",
    "    SMILES. The function also runs the sirius command custom db to create\n",
    "    fingerprints for each SMILES in a folder that we by default name as\n",
    "    SL_Frag/. This fingerprints folder is later used by SIRIUS to use \n",
    "    these compounds as a another small list of compounds to match against\n",
    "    the input spectra fingerprints.\n",
    "    Since SIRIUS doesn't take disconnected structure, Multiply charged, \n",
    "    Incorrect syntax, wild card(*) in smiles; this function removes all\n",
    "    such SMILES from the Suspect List.\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored. For this \n",
    "    function this directory must contain a csv file that has a column \n",
    "    named \"SMILES\".\n",
    "    \n",
    "    slist_csv (str): This is the csv file that contains a column of \n",
    "    \"SMILES\". Additionally this file can contain other information \n",
    "    about the compounds, but for this function, column of \"SMILES\", \n",
    "    named as \"SMILES\" is necessary.\n",
    "    \n",
    "    substring (list): provide a list of strings of SMILES that \n",
    "    shouldn't be considered, provide a list even if there is one string\n",
    "    that shouldnt be considered. e.g: \"[Fe+2]\". \n",
    "\n",
    "    Returns:\n",
    "    tsv: a tsv file of list of SMILES, named as SL_Sirius.tsv, is stored \n",
    "    in input_dir\n",
    "    directory: a directory with compound fragmentations will be created \n",
    "    in a folder named SL_Frag/ within the same input_dir\n",
    "    \n",
    "    \n",
    "    Usage:\n",
    "    slist_sirius(\"/user/project/\", \"suspectlist.csv\", \n",
    "    substring = None)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    sl = pd.read_csv(slist_csv)\n",
    "    \n",
    "    # define function to neutralize the charged SMILES\n",
    "    def neutralize_atoms(mol):\n",
    "        \n",
    "        pattern = Chem.MolFromSmarts(\"[+1!h0!$([*]~[-1,-2,-3,-4]),-1!$([*]~[+1,+2,+3,+4])]\")\n",
    "        at_matches = mol.GetSubstructMatches(pattern)\n",
    "        at_matches_list = [y[0] for y in at_matches]\n",
    "        if len(at_matches_list) > 0:\n",
    "            for at_idx in at_matches_list:\n",
    "                atom = mol.GetAtomWithIdx(at_idx)\n",
    "                chg = atom.GetFormalCharge()\n",
    "                hcount = atom.GetTotalNumHs()\n",
    "                atom.SetFormalCharge(0)\n",
    "                atom.SetNumExplicitHs(hcount - chg)\n",
    "                atom.UpdatePropertyCache()\n",
    "        return mol\n",
    "    \n",
    "\n",
    "    for i, row in sl.iterrows():\n",
    "        # remove SMILES with wild card\n",
    "        if \"*\" in sl[\"SMILES\"][i]:\n",
    "            sl = sl.drop(labels = i, axis = 0) \n",
    "    for i, row in sl.iterrows():\n",
    "        # remove SMILES with any string present in the substring\n",
    "        if substring:\n",
    "            if bool([ele for ele in substring if(ele in sl[\"SMILES\"][i])]):\n",
    "                sl = sl.drop(labels = i, axis = 0)\n",
    "    for i, row in sl.iterrows():\n",
    "        if \".\" in sl[\"SMILES\"][i]:\n",
    "            sl.loc[i, \"SMILES\"] = sl[\"SMILES\"][i].split('.')[0]\n",
    "    # Neutralize the charged SMILES\n",
    "    for i, row in sl.iterrows():\n",
    "        if \"+\" in sl[\"SMILES\"][i] or \"-\" in sl[\"SMILES\"][i]:\n",
    "            mol = Chem.MolFromSmiles(sl[\"SMILES\"][i])\n",
    "            neutralize_atoms(mol)\n",
    "            sl.loc[i, \"SMILES\"] = Chem.MolToSmiles(mol)\n",
    "            \n",
    "            # Remove multiple charged SMILES\n",
    "            if \"+\" in sl[\"SMILES\"][i] or \"-\" in sl[\"SMILES\"][i]:\n",
    "                pos = sl[\"SMILES\"][i].count('+')\n",
    "                neg = sl[\"SMILES\"][i].count('-')\n",
    "                charge = pos + neg \n",
    "                if charge > 1:\n",
    "                    sl = sl.drop(labels = i, axis = 0) \n",
    "                    \n",
    "    slsirius = pd.DataFrame({'smiles':sl[\"SMILES\"]})\n",
    "    slsirius.to_csv(input_dir+ \"SL_Sirius.tsv\", sep = \"\\t\", header = False, index = False)\n",
    "    os.system(\"sirius --input \" + input_dir + \"SL_Sirius.tsv custom-db --name=SL_Frag --output \"+ input_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09b81b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(slist_metfrag.__doc__)\n",
    "#print(slist_sirius.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193fa5f0",
   "metadata": {},
   "source": [
    "## Spectral DB dereplication Results PostProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e56d0de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spec_postproc(input_dir, Source = \"all\"):\n",
    "    \n",
    "    \"\"\"spec_postproc function processes the resulst from dereplication \n",
    "    using different spectral DBs. \n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "    \n",
    "    Source (str): either \"mbank\" or \"hmdb\" or \"gnps\", or \"all\"\n",
    "\n",
    "    Returns:\n",
    "    \n",
    "    dataframe: of the paths of the processed DB results\n",
    "    \n",
    "    \n",
    "    Usage:\n",
    "    spec_postproc(input_dir = \"/user/project/\", Source = \"all\")\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def isNaN(string):\n",
    "        return string != string\n",
    "    \n",
    "    \n",
    "    def HMDB_Scoring(db, i):\n",
    "        if db['HMDBintScore'][i] >= 0.50 and db['HMDBmzScore'][i] >= 0.50 and db['HQMatchingPeaks'][i]/db['hQueryTotalPeaks'][i] >= 0.50:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "    def GNPS_Scoring(db, i):\n",
    "        if db['GNPSintScore'][i] >= 0.50 and db['GNPSmzScore'][i] >= 0.50 and db['GQMatchingPeaks'][i]/db['gQueryTotalPeaks'][i] >= 0.50:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "    def MB_Scoring(db, i):\n",
    "        if db['MBintScore'][i] >= 0.50 and db['MBmzScore'][i] >= 0.50 and db['MQMatchingPeaks'][i]/db['mQueryTotalPeaks'][i] >= 0.50:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    \n",
    "    \n",
    "    #list all files and directories\n",
    "    for entry in os.listdir(input_dir):\n",
    "        if os.path.isdir(os.path.join(input_dir, entry)):\n",
    "            #print(entry)\n",
    "            msp_file = (glob.glob(input_dir + \"/\" + entry + '/spectral_dereplication' +'/*.csv'))\n",
    "            #print(msp_file)\n",
    "            if len(msp_file) > 0:\n",
    "                if os.path.exists(msp_file[0]):\n",
    "                    msp = pd.read_csv(msp_file[0])\n",
    "                    # enter the directory with /spectral_dereplication/ results\n",
    "                    \n",
    "                    # enter the directory with /spectral_dereplication/ results\n",
    "                    if Source == \"gnps\" or Source == \"all\":\n",
    "                        #currently only these subsets are removed from the names from GNPS\n",
    "                        matches = [\"M+\",\"[M\", \"M-\", \"2M\", \"M*\" \"20.0\", \"50.0\", \"30.0\", \"40.0\", \"60.0\", \"70.0\", \"eV\", \"Massbank\"\n",
    "                                   , \"Spectral\", \"Match\", \"to\", \"from\", \"NIST14\", \"MoNA\", '[IIN-based:',  '[IIN-based', 'on:', 'CCMSLIB00003136269]']\n",
    "\n",
    "                        #open another csv path holding empty list, which will be filled \n",
    "                        #with post processed csv results\n",
    "                        GNPScsvfiles2 = []\n",
    "                        #print(entry)\n",
    "                        # enter the directory with /spectral_dereplication/ results\n",
    "                        sub_dir = input_dir + \"/\" + entry + '/spectral_dereplication/GNPS/'\n",
    "                        if os.path.exists(sub_dir):\n",
    "                            files = (glob.glob(sub_dir+'/*.csv'))\n",
    "                            #print(files)\n",
    "                            for mz, row in msp.iterrows():\n",
    "                                #print(msp[\"id_X\"][mz])\n",
    "                                for fls_g in files:\n",
    "                                    if msp[\"id_X\"][mz] in fls_g:\n",
    "                                        gnps_df = pd.read_csv(fls_g)\n",
    "                                        gnps_df = gnps_df.drop_duplicates(subset=['GNPSspectrumID'])\n",
    "                                        if len(gnps_df)>0:\n",
    "\n",
    "                                            for i, row in gnps_df.iterrows():\n",
    "                                                # if compound name is present\n",
    "                                                if GNPS_Scoring(gnps_df, i):\n",
    "                                                    if not isNaN(gnps_df['GNPScompound_name'][i]):\n",
    "                                                        # split if there is a gap in the names\n",
    "                                                        string_chng = (gnps_df['GNPScompound_name'][i].split(\" \"))\n",
    "                                                        # create an empty list\n",
    "                                                        newstr = []\n",
    "\n",
    "                                                        # for each part of the string in the names\n",
    "                                                        chng = []\n",
    "\n",
    "                                                        for j in range(len(string_chng)):\n",
    "                                                            # check if the substrings are present in the matches and no - is present\n",
    "                                                            if not any(x in string_chng[j] for x in matches): #and not '-' == string_chng[j]:\n",
    "                                                                # IF | and ! not in the substring\n",
    "                                                                if '|' not in string_chng[j] or '!' not in string_chng[j]:\n",
    "                                                                    newstr.append(string_chng[j])\n",
    "                                                                # if | present in the substring   \n",
    "                                                                elif '|' in string_chng[j]:\n",
    "\n",
    "                                                                    #split the string\n",
    "                                                                    jlen = string_chng[j].split(\"|\")\n",
    "                                                                    #how many substrings are left now\n",
    "                                                                    lst = len(jlen)-1\n",
    "                                                                    #append this to chng\n",
    "                                                                    chng.append(jlen[lst])\n",
    "                                                                    break\n",
    "\n",
    "                                                                    # now append chng to newstr            \n",
    "                                                        chng.append(' '.join(newstr))\n",
    "                                                        #save this as the correct name\n",
    "                                                        gnps_df.loc[i, \"corr_names\"] = chng[0]\n",
    "                                                        if not isNaN(gnps_df['GNPSSMILES'][i]):\n",
    "                                                            if chng == '':\n",
    "                                                                break\n",
    "                                                            elif gnps_df['GNPSSMILES'][i].isalpha():\n",
    "                                                                s = pcp.get_compounds(chng[0], 'name')\n",
    "                                                                if s:\n",
    "                                                                    for comp in s:\n",
    "                                                                        gnps_df[\"GNPSSMILES\"][i] = comp.isomeric_smiles\n",
    "                                                                else:\n",
    "                                                                    gnps_df[\"GNPSSMILES\"][i] = ''\n",
    "                                                    else:\n",
    "                                                        gnps_df[\"GNPSSMILES\"][i] = ''\n",
    "                                                else:\n",
    "                                                    gnps_df.drop([i], axis=0, inplace=True)\n",
    "\n",
    "                                            for k, row in gnps_df.iterrows():\n",
    "                                                if isNaN(gnps_df['GNPSSMILES'][k]):\n",
    "                                                    if \"[\" in gnps_df['GNPScompound_name'][k].split(\" \")[-1]:\n",
    "                                                        string_chng = (gnps_df['GNPScompound_name'][k].split(\"[\"))\n",
    "                                                        #print(gnps_df['GNPScompound_name'][i])\n",
    "                                                        keep_names = []\n",
    "                                                        for j in range(len(string_chng)-1):\n",
    "                                                            gnps_df.loc[k, \"corr_names\"] == string_chng[j]\n",
    "                                                            s = pcp.get_compounds(string_chng[j], 'name')\n",
    "\n",
    "                                                            if s:\n",
    "                                                                for comp in s:\n",
    "                                                                    gnps_df[\"GNPSSMILES\"][k] = comp.isomeric_smiles\n",
    "                                                                    gnps_df.loc[k, \"GNPSformula\"] = comp.molecular_formula\n",
    "                                                                    gnps_df.loc[k, \"GNPSinchi\"] = Chem.MolToInchi(Chem.MolFromSmiles(comp.isomeric_smiles))\n",
    "                                                                    \n",
    "                                                                    \n",
    "                                                            else:\n",
    "                                                                gnps_df[\"GNPSSMILES\"][k] = ''\n",
    "                                                                gnps_df.loc[k, \"GNPSformula\"] = ''\n",
    "                                                                gnps_df.loc[k, \"GNPSinchi\"] = ''\n",
    "                                                if not isNaN(gnps_df['GNPSSMILES'][k]):\n",
    "                                                    try:\n",
    "                                                        \n",
    "                                                        sx = pcp.get_compounds(gnps_df['GNPSSMILES'][k], 'smiles')\n",
    "                                                        gnps_df.loc[k, \"GNPSinchi\"] = Chem.MolToInchi(Chem.MolFromSmiles(comp.isomeric_smiles))\n",
    "                                                        if sx:\n",
    "                                                            sx = str(sx)\n",
    "                                                            comp = pcp.Compound.from_cid([int(x) for x in re.findall(r'\\b\\d+\\b', sx)])\n",
    "                                                            gnps_df.loc[k, 'GNPSformula'] = comp.molecular_formula\n",
    "                                                            \n",
    "                                                    except:\n",
    "                                                        gnps_df.loc[k, \"GNPSformula\"] = ''\n",
    "                                                        gnps_df.loc[k, \"GNPSinchi\"] = ''\n",
    "\n",
    "\n",
    "\n",
    "                                        csvname = (os.path.splitext(fls_g)[0])+\"proc\"+\".csv\"\n",
    "                                        gnps_results_csv = csvname.replace(input_dir, \".\")\n",
    "                                        msp.loc[mz, \"gnps_results_csv\"] = gnps_results_csv \n",
    "                                        gnps_df.to_csv(csvname)\n",
    "                                        GNPScsvfiles2.append(csvname)\n",
    "                                    dict1 = {'GNPSr': GNPScsvfiles2} \n",
    "                                    df = pd.DataFrame(dict1)\n",
    "                                    #return(df)\n",
    "                                    \n",
    "                    msp.to_csv(msp_file[0])                \n",
    "                                    \n",
    "                    if Source == \"hmdb\" or Source == \"all\":                 \n",
    "\n",
    "                        if not os.path.exists(input_dir+\"/structures.sdf\"):\n",
    "                            #download SDF structures\n",
    "                            os.system(\"wget -P \" + input_dir + \" https://hmdb.ca/system/downloads/current/structures.zip\")\n",
    "                            os.system(\"unzip \"+ input_dir + \"/structures.zip\" + \" -d \" + input_dir)\n",
    "                        # Load the sdf\n",
    "                        dframe = PandasTools.LoadSDF((input_dir+\"/structures.sdf\"),\n",
    "                                                     idName='HMDB_ID',smilesName='SMILES',\n",
    "                                                     molColName='Molecule', includeFingerprints=False)\n",
    "\n",
    "\n",
    "                        HMDBcsvfiles2 = []\n",
    "                        #print(entry)\n",
    "                        # enter the directory with /spectral_dereplication/ results\n",
    "                        sub_dir = input_dir + \"/\" + entry + '/spectral_dereplication/HMDB/'\n",
    "\n",
    "\n",
    "                        if os.path.exists(sub_dir):\n",
    "\n",
    "                            #print(sub_dir)\n",
    "                            files = (glob.glob(sub_dir+'/*.csv'))\n",
    "                            #print(files)\n",
    "                            for mz, row in msp.iterrows():\n",
    "                                #print(msp[\"id_X\"][mz])\n",
    "                                for fls_h in files:\n",
    "                                    if msp[\"id_X\"][mz] in fls_h:\n",
    "                                        hmdb_df = pd.read_csv(fls_h)\n",
    "                                        hmdb_df = hmdb_df.drop_duplicates(subset=['HMDBcompoundID'])\n",
    "\n",
    "                                        if len(hmdb_df)>0:\n",
    "                                            print(entry)\n",
    "                                            # merge on basis of id, frame and hmdb result files\n",
    "                                            SmilesHM = pd.merge(hmdb_df, dframe, left_on=hmdb_df.HMDBcompoundID, right_on=dframe.DATABASE_ID)\n",
    "\n",
    "\n",
    "                                            for i, row in hmdb_df.iterrows():\n",
    "                                                if HMDB_Scoring(hmdb_df, i):\n",
    "\n",
    "                                                    for j, row in SmilesHM.iterrows():\n",
    "\n",
    "                                                        # where index for both match, add the name and SMILES\n",
    "                                                        if hmdb_df['HMDBcompoundID'][i]== SmilesHM['HMDBcompoundID'][j]:\n",
    "                                                            hmdb_df.loc[i, 'HMDBSMILES'] = SmilesHM['SMILES'][j]#add SMILES\n",
    "                                                            hmdb_df.loc[i, 'HMDBcompound_name'] = SmilesHM[\"GENERIC_NAME\"][j]#add name\n",
    "                                                            hmdb_df.loc[i, 'HMDBformula'] = SmilesHM[\"FORMULA\"][j]#add formula\n",
    "                                                            hmdb_df.loc[i, 'HMDBinchi'] = Chem.MolToInchi(Chem.MolFromSmiles(SmilesHM['SMILES'][j]))\n",
    "                                                else:\n",
    "                                                    hmdb_df.drop([i], axis=0, inplace=True)\n",
    "\n",
    "                                        csvname = (os.path.splitext(fls_h)[0])+\"proc\"+\".csv\" # name for writing it in a new file\n",
    "                                        hmdb_results_csv = csvname.replace(input_dir, \".\")\n",
    "                                        msp.loc[mz, \"hmdb_results_csv\"] = hmdb_results_csv \n",
    "                                        hmdb_df.to_csv(csvname) #write\n",
    "                                        HMDBcsvfiles2.append(csvname)# add to a list\n",
    "                                    dict1 = {'HMDBr': HMDBcsvfiles2} \n",
    "                                    df = pd.DataFrame(dict1)\n",
    "                                    #return(df)\n",
    "                    \n",
    "                    msp.to_csv(msp_file[0])\n",
    "                    # enter the directory with /spectral_dereplication/ results\n",
    "                    if Source == \"mbank\" or Source == \"all\":\n",
    "                        #open another csv path holding empty list, which will be filled \n",
    "                        #with post processed csv results\n",
    "                        MassBankcsvfiles2 = []\n",
    "                        #print(entry)\n",
    "                        # enter the directory with /spectral_dereplication/ results\n",
    "                        sub_dir = input_dir + \"/\" + entry + '/spectral_dereplication/MassBank/'\n",
    "                        if os.path.exists(sub_dir):\n",
    "                            files = (glob.glob(sub_dir+'/*.csv'))\n",
    "                            #print(files)\n",
    "                            for mz, row in msp.iterrows():\n",
    "                                #print(msp[\"id_X\"][mz])\n",
    "                                for fls_m in files:\n",
    "                                    if msp[\"id_X\"][mz] in fls_m:\n",
    "                                        print(fls_m)\n",
    "                                        mbank_df = pd.read_csv(fls_m)\n",
    "                                        mbank_df = mbank_df.drop_duplicates(subset=['MBspectrumID'])\n",
    "                                        if len(mbank_df)>0:\n",
    "\n",
    "\n",
    "                                            for i, row in mbank_df.iterrows():\n",
    "                                                if MB_Scoring(mbank_df, i):\n",
    "\n",
    "                                                    inchiK = str(mbank_df[\"MBinchiKEY\"][i])\n",
    "\n",
    "                                                    #extract inchikeys\n",
    "                                                    y = pcp.get_compounds(inchiK, 'inchikey')#compound based on inchikey\n",
    "\n",
    "                                                    for compound in y:\n",
    "\n",
    "                                                        #add smiles\n",
    "                                                        smles = compound.isomeric_smiles   \n",
    "                                                        mbank_df.loc[i, 'MBSMILES'] = smles\n",
    "                                                        mbank_df.loc[i, 'MBinchi'] =Chem.MolToInchi(Chem.MolFromSmiles(smles))\n",
    "                                                else:\n",
    "                                                    mbank_df.drop([i], axis=0, inplace=True)\n",
    "\n",
    "\n",
    "                                        csvname = (os.path.splitext(fls_m)[0])+\"proc\"+\".csv\"\n",
    "                                        mbank_results_csv = csvname.replace(input_dir, \".\")\n",
    "                                        msp.loc[mz, \"mbank_results_csv\"] = mbank_results_csv \n",
    "                                        mbank_df.to_csv(csvname)\n",
    "                                        MassBankcsvfiles2.append(csvname)\n",
    "\n",
    "                                    dict1 = {'MBr': MassBankcsvfiles2} \n",
    "                                    df = pd.DataFrame(dict1)\n",
    "                                    #return(df)                \n",
    "                    \n",
    "                    msp.to_csv(msp_file[0])\n",
    "\n",
    "    if Source == \"all\":\n",
    "\n",
    "        dict1 = {'GNPSr': GNPScsvfiles2, 'HMDBr': HMDBcsvfiles2, 'MBr': MassBankcsvfiles2} \n",
    "        df = pd.DataFrame(dict1)\n",
    "\n",
    "        return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e84f7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spec_postproc(input_dir, Source = \"mbank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27899d3a",
   "metadata": {},
   "source": [
    "# SIRIUS Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ef42aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sirius_postproc(input_dir, exp_int = 0.90, csi_score = -150):\n",
    "    def isNaN(string):\n",
    "        return string != string\n",
    "    def str_can_score(db, i):\n",
    "        if db['explainedIntensity'][i] >= exp_int and db['CSI:FingerIDScore'][i] >= csi_score:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    # entry is all files and folders in input_dir\n",
    "    for entry in os.listdir(input_dir):\n",
    "        #if the entry is also a directory\n",
    "        if os.path.isdir(os.path.join(input_dir, entry)):\n",
    "            sub_dir = input_dir + \"/\" + entry + '/insilico/SIRIUS/'\n",
    "            msp_csv = input_dir + \"/\" + entry + \"/insilico/MS1DATA.csv\"\n",
    "            if os.path.exists(msp_csv) and os.path.exists(sub_dir):\n",
    "                #output json files from SIRIUS\n",
    "                files_S = (glob.glob(sub_dir+'/*.json'))\n",
    "                #list of precursor m/z\n",
    "                msp = pd.read_csv(msp_csv)\n",
    "\n",
    "                # for each mz\n",
    "                for mz, row in msp.iterrows():\n",
    "                    # make a list of files with this mz\n",
    "                    files_for_mz = []\n",
    "\n",
    "                    for file in files_S:\n",
    "                        if str(msp[\"premz\"][mz]) in file:\n",
    "                            files_for_mz.append(file)\n",
    "\n",
    "                    # in case if SL and ALL are given\n",
    "                    if len(files_for_mz)==2:\n",
    "\n",
    "                        # if the SL file is before the ALL file\n",
    "                        if len(files_for_mz[0])>len(files_for_mz[1]):\n",
    "\n",
    "                            # extract the formula and structure files\n",
    "                            json_dirSL = next(os.walk(files_for_mz[0]))[1]\n",
    "                            sub_sub_dirSL_structure_can = files_for_mz[0] + \"/\" + json_dirSL[0]  + \"/structure_candidates.tsv\"                   \n",
    "                            sub_sub_dirSL_formula_can = files_for_mz[0] + \"/\" + json_dirSL[0]  + \"/formula_candidates.tsv\" \n",
    "                            SL_Canopus_csv = files_for_mz[0] + \"/canopus_summary.tsv\"\n",
    "\n",
    "\n",
    "                            # extract the formula and structure files\n",
    "                            json_dirALL = next(os.walk(files_for_mz[1]))[1]\n",
    "                            sub_sub_dirALL_structure_can = files_for_mz[1] + \"/\" + json_dirALL[0] +\"/structure_candidates.tsv\"\n",
    "                            sub_sub_dirALL_formula_can = files_for_mz[1] + \"/\" + json_dirALL[0] +\"/formula_candidates.tsv\"\n",
    "                            ALL_Canopus_csv = files_for_mz[1] + \"/canopus_summary.tsv\"\n",
    "\n",
    "\n",
    "                        # if the ALL file is before the SL file\n",
    "                        elif len(files_for_mz[1]) >len(files_for_mz[0]):\n",
    "\n",
    "\n",
    "                            # extract the formula and structure files\n",
    "                            json_dirALL = next(os.walk(files_for_mz[0]))[1]\n",
    "                            sub_sub_dirALL_structure_can = files_for_mz[0] + \"/\" + json_dirALL[0]  + \"/structure_candidates.tsv\"                   \n",
    "                            sub_sub_dirALL_formula_can = files_for_mz[0] + \"/\" + json_dirALL[0]  + \"/formula_candidates.tsv\"\n",
    "                            ALL_Canopus_csv = files_for_mz[0] + \"/canopus_summary.tsv\"\n",
    "\n",
    "\n",
    "                            # extract the formula and structure files\n",
    "                            json_dirSL = next(os.walk(files_for_mz[1]))[1]\n",
    "                            sub_sub_dirSL_structure_can = files_for_mz[1] + \"/\" + json_dirSL[0] +\"/structure_candidates.tsv\"\n",
    "                            sub_sub_dirSL_formula_can = files_for_mz[1] + \"/\" + json_dirALL[0]  + \"/formula_candidates.tsv\"\n",
    "                            SL_Canopus_csv = files_for_mz[1] + \"/canopus_summary.tsv\"\n",
    "\n",
    "                        # if both structure files exist and they have more than 0 rows\n",
    "                        if os.path.exists(sub_sub_dirSL_structure_can) and len(pd.read_csv(sub_sub_dirSL_structure_can, sep = \"\\t\"))>0 and os.path.exists(sub_sub_dirALL_structure_can) and len(pd.read_csv(sub_sub_dirALL_structure_can, sep = \"\\t\")):\n",
    "\n",
    "                            # read strcuture and formula tsv files for both SL and ALL\n",
    "                            SL_structure_csv = pd.read_csv(sub_sub_dirSL_structure_can, sep = \"\\t\")\n",
    "                            SL_formula_csv = pd.read_csv(sub_sub_dirSL_formula_can, sep = \"\\t\")\n",
    "                            SL_Canopus = pd.read_csv(SL_Canopus_csv, sep = \"\\t\")\n",
    "\n",
    "\n",
    "                            ALL_structure_csv = pd.read_csv(sub_sub_dirALL_structure_can, sep = \"\\t\")\n",
    "                            ALL_formula_csv = pd.read_csv(sub_sub_dirALL_formula_can, sep = \"\\t\")\n",
    "                            ALL_Canopus = pd.read_csv(ALL_Canopus_csv, sep = \"\\t\")\n",
    "\n",
    "                            # Add the structure and formula files together\n",
    "                            for structure, rows in ALL_structure_csv.iterrows():\n",
    "                                for formula, rows in ALL_formula_csv.iterrows():\n",
    "                                    if ALL_structure_csv[\"formulaRank\"][structure] == ALL_formula_csv[\"rank\"][formula]:\n",
    "                                        ALL_structure_csv.loc[structure, 'SiriusScore'] = ALL_formula_csv['SiriusScore'][formula]\n",
    "                                        ALL_structure_csv.loc[structure, 'numExplainedPeaks'] = ALL_formula_csv['numExplainedPeaks'][formula]\n",
    "                                        ALL_structure_csv.loc[structure, 'explainedIntensity'] = ALL_formula_csv['explainedIntensity'][formula]\n",
    "                                        ALL_structure_csv.loc[structure, \"SuspectListEntry\"] = \"FALSE\"\n",
    "                                        if ALL_formula_csv[\"molecularFormula\"][formula] == ALL_Canopus[\"molecularFormula\"][0]:\n",
    "                                            ALL_structure_csv.loc[structure, 'superclass'] = ALL_Canopus['superclass'][0]\n",
    "                                            ALL_structure_csv.loc[structure, 'class'] = ALL_Canopus['class'][0]\n",
    "                                            ALL_structure_csv.loc[structure, 'subclass'] = ALL_Canopus['subclass'][0]\n",
    "\n",
    "                            # Add the structure and formula files together\n",
    "                            for structure_sl, rows in SL_structure_csv.iterrows():\n",
    "                                for formula_sl, rows in SL_formula_csv.iterrows():\n",
    "                                    if SL_structure_csv[\"formulaRank\"][structure_sl] == SL_formula_csv[\"rank\"][formula_sl]:\n",
    "                                        SL_structure_csv.loc[structure_sl, 'SiriusScore'] = SL_formula_csv['SiriusScore'][formula_sl]\n",
    "                                        SL_structure_csv.loc[structure_sl, 'numExplainedPeaks'] = SL_formula_csv['numExplainedPeaks'][formula_sl]\n",
    "                                        SL_structure_csv.loc[structure_sl, 'explainedIntensity'] = SL_formula_csv['explainedIntensity'][formula_sl]\n",
    "                                        SL_structure_csv.loc[structure_sl, \"SuspectListEntry\"] = \"TRUE\"   \n",
    "                                        if SL_formula_csv[\"molecularFormula\"][formula_sl] == SL_Canopus[\"molecularFormula\"][0]:\n",
    "                                            SL_structure_csv.loc[structure_sl, 'superclass'] = SL_Canopus['superclass'][0]\n",
    "                                            SL_structure_csv.loc[structure_sl, 'class'] = SL_Canopus['class'][0]\n",
    "                                            SL_structure_csv.loc[structure_sl, 'subclass'] = SL_Canopus['subclass'][0] \n",
    "\n",
    "\n",
    "                            # after formula and structure have been merged, merge SL and ALL results\n",
    "                            all_sl_db = pd.concat([ALL_structure_csv, SL_structure_csv], ignore_index=True)\n",
    "                            for str_sirius, row in all_sl_db.iterrows():\n",
    "                                if not str_can_score(all_sl_db, str_sirius):\n",
    "                                    all_sl_db = all_sl_db.drop(str_sirius, inplace=False)\n",
    "\n",
    "                            if not os.path.exists(sub_dir+\"results_for_\"+json_dirALL[0].split(\"_\")[-1]):\n",
    "                                os.mkdir(sub_dir+\"results_for_\"+json_dirALL[0].split(\"_\")[-1])\n",
    "\n",
    "                            result_sirius_name = (sub_dir+\"results_for_\"+json_dirALL[0].split(\"_\")[-1]+\"_\"+\"structure_\"+json_dirALL[0].split(\"_\")[-1] + \".csv\")\n",
    "                            msp.loc[mz, \"sirius_result_dir\"] = result_sirius_name.replace(input_dir, \".\")\n",
    "                            all_sl_db.to_csv(sub_dir+\"results_for_\"+json_dirALL[0].split(\"_\")[-1]+\"_\"+\"structure_\"+json_dirALL[0].split(\"_\")[-1] + \".csv\")\n",
    "\n",
    "                        # if only ALL structure file exists and they have more than 0 rows\n",
    "                        elif not (os.path.exists(sub_sub_dirSL_structure_can) and len(pd.read_csv(sub_sub_dirSL_structure_can, sep = \"\\t\"))>0) and os.path.exists(sub_sub_dirALL_structure_can) and len(pd.read_csv(sub_sub_dirALL_structure_can, sep = \"\\t\")):\n",
    "                            ALL_structure_csv = pd.read_csv(sub_sub_dirALL_structure_can, sep = \"\\t\")\n",
    "                            ALL_formula_csv = pd.read_csv(sub_sub_dirALL_formula_can, sep = \"\\t\")\n",
    "                            ALL_Canopus = pd.read_csv(ALL_Canopus_csv, sep = \"\\t\")\n",
    "\n",
    "                            # Add the structure and formula files together\n",
    "                            for structure, rows in ALL_structure_csv.iterrows():\n",
    "                                for formula, rows in ALL_formula_csv.iterrows():\n",
    "                                    if ALL_structure_csv[\"formulaRank\"][structure] == ALL_formula_csv[\"rank\"][formula]:\n",
    "                                        ALL_structure_csv.loc[structure, 'SiriusScore'] = ALL_formula_csv['SiriusScore'][formula]\n",
    "                                        ALL_structure_csv.loc[structure, 'numExplainedPeaks'] = ALL_formula_csv['numExplainedPeaks'][formula]\n",
    "                                        ALL_structure_csv.loc[structure, 'explainedIntensity'] = ALL_formula_csv['explainedIntensity'][formula]\n",
    "                                        ALL_structure_csv.loc[structure, \"SuspectListEntry\"] = \"FALSE\"\n",
    "                                        if ALL_formula_csv[\"molecularFormula\"][formula] == ALL_Canopus[\"molecularFormula\"][0]:\n",
    "                                            ALL_structure_csv.loc[structure, 'superclass'] = ALL_Canopus['superclass'][0]\n",
    "                                            ALL_structure_csv.loc[structure, 'class'] = ALL_Canopus['class'][0]\n",
    "                                            ALL_structure_csv.loc[structure, 'subclass'] = ALL_Canopus['subclass'][0]\n",
    "\n",
    "                            for str_siriusA, row in ALL_structure_csv.iterrows():\n",
    "                                if not str_can_score(ALL_structure_csv, str_siriusA):\n",
    "                                    ALL_structure_csv = ALL_structure_csv.drop(str_siriusA, inplace=False)\n",
    "\n",
    "\n",
    "                            if not os.path.exists(sub_dir+\"results_for_\"+json_dirALL[0].split(\"_\")[-1]):\n",
    "                                os.mkdir(sub_dir+\"results_for_\"+json_dirALL[0].split(\"_\")[-1])\n",
    "\n",
    "                            result_sirius_name = (sub_dir+\"results_for_\"+json_dirALL[0].split(\"_\")[-1]+\"_\"+\"structure_\"+json_dirALL[0].split(\"_\")[-1] + \".csv\")\n",
    "                            msp.loc[mz, \"sirius_result_dir\"] = result_sirius_name.replace(input_dir, \".\")\n",
    "                            ALL_structure_csv.to_csv(sub_dir+\"results_for_\"+json_dirALL[0].split(\"_\")[-1]+\"_\"+\"structure_\"+json_dirALL[0].split(\"_\")[-1] + \".csv\")\n",
    "\n",
    "                        elif not(os.path.exists(sub_sub_dirSL_structure_can) and len(pd.read_csv(sub_sub_dirSL_structure_can, sep = \"\\t\"))>0 and os.path.exists(sub_sub_dirALL_structure_can) and len(pd.read_csv(sub_sub_dirALL_structure_can, sep = \"\\t\"))):\n",
    "                            if os.path.exists(sub_sub_dirALL_formula_can) and pd.read_csv(sub_sub_dirALL_formula_can, sep = \"\\t\"):\n",
    "                                ALL_formula_csv = pd.read_csv(sub_sub_dirALL_formula_can, sep = \"\\t\")\n",
    "                                ALL_Canopus = pd.read_csv(ALL_Canopus_csv, sep = \"\\t\")\n",
    "                                for formula, rows in ALL_formula_csv.iterrows():\n",
    "                                    ALL_formula_csv.loc[formula, 'superclass'] = ALL_Canopus['superclass'][0]\n",
    "                                    ALL_formula_csv.loc[formula, 'class'] = ALL_Canopus['class'][0]\n",
    "                                    ALL_formula_csv.loc[formula, 'subclass'] = ALL_Canopus['subclass'][0]\n",
    "\n",
    "                                for for_siriusA, row in ALL_formula_csv.iterrows():\n",
    "                                    if not ALL_formula_csv['explainedIntensity'][for_siriusA] >= exp_int:\n",
    "                                        ALL_formula_csv = ALL_formula_csv.drop(for_siriusA, inplace=False)\n",
    "                                if not os.path.exists(sub_dir+\"results_for_\"+json_dirALL[0].split(\"_\")[-1]):\n",
    "                                    os.mkdir(sub_dir+\"results_for_\"+json_dirALL[0].split(\"_\")[-1])\n",
    "\n",
    "                                result_sirius_name = (sub_dir+\"results_for_\"+json_dirALL[0].split(\"_\")[-1]+\"_\"+\"formula_\"+json_dirALL[0].split(\"_\")[-1] + \".csv\")\n",
    "                                msp.loc[mz, \"sirius_result_dir\"] = result_sirius_name.replace(input_dir, \".\")\n",
    "\n",
    "                                ALL_formula_csv.to_csv(sub_dir+\"results_for_\"+json_dirALL[0].split(\"_\")[-1]+\"_\"+\"formula_\"+json_dirALL[0].split(\"_\")[-1] + \".csv\")\n",
    "\n",
    "                            else:\n",
    "                                pass\n",
    "                        else:\n",
    "                            pass\n",
    "\n",
    "                    elif len(files_for_mz) == 1:\n",
    "                        # extract the formula and structure files\n",
    "                        json_dirALL = next(os.walk(files_for_mz[0]))[1]\n",
    "                        sub_sub_dirALL_structure_can = files_for_mz[0] + \"/\" + json_dirALL[0]  + \"/structure_candidates.tsv\"                   \n",
    "                        sub_sub_dirALL_formula_can = files_for_mz[0] + \"/\" + json_dirALL[0]  + \"/formula_candidates.tsv\" \n",
    "                        ALL_Canopus_csv = files_for_mz[0] + \"/canopus_summary.tsv\"\n",
    "\n",
    "                        # if both structure files exist\n",
    "                        if os.path.exists(sub_sub_dirALL_structure_can) and len(pd.read_csv(sub_sub_dirALL_structure_can, sep = \"\\t\"))>0:\n",
    "                            ALL_structure_csv = pd.read_csv(sub_sub_dirALL_structure_can, sep = \"\\t\")\n",
    "                            ALL_formula_csv = pd.read_csv(sub_sub_dirALL_formula_can, sep = \"\\t\")\n",
    "                            ALL_Canopus = pd.read_csv(ALL_Canopus_csv, sep = \"\\t\")\n",
    "                            # Add the structure and formula files together\n",
    "                            for structure, rows in ALL_structure_csv.iterrows():\n",
    "                                for formula, rows in ALL_formula_csv.iterrows():\n",
    "                                    if ALL_structure_csv[\"formulaRank\"][structure] == ALL_formula_csv[\"rank\"][formula]:\n",
    "                                        ALL_structure_csv.loc[structure, 'SiriusScore'] = ALL_formula_csv['SiriusScore'][formula]\n",
    "                                        ALL_structure_csv.loc[structure, 'numExplainedPeaks'] = ALL_formula_csv['numExplainedPeaks'][formula]\n",
    "                                        ALL_structure_csv.loc[structure, 'explainedIntensity'] = ALL_formula_csv['explainedIntensity'][formula]\n",
    "                                        ALL_structure_csv.loc[structure, \"SuspectListEntry\"] = \"FALSE\"\n",
    "\n",
    "                                        if ALL_formula_csv[\"molecularFormula\"][formula] == ALL_Canopus[\"molecularFormula\"][0]:\n",
    "                                            ALL_structure_csv.loc[structure, 'superclass'] = ALL_Canopus['superclass'][0]\n",
    "                                            ALL_structure_csv.loc[structure, 'class'] = ALL_Canopus['class'][0]\n",
    "                                            ALL_structure_csv.loc[structure, 'subclass'] = ALL_Canopus['subclass'][0]\n",
    "\n",
    "                            for str_siriusA, row in ALL_structure_csv.iterrows():\n",
    "                                if not str_can_score(ALL_structure_csv, str_siriusA):\n",
    "                                    ALL_structure_csv = ALL_structure_csv.drop(str_siriusA, inplace=False)\n",
    "                            if not os.path.exists(sub_dir+\"results_for_\"+json_dirALL[0].split(\"_\")[-1]):\n",
    "                                os.mkdir(sub_dir+\"results_for_\"+json_dirALL[0].split(\"_\")[-1])\n",
    "\n",
    "                            result_sirius_name = (sub_dir+\"results_for_\"+json_dirALL[0].split(\"_\")[-1]+\"_\"+\"structure_\"+json_dirALL[0].split(\"_\")[-1] + \".csv\")\n",
    "                            msp.loc[mz, \"sirius_result_dir\"] = result_sirius_name.replace(input_dir, \".\")\n",
    "\n",
    "                            ALL_structure_csv.to_csv(sub_dir+\"results_for_\"+json_dirALL[0].split(\"_\")[-1]+\"_\"+\"structure_\"+json_dirALL[0].split(\"_\")[-1] + \".csv\")\n",
    "\n",
    "\n",
    "                        elif not (os.path.exists(sub_sub_dirALL_structure_can) and len(pd.read_csv(sub_sub_dirALL_structure_can, sep = \"\\t\"))):\n",
    "                            if os.path.exists(sub_sub_dirALL_formula_can) and pd.read_csv(sub_sub_dirALL_formula_can, sep = \"\\t\"):\n",
    "                                ALL_formula_csv = pd.read_csv(sub_sub_dirALL_formula_can, sep = \"\\t\")\n",
    "                                ALL_Canopus = pd.read_csv(ALL_Canopus_csv, sep = \"\\t\")\n",
    "                                for formula, rows in ALL_formula_csv.iterrows():\n",
    "                                    ALL_formula_csv.loc[formula, 'superclass'] = ALL_Canopus['superclass'][0]\n",
    "                                    ALL_formula_csv.loc[formula, 'class'] = ALL_Canopus['class'][0]\n",
    "                                    ALL_formula_csv.loc[formula, 'subclass'] = ALL_Canopus['subclass'][0]\n",
    "\n",
    "                                for for_siriusA, row in ALL_formula_csv.iterrows():\n",
    "                                    if not ALL_formula_csv['explainedIntensity'][for_siriusA] >= exp_int:\n",
    "                                        ALL_formula_csv = ALL_formula_csv.drop(for_siriusA, inplace=False)\n",
    "                                if not os.path.exists(sub_dir+\"results_for_\"+json_dirALL[0].split(\"_\")[-1]):\n",
    "                                    os.mkdir(sub_dir+\"results_for_\"+json_dirALL[0].split(\"_\")[-1])\n",
    "\n",
    "                                result_sirius_name = (sub_dir+\"results_for_\"+json_dirALL[0].split(\"_\")[-1]+\"_\"+\"formula_\"+json_dirALL[0].split(\"_\")[-1] + \".csv\")\n",
    "                                msp.loc[mz, \"sirius_result_dir\"] = result_sirius_name.replace(input_dir, \".\")\n",
    "\n",
    "                                ALL_formula_csv.to_csv(sub_dir+\"results_for_\"+json_dirALL[0].split(\"_\")[-1]+\"_\"+\"formula_\"+json_dirALL[0].split(\"_\")[-1] + \".csv\")\n",
    "\n",
    "                            else:\n",
    "                                pass\n",
    "                        else:\n",
    "                            pass\n",
    "                msp.to_csv(msp_csv)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d004823d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4439333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sirius_postproc(input_dir, exp_int = 0.90, csi_score = -150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312e96b1",
   "metadata": {},
   "source": [
    "# MCSS for SpecDBs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "943f9ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MCSS_for_SpecDB(input_dir, Source):\n",
    "    def isNaN(string):\n",
    "        return string != string\n",
    "    # Describe the heavy atoms to be considered for MCSS\n",
    "    heavy_atoms = ['C', 'N', 'P', 'O', 'S']\n",
    "    #list all files and directories\n",
    "    for entry in os.listdir(input_dir):\n",
    "\n",
    "        if os.path.isdir(os.path.join(input_dir, entry)):\n",
    "\n",
    "            # for specdb\n",
    "            specdb_msp_file = (glob.glob(input_dir + \"/\" + entry + '/spectral_dereplication' +'/*.csv'))\n",
    "\n",
    "            if len(specdb_msp_file) > 0:\n",
    "\n",
    "                if os.path.exists(specdb_msp_file[0]):\n",
    "\n",
    "                    spec_msp = pd.read_csv(specdb_msp_file[0])\n",
    "\n",
    "                    for mz, row in spec_msp.iterrows():\n",
    "\n",
    "                        if Source == \"gnps\" or Source == \"specdb\" or Source == \"all\":\n",
    "\n",
    "\n",
    "                            sub_dir = input_dir + \"/\" + entry + '/spectral_dereplication/GNPS/'\n",
    "                            if os.path.exists(sub_dir):\n",
    "                                gnps_files = (glob.glob(sub_dir+'/*proc.csv'))\n",
    "\n",
    "                                for files in gnps_files:\n",
    "                                    if spec_msp[\"id_X\"][mz] in files:\n",
    "                                        gnpsproc = pd.read_csv(files) \n",
    "\n",
    "\n",
    "                                        if len(gnpsproc)>0:\n",
    "                                            G_Smiles = gnpsproc[\"GNPSSMILES\"]\n",
    "                                            G_Smiles = list(filter(None, G_Smiles))\n",
    "                                            #print(G_Smiles)\n",
    "                                            #create empty list of GNPS top smiles\n",
    "                                            GNPS_Mol = []\n",
    "                                            # extract only the InChI of the top 5\n",
    "                                            for j in list(G_Smiles):\n",
    "                                                if not isNaN(j):\n",
    "                                                    print(type(j))\n",
    "                                                    mol2 = Chem.MolFromSmiles(j)\n",
    "                                                    GNPS_Mol.append(mol2)\n",
    "\n",
    "                                            if len(GNPS_Mol) >= 2:\n",
    "                                                res = rdFMCS.FindMCS(GNPS_Mol)\n",
    "                                                sm_res = Chem.MolToSmiles(Chem.MolFromSmarts(res.smartsString))\n",
    "                                                # if there are atleast 3 heavy atoms in the MCSS, then add it to the result file\n",
    "                                                elem = [ele for ele in heavy_atoms if(ele in sm_res)]\n",
    "                                                if elem and len(sm_res)>=3:\n",
    "                                                    spec_msp.loc[mz, 'GNPS_MCSSstring'] = res.smartsString\n",
    "                                                    spec_msp.loc[mz, 'GNPS_MCSS_SMILES'] = Chem.MolToSmiles(Chem.MolFromSmarts(res.smartsString))\n",
    "                        if Source == \"hmdb\" or Source == \"specdb\" or Source == \"all\":\n",
    "                            sub_dir = input_dir + \"/\" + entry + '/spectral_dereplication/HMDB/'\n",
    "                            if os.path.exists(sub_dir):\n",
    "                                hmdb_files = (glob.glob(sub_dir+'/*proc.csv'))\n",
    "\n",
    "                                for files in hmdb_files:\n",
    "                                    if spec_msp[\"id_X\"][mz] in files:\n",
    "                                        hmdbproc = pd.read_csv(files) \n",
    "\n",
    "                                        if len(hmdbproc)>0:\n",
    "                                            H_Smiles = hmdbproc[\"HMDBSMILES\"]\n",
    "                                            H_Smiles = list(filter(None, H_Smiles))\n",
    "\n",
    "                                            HMDB_Mol = []\n",
    "                                            # extract only the InChI of the top 5\n",
    "                                            for j in list(H_Smiles):\n",
    "                                                if not isNaN(j):\n",
    "\n",
    "                                                    mol2 = Chem.MolFromSmiles(j)\n",
    "                                                    HMDB_Mol.append(mol2)\n",
    "\n",
    "                                            if len(HMDB_Mol) >= 2:\n",
    "                                                res = rdFMCS.FindMCS(HMDB_Mol)\n",
    "                                                sm_res = Chem.MolToSmiles(Chem.MolFromSmarts(res.smartsString))\n",
    "                                                # if there are atleast 3 heavy atoms in the MCSS, then add it to the result file\n",
    "                                                elem = [ele for ele in heavy_atoms if(ele in sm_res)]\n",
    "                                                if elem and len(sm_res)>=3:\n",
    "                                                    spec_msp.loc[mz, 'HMDB_MCSSstring'] = res.smartsString\n",
    "                                                    spec_msp.loc[mz, 'HMDB_MCSS_SMILES'] = Chem.MolToSmiles(Chem.MolFromSmarts(res.smartsString))\n",
    "                        if Source == \"mbank\" or Source == \"specdb\" or Source == \"all\":\n",
    "                            sub_dir = input_dir + \"/\" + entry + '/spectral_dereplication/MassBank/'\n",
    "                            if os.path.exists(sub_dir):\n",
    "                                mbank_files = (glob.glob(sub_dir+'/*proc.csv'))\n",
    "\n",
    "                                for files in mbank_files:\n",
    "                                    if spec_msp[\"id_X\"][mz] in files:\n",
    "                                        mbankproc = pd.read_csv(files) \n",
    "\n",
    "                                        if len(mbankproc)>0:\n",
    "                                            M_Smiles = mbankproc[\"MBSMILES\"]\n",
    "                                            M_Smiles = list(filter(None, M_Smiles))\n",
    "\n",
    "                                            MB_Mol = []\n",
    "                                            # extract only the InChI of the top 5\n",
    "                                            for j in list(M_Smiles):\n",
    "                                                if not isNaN(j):\n",
    "\n",
    "                                                    mol2 = Chem.MolFromSmiles(j)\n",
    "                                                    MB_Mol.append(mol2)\n",
    "\n",
    "                                            if len(MB_Mol) >= 2:\n",
    "                                                res = rdFMCS.FindMCS(MB_Mol)\n",
    "                                                sm_res = Chem.MolToSmiles(Chem.MolFromSmarts(res.smartsString))\n",
    "                                                # if there are atleast 3 heavy atoms in the MCSS, then add it to the result file\n",
    "                                                elem = [ele for ele in heavy_atoms if(ele in sm_res)]\n",
    "                                                if elem and len(sm_res)>=3:\n",
    "                                                    spec_msp.loc[mz, 'MB_MCSSstring'] = res.smartsString\n",
    "                                                    spec_msp.loc[mz, 'MB_MCSS_SMILES'] = Chem.MolToSmiles(Chem.MolFromSmarts(res.smartsString))\n",
    "                    spec_msp.to_csv(specdb_msp_file[0])\n",
    "            return(specdb_msp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1c96f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MCSS_for_SpecDB(input_dir, Source = \"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0196f851",
   "metadata": {},
   "source": [
    "# MCSS for SIRIUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20a460d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MCSS_for_SIRIUS(input_dir):\n",
    "    def isNaN(string):\n",
    "        return string != string\n",
    "    # Describe the heavy atoms to be considered for MCSS\n",
    "    heavy_atoms = ['C', 'N', 'P', 'O', 'S']\n",
    "    #list all files and directories\n",
    "    for entry in os.listdir(input_dir):\n",
    "\n",
    "        if os.path.isdir(os.path.join(input_dir, entry)):\n",
    "            #for sirius\n",
    "            sirius_msp_csv = input_dir + \"/\" + entry + \"/insilico/MS1DATA.csv\"\n",
    "            sub_dir = input_dir + \"/\" + entry + '/insilico/SIRIUS/'\n",
    "            if os.path.exists(sirius_msp_csv) and os.path.exists(sub_dir):\n",
    "                sirius_msp = pd.read_csv(sirius_msp_csv) \n",
    "                sirius_files = (glob.glob(sub_dir))\n",
    "                for mz, row in sirius_msp.iterrows():\n",
    "                    for sir_file in sirius_files:\n",
    "                        r = [s for s in os.listdir(sir_file) if \"results\" in s][0]\n",
    "                        sirius_f = (glob.glob((sir_file+r)+'/*.csv'))\n",
    "                        if str(sirius_msp[\"id_X\"][mz].split(\"_\")[1]) in sirius_f[0]:\n",
    "                            print(sirius_f)\n",
    "                            if len(sirius_f) == 1:\n",
    "                                s_f = pd.read_csv(sirius_f[0])\n",
    "                                if len(s_f)>0 and 'smiles' in s_f.columns.values.tolist():\n",
    "\n",
    "                                    S_Smiles = s_f[\"smiles\"]\n",
    "                                    #create empty list of MB top smiles\n",
    "                                    SIRIUS_Mol = []\n",
    "\n",
    "                                    # extract only the InChI of the top 5\n",
    "                                    for j in list(S_Smiles):\n",
    "                                        mol2 = Chem.MolFromSmiles(j)\n",
    "                                        SIRIUS_Mol.append(mol2)\n",
    "                                    if len(SIRIUS_Mol) >= 2:\n",
    "                                        res = rdFMCS.FindMCS(SIRIUS_Mol)\n",
    "                                        sm_res = Chem.MolToSmiles(Chem.MolFromSmarts(res.smartsString))\n",
    "                                        # if there are atleast 3 heavy atoms in the MCSS, then add it to the result file\n",
    "                                        elem = [ele for ele in heavy_atoms if(ele in sm_res)]\n",
    "                                        if elem and len(sm_res)>=3:\n",
    "                                            sirius_msp.loc[mz, 'SIRIUS_MCSSstring'] = res.smartsString\n",
    "                                            sirius_msp.loc[mz, 'SIRIUS_MCSS_SMILES'] = Chem.MolToSmiles(Chem.MolFromSmarts(res.smartsString))\n",
    "                sirius_msp.to_csv(sirius_msp_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0acae26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cf6a781",
   "metadata": {},
   "source": [
    "# Suspect list Screening with Spec DBs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3630ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SuspectListScreening(input_dir, SuspectListPath, tanimoto, Source):\n",
    "    def isNaN(string):\n",
    "        return string != string\n",
    "    \n",
    "    SuspectList = pd.read_csv(SuspectListPath)\n",
    "    \n",
    "    \n",
    "    for entry in os.listdir(input_dir):\n",
    "\n",
    "        if os.path.isdir(os.path.join(input_dir, entry)):\n",
    "            \n",
    "            if Source == \"gnps\" or Source == \"specdb\" or Source == \"all\":\n",
    "                \n",
    "                sub_dir = input_dir + \"/\" + entry + '/spectral_dereplication/GNPS/'\n",
    "                if os.path.exists(sub_dir):\n",
    "                    gnps_files = (glob.glob(sub_dir+'/*proc.csv'))\n",
    "                    for file in gnps_files:\n",
    "                        gnpsproc = pd.read_csv(file) \n",
    "                        if len(gnpsproc)>0:\n",
    "                            for g, row in gnpsproc.iterrows():\n",
    "                                for s, row in SuspectList.iterrows():\n",
    "                                    if not isNaN(gnpsproc['GNPSSMILES'][g]) and gnpsproc['GNPSSMILES'][g] != \" \":\n",
    "                                        if not isNaN(SuspectList['SMILES'][s]) and SuspectList['SMILES'][s] != \" \":\n",
    "                                            LHms2 = [Chem.MolFromSmiles(gnpsproc['GNPSSMILES'][g]), Chem.MolFromSmiles(SuspectList['SMILES'][s])]\n",
    "                                            LHfps2 = [AllChem.GetMorganFingerprintAsBitVect(x2,2, nBits=2048) for x2 in LHms2]\n",
    "                                            LHtn2 = DataStructs.FingerprintSimilarity(LHfps2[0],LHfps2[1])\n",
    "                                            if LHtn2 >= tanimoto:\n",
    "                                                gnpsproc.loc[g, 'SLGsmiles'] = SuspectList['SMILES'][s]\n",
    "                                                gnpsproc.loc[g, 'SLGname'] = SuspectList['Name'][s]\n",
    "                                                gnpsproc.loc[g, 'SLGtanimoto'] = LHtn2\n",
    "                        gnpsproc.to_csv(file)                       \n",
    "                        return(gnpsproc)\n",
    "            if Source == \"hmdb\" or Source == \"specdb\" or Source == \"all\":\n",
    "                \n",
    "                sub_dir = input_dir + \"/\" + entry + '/spectral_dereplication/HMDB/'\n",
    "                if os.path.exists(sub_dir):\n",
    "                    hmdb_files = (glob.glob(sub_dir+'/*proc.csv'))\n",
    "                    for file in hmdb_files:\n",
    "\n",
    "                        hmdbproc = pd.read_csv(file) \n",
    "                        if len(hmdbproc)>0:\n",
    "                            for h, row in hmdbproc.iterrows():\n",
    "                                for s, row in SuspectList.iterrows():\n",
    "                                    if not isNaN(hmdbproc['HMDBSMILES'][h]) and hmdbproc['HMDBSMILES'][h] != \" \":\n",
    "                                        if not isNaN(SuspectList['SMILES'][s]) and SuspectList['SMILES'][s] != \" \":\n",
    "                                            LHms2 = [Chem.MolFromSmiles(hmdbproc['HMDBSMILES'][h]), Chem.MolFromSmiles(SuspectList['SMILES'][s])]\n",
    "                                            LHfps2 = [AllChem.GetMorganFingerprintAsBitVect(x2,2, nBits=2048) for x2 in LHms2]\n",
    "                                            LHtn2 = DataStructs.FingerprintSimilarity(LHfps2[0],LHfps2[1])\n",
    "                                            if LHtn2 >= tanimoto:\n",
    "                                                hmdbproc.loc[h, 'SLHsmiles'] = SuspectList['SMILES'][s]\n",
    "                                                hmdbproc.loc[h, 'SLHname'] = SuspectList['Name'][s]\n",
    "                                                hmdbproc.loc[h, 'SLHtanimoto'] = LHtn2\n",
    "\n",
    "\n",
    "                        hmdbproc.to_csv(file)                       \n",
    "                        return(hmdbproc)\n",
    "                        \n",
    "                        \n",
    "            if Source == \"mbank\" or Source == \"specdb\" or Source == \"all\":\n",
    "                \n",
    "                sub_dir = input_dir + \"/\" + entry + '/spectral_dereplication/MassBank/'\n",
    "                if os.path.exists(sub_dir):\n",
    "                    mbank_files = (glob.glob(sub_dir+'/*proc.csv'))\n",
    "                    for file in mbank_files:\n",
    "\n",
    "                        mbankproc = pd.read_csv(file) \n",
    "\n",
    "                        if len(mbankproc)>0:\n",
    "                            for m, row in mbankproc.iterrows():\n",
    "                                for s, row in SuspectList.iterrows():\n",
    "                                    if not isNaN(mbankproc['MBSMILES'][m]) and mbankproc['MBSMILES'][m] != \" \":\n",
    "                                        if not isNaN(SuspectList['SMILES'][s]) and SuspectList['SMILES'][s] != \" \":\n",
    "                                            LHms2 = [Chem.MolFromSmiles(mbankproc['MBSMILES'][m]), Chem.MolFromSmiles(SuspectList['SMILES'][s])]\n",
    "                                            LHfps2 = [AllChem.GetMorganFingerprintAsBitVect(x2,2, nBits=2048) for x2 in LHms2]\n",
    "                                            LHtn2 = DataStructs.FingerprintSimilarity(LHfps2[0],LHfps2[1])\n",
    "                                            if LHtn2 >= tanimoto:\n",
    "                                                mbankproc.loc[m, 'SLMsmiles'] = SuspectList['SMILES'][s]\n",
    "                                                mbankproc.loc[m, 'SLMname'] = SuspectList['Name'][s]\n",
    "                                                mbankproc.loc[m, 'SLMtanimoto'] = LHtn2\n",
    "\n",
    "\n",
    "                        mbankproc.to_csv(file)                       \n",
    "                        return(mbankproc)\n",
    "                            \n",
    "                            \n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "82f0a82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkSMILES_validity(input_dir, resultcsv):\n",
    "    def isNaN(string):\n",
    "        return string != string\n",
    "    \"\"\"checkSMILES_validity does exactly as the name says, using\n",
    "    RDKit, whether the SMILES are invalid or have invalid \n",
    "    chemistry\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "    \n",
    "    results: df from combine_CuratedR\n",
    "    \n",
    "    Returns:\n",
    "    dataframe: with valid SMILES\n",
    "    csv: \"MetabolomicsResults/final_curation_with_validSMILES.csv\"\n",
    "    \n",
    "    Usage:\n",
    "    checkSMILES_validity(input_dir = \"usr/project/\", results)\n",
    "\n",
    "    \"\"\"\n",
    "    results = pd.read_csv(resultcsv)\n",
    "    # check validity of SMILES\n",
    "    for i, row in results.iterrows():\n",
    "        if not isNaN(results['SMILES'][i]):\n",
    "            m = Chem.MolFromSmiles(results['SMILES'][i] ,sanitize=False)\n",
    "            if m is None:\n",
    "                results['SMILES_final'][i] = 'invalid_SMILES'\n",
    "            else:\n",
    "                try:\n",
    "                    Chem.SanitizeMol(m)\n",
    "                except:\n",
    "                    results['SMILES_final'][i] = 'invalid_chemistry'\n",
    "    results.to_csv(input_dir + \"MetabolomicsResults/final_curation_with_validSMILES.csv\")\n",
    "    return(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6e99e6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(input_dir, resultcsv):\n",
    "    def isNaN(string):\n",
    "        return string != string\n",
    "    \"\"\"classification function uses ClassyFire ChemONT\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "    \n",
    "    resultcsv: csv of df from combine_CuratedR or checkSMILES_validity\n",
    "    \n",
    "    Returns:\n",
    "    dataframe: with classification\n",
    "    csv: \"MetabolomicsResults/final_curationList.csv\"\n",
    "    \n",
    "    Usage:\n",
    "    checkSMILES_validity(input_dir = \"usr/project/\", frame)\n",
    "\n",
    "    \"\"\"\n",
    "    frame = pd.read_csv(resultcsv)\n",
    "    inchis = []\n",
    "    for i, row in frame.iterrows():\n",
    "        if not isNaN(frame['SMILES'][i]) and isNaN(frame['Classification_Source'][i]):\n",
    "            try:\n",
    "                InChI = Chem.MolToInchi(Chem.MolFromSmiles(frame[\"SMILES\"][i]))\n",
    "                InChIKey = Chem.inchi.InchiToInchiKey(InChI)\n",
    "                inchis.append({\n",
    "                    'index': i,\n",
    "                    'smiles':frame[\"SMILES\"][i],\n",
    "                    'inchi': InChI,\n",
    "                    'inchikey': InChIKey\n",
    "                })\n",
    "            except:\n",
    "                pass\n",
    "    inchis = pd.DataFrame(inchis)\n",
    "    if len(inchis):\n",
    "        inchis = inchis.loc[-isNaN(inchis['inchikey'])]\n",
    "        ## Retrieve ClassyFire classifications ##\n",
    "\n",
    "        # This first step is done using inchikey and interrogation of the gnps classified structures\n",
    "        gnps_proxy = True \n",
    "        url = \"http://classyfire.wishartlab.com\"\n",
    "        proxy_url =  \"https://gnps-classyfire.ucsd.edu\"\n",
    "        chunk_size = 1000\n",
    "        sleep_interval = 12\n",
    "\n",
    "        all_inchi_keys = list(inchis['inchikey'].drop_duplicates())\n",
    "\n",
    "        resolved_ik_number_list = [0, 0]\n",
    "        total_inchikey_number = len(all_inchi_keys)\n",
    "\n",
    "        while True:\n",
    "\n",
    "            #start_time = time.time()\n",
    "\n",
    "            #print('%s inchikey to resolve' % total_inchikey_number )\n",
    "            get_classifications_cf_mod(all_inchi_keys, par_level = 6)\n",
    "\n",
    "            cleanse('all_json.json', 'all_json.json')\n",
    "\n",
    "            with open(\"all_json.json\") as tweetfile:\n",
    "                jsondic = json.loads(tweetfile.read())\n",
    "\n",
    "            df = json_normalize(jsondic)\n",
    "            df = df.drop_duplicates( 'inchikey' )\n",
    "            resolved_ik_number = len( df.drop_duplicates('inchikey').inchikey )\n",
    "            resolved_ik_number_list.append( resolved_ik_number )\n",
    "            #print('%s resolved inchikeys' % resolved_ik_number )\n",
    "            #print(\"done in --- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "            if resolved_ik_number_list[-1] < resolved_ik_number_list[-2] or resolved_ik_number_list[-1] == resolved_ik_number_list[-3]:\n",
    "                break\n",
    "            cleanse('all_json.json', 'all_json_cleaned.json')\n",
    "\n",
    "            with open(\"all_json_cleaned.json\") as tweetfile:\n",
    "                jsondic = json.loads(tweetfile.read())\n",
    "\n",
    "        flattened_classified_json = json_normalize(jsondic)\n",
    "        flattened_df = flattened_classified_json.drop_duplicates('inchikey')\n",
    "        flattened_df['inchikey'] = flattened_df['inchikey'].str.replace(r'InChIKey=', '')\n",
    "        df_merged = pd.merge(inchis, flattened_df, left_on='inchikey', right_on='inchikey', how='left')\n",
    "\n",
    "        for p, rowp in df_merged.iterrows():\n",
    "            for q, rowq in frame.iterrows():\n",
    "                if df_merged[\"smiles_x\"][p] is frame[\"SMILES\"][q]:\n",
    "                    frame.loc[q, 'subclass'] = df_merged[\"subclass.name\"][p]\n",
    "                    frame.loc[q, 'class'] = df_merged[\"class.name\"][p]\n",
    "                    frame.loc[q, 'superclass'] = df_merged[\"superclass.name\"][p]\n",
    "                    frame.loc[q, 'Classification_Source'] = \"ClassyFire\"\n",
    "\n",
    "\n",
    "\n",
    "        frame.to_csv(input_dir + \"MetabolomicsResults/final_curationList.csv\")\n",
    "        return(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5e1ccf",
   "metadata": {},
   "source": [
    "# Comparison with a list of SMILES from any Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "77bdce92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMILESscreening(input_dir, resultcsv, complist, listname):\n",
    "    def isNaN(string):\n",
    "        return string != string\n",
    "    \"\"\"SMILESscreening takes a list of SMILES\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "    \n",
    "    resultcsv: df from combine_CuratedR or checkSMILES_validity or classification\n",
    "    complist: list of /n separated txt file conyaining smiles on each line\n",
    "    listname: name of the list of compounds\n",
    "    \n",
    "    Returns:\n",
    "    dataframe: comparison with another list of compounds\n",
    "    csv: \"MetabolomicsResults/final_curation_with_validSMILES.csv\"\n",
    "    \n",
    "    Usage:\n",
    "    checkSMILES_validity(input_dir = \"usr/project/\", results)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    results = pd.read_csv(resultcsv)\n",
    "    with open(complist, \"r\") as text_file:\n",
    "        cd = text_file.read().split('\\n')\n",
    "    \n",
    "    for i, row in results.iterrows():\n",
    "        if not isNaN(results['SMILES'][i]):\n",
    "            if 'invalid_SMILES' not in results['SMILES'][i] and 'invalid_chemistry' not in results['SMILES'][i]:\n",
    "                for j in cd:\n",
    "                    if not isNaN(j):\n",
    "                        CGms = [Chem.MolFromSmiles(results['SMILES'][i]), Chem.MolFromSmiles(j)]\n",
    "                        CGfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=1024) for x in CGms]\n",
    "                        CGtn = DataStructs.FingerprintSimilarity(CGfps[0],CGfps[1])\n",
    "                        if CGtn == 1 and listname not in results['Annotation_Source'][i]:\n",
    "                            results['Annotation_Source'][i] = results['Annotation_Source'][i] + ', ' + listname\n",
    "    \n",
    "\n",
    "    frame.to_csv(input_dir + \"MetabolomicsResults/final_curationListVS\"+listname+\".csv\")\n",
    "    return(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44c993e",
   "metadata": {},
   "source": [
    "## NP_Classifier classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "728ebeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Np_pathways(input_dir, resultcsv):\n",
    "    df = pd.read_csv(resultcsv)\n",
    "    npresults = []\n",
    "    for i, row in df.iterrows():\n",
    "        if not isNaN(df['SMILES'][i]):\n",
    "            try:\n",
    "                cvv = Chem.MolFromSmiles(df['SMILES'][i])\n",
    "                cvv = Chem.MolToSmiles(cvv, isomericSmiles = False)\n",
    "                c = urllib.parse.quote_plus(cvv, safe=' ')\n",
    "            \n",
    "                url = 'https://npclassifier.ucsd.edu/classify?smiles='+c\n",
    "                names = str(df['id_X'][i])\n",
    "                outx = str(\"myFile\"+names+\".txt\")\n",
    "                file = wget.download(url, out = outx)\n",
    "                a_dataframe = pd.read_csv(file, delimiter = \"]\")\n",
    "                xox = list(a_dataframe.columns.values)\n",
    "                splitting0 = xox[0].split(':')\n",
    "                xoc = re.sub('\\ |\\[|\\]|\\\"', ' ', splitting0[1]).strip()\n",
    "                splitting1 = xox[1].split(':')\n",
    "                xos = re.sub('\\ |\\[|\\]|\\\"', ' ', splitting1[1]).strip()\n",
    "                #except:\n",
    "                    #splitting1 = xox[1].split(':')\n",
    "                    #xos = re.sub('\\ |\\[|\\]|\\\"', '', splitting1[0])\n",
    "                splitting2 = xox[2].split(':')\n",
    "                xop = re.sub('\\ |\\[|\\]|\\\"', ' ', splitting2[1]).strip()\n",
    "                #df.loc[i, 'npclass'] = xoc\n",
    "                #df.loc[i, 'npsuper_class'] = xos\n",
    "                if not isNaN(df['class'][i]) and df['class'][i] in xoc:\n",
    "                    df.loc[i, 'np_pathway'] = xop\n",
    "                os.remove(outx)\n",
    "                time.sleep(0.5)\n",
    "\n",
    "                npresults.append({\n",
    "                    'index':i,\n",
    "                    #'id': df['file_id'][i],\n",
    "                    'mz': df['premz'][i],\n",
    "                    'rt': df['rtmed'][i],\n",
    "                    'SMILES': df['SMILES'][i],\n",
    "                    'class': xoc,\n",
    "                    'subclass': xos,\n",
    "                    'pathway': xop\n",
    "                })\n",
    "            except:\n",
    "                pass\n",
    "    np_results = pd.DataFrame(npresults)\n",
    "    np_results.to_csv(input_dir + \"/MetabolomicsResults/NPClassifier_Results.csv\")\n",
    "    df.to_csv(input_dir + \"/MetabolomicsResults/final_results_with_Pathways.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc051ea9",
   "metadata": {},
   "source": [
    "## Chemical Similarity MN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "24fa0427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chemMN(input_dir, resultcsv):\n",
    "    #read csv\n",
    "    df = pd.read_csv(resultcsv)\n",
    "    \n",
    "    # define empty variable\n",
    "    dbn= []\n",
    "\n",
    "    # check the result csv\n",
    "    for i, row in df.iterrows():\n",
    "        # to compare each element with each opther element\n",
    "        for j, row in df.iterrows():\n",
    "\n",
    "            # if its not same id\n",
    "            if df['SMILES'][i] != df['SMILES'][j]:\n",
    "\n",
    "                if not isNaN(df['SMILES'][i]):\n",
    "                    if not isNaN(df['SMILES'][j]):\n",
    "\n",
    "                        try:\n",
    "                            ms = [Chem.MolFromSmiles(df['SMILES'][i]), Chem.MolFromSmiles(df['SMILES'][j])]\n",
    "                            fps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in ms]\n",
    "                            tn = DataStructs.FingerprintSimilarity(fps[0],fps[1])\n",
    "                            dbn.append({\n",
    "                                'Name_i':df['id_X'][i],\n",
    "                                'Name_j':df['id_X'][j],\n",
    "                                'i': df['SMILES'][i],\n",
    "                                'j': df['SMILES'][j],\n",
    "                                'Tanimoto': tn\n",
    "                            })\n",
    "                        except Exception as e:\n",
    "                            print(i)\n",
    "                            print(j)\n",
    "                            print(e)\n",
    "    # save chemical similarities                    \n",
    "    db_edgenode = pd.DataFrame(dbn)\n",
    "\n",
    "    dfe = []\n",
    "    heavy_atoms = ['C', 'N', 'P', 'O', 'S']\n",
    "    for i, row in db_edgenode.iterrows():        \n",
    "        if 1.0 > db_edgenode['Tanimoto'][i] >= 0.70:\n",
    "            # list of mol used to calaculate the MCSS\n",
    "            n = [Chem.MolFromSmiles(db_edgenode['i'][i]),Chem.MolFromSmiles(db_edgenode['j'][i])]\n",
    "            res = rdFMCS.FindMCS(n)\n",
    "            sm_res = Chem.MolToSmiles(Chem.MolFromSmarts(res.smartsString))\n",
    "            # Check if the MCSS has one of the heavy atoms and whether they are\n",
    "            # more than 3\n",
    "            elem = [ele for ele in heavy_atoms if(ele in sm_res)]\n",
    "            if elem and len(sm_res)>=3:\n",
    "                MCSS_SMILES = Chem.MolToSmiles(Chem.MolFromSmarts(res.smartsString))\n",
    "\n",
    "            dfe.append({\n",
    "                'Start':db_edgenode['Name_i'][i],\n",
    "                'End':db_edgenode['Name_j'][i],\n",
    "                'Tanimoto':db_edgenode['Tanimoto'][i],\n",
    "                'Start_SMILES':db_edgenode['i'][i],\n",
    "                'End_SMILES':db_edgenode['j'][i],\n",
    "                #'Start_Source':db_edgenode['Source_i'][i],\n",
    "                #'End_Source':db_edgenode['Source_j'][i],\n",
    "                'MCSS': MCSS_SMILES\n",
    "            })\n",
    "\n",
    "    df_edge = pd.DataFrame(dfe)\n",
    "    df_edge['Start'] = df_edge['Start'].astype(str)\n",
    "    df_edge['End'] = df_edge['End'].astype(str)\n",
    "    df_edge['sorted_row'] = [sorted([a,b]) for a,b in zip(df_edge.Start,df_edge.End)]\n",
    "    df_edge['sorted_row'] = df_edge['sorted_row'].astype(str)\n",
    "    df_edge.drop_duplicates(subset=['sorted_row'], inplace=True)\n",
    "\n",
    "    nodes= []\n",
    "    for i, row in df.iterrows():\n",
    "        n = df['id_X'][i]\n",
    "        nodes.append({\n",
    "            'nodes':n\n",
    "        })\n",
    "\n",
    "    node= pd.DataFrame(nodes)\n",
    "    \n",
    "    \n",
    "    df_edge.to_csv(input_dir + \"/MetabolomicsResults/ChemMNedges.tsv\", sep='\\t')\n",
    "    node.to_csv(input_dir + \"/MetabolomicsResults/ChemMNnodes.csv\", index = False)\n",
    "\n",
    "    newdf = df_edge\n",
    "    newdf['StartAtt']=np.nan\n",
    "    newdf['EndAtt']=np.nan\n",
    "    for i, row in newdf.iterrows():\n",
    "        for j, row in df.iterrows():\n",
    "            if newdf['Start'][i]==df['id_X'][j]:\n",
    "                newdf.loc[i, 'StartAtt'] = df['class'][j]\n",
    "            if newdf['End'][i]==df['id_X'][j]:\n",
    "                newdf.loc[i, 'EndAtt'] = df['class'][j]\n",
    "    newdf.to_csv(input_dir + \"/MetabolomicsResults/ChemMNcys.tsv\", sep='\\t')\n",
    "    \n",
    "    return(newdf)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2af6da09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_tuples = []\n",
    "#my_list = newdf[\"sorted_row\"]\n",
    "#for i in my_list:\n",
    "    #save_tuples.append(str(i).replace('[','(').replace(']',')'))\n",
    "#save_tuples\n",
    "#list(save_tuples)\n",
    "#G=nx.from_edgelist(save_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede7f7c7",
   "metadata": {},
   "source": [
    "## Molecular Networking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a74e65",
   "metadata": {},
   "source": [
    "## MN with GNPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2e1259db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gnpsMNvsgnpsMAW(input_dir):\n",
    "    def isNaN(string):\n",
    "        return string != string\n",
    "    \"\"\"gnpsMNvsgnpsMAW checks with tanimoto similarity score, whether\n",
    "    results from MAW GNPS and GNPS MN Masst results give same candidate\n",
    "\n",
    "    Parameters:\n",
    "    input_dir = input directory where you have stored the cytoscape file \n",
    "    from GNPS MN results and have exported edge and node tables from cytoscape\n",
    "    These two csv egde and node files must have \"edge\" and \"node\" in their name\n",
    "    \n",
    "    Returns:\n",
    "    GNPS results with cluster index named\n",
    "    GNPS MN results with a confirmation column if MAW detected same candidate, \n",
    "    file named: \n",
    "    \n",
    "    Usage: \n",
    "    gnpsMNvsgnpsMAW(input_dir)\n",
    "    \n",
    "    \"\"\"\n",
    "    # extract files with edges from MN results\n",
    "    GMNfile_edge = [f for f in os.listdir(input_dir) if \"edge\" in f]\n",
    "    # extract files with nodes from MN results\n",
    "    GMNfile_node = [f for f in os.listdir(input_dir) if \"node\" in f]\n",
    "    # read the files\n",
    "    GMNdf_node = pd.read_csv(GMNfile_node[0])\n",
    "    GMNdf_edge = pd.read_csv(GMNfile_edge[0])\n",
    "    \n",
    "    # extract only important columns from both csv files\n",
    "    GMNdf_node = GMNdf_node[['precursor mass', 'RTMean', 'UniqueFileSources', \n",
    "                   'charge', 'cluster index', 'componentindex', \n",
    "                   'Compound_Name', 'Smiles', 'SpectrumID']]\n",
    "    GMNdf_edge = GMNdf_edge[['cosine_score', 'EdgeAnnotation', 'node1', 'node2',\n",
    "                     'mass_difference']]\n",
    "    \n",
    "    # rename node1 to cluster index to merge nodes and edges results from MN\n",
    "    GMNdf_edge = GMNdf_edge.rename(columns={'node1': 'cluster index'})\n",
    "    GMNdf = pd.merge(GMNdf_node, GMNdf_edge, on = \"cluster index\")\n",
    "    \n",
    "    # Read results obtained from scoring_spec, named input_dir/MetabolomicsResults/scoredSpecDB.csv\n",
    "    SDB = pd.read_csv(input_dir + \"/MetabolomicsResults/scoredSpecDB.csv\")\n",
    "    # only keep GNPS resulst and remove other columns\n",
    "    only_GNPS = SDB[SDB['annotation'].str.contains('GNPS')]\n",
    "    only_GNPS = only_GNPS[['id_X', 'premz_x', 'rtmean_x', 'GNPSmax_similarity', \n",
    "                       'GNPSSMILES', 'GNPSspectrumID', 'GNPScompound_name', \n",
    "                       'GNPSmirrorSpec']]\n",
    "    \n",
    "    # from GNPS MAW results and GNPS MN results, calculate how many MAW results are same as MN:\n",
    "    for i, row in only_GNPS.iterrows():\n",
    "        for j, row in GMNdf.iterrows():\n",
    "            if not isNaN(only_GNPS[\"GNPSSMILES\"][i]) and not isNaN(GMNdf[\"Smiles\"][j]):\n",
    "                SKms = [Chem.MolFromSmiles(only_GNPS['GNPSSMILES'][i]), Chem.MolFromSmiles(GMNdf['Smiles'][j])]\n",
    "                SKfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in SKms]\n",
    "                SKtn = DataStructs.FingerprintSimilarity(SKfps[0],SKfps[1])\n",
    "                if SKtn == 1.0:\n",
    "                    GMNdf.loc[j, \"gnps_maw\"] = \"confirmed\"\n",
    "                    only_GNPS.loc[i, \"index_MN_nodes\"] = j\n",
    "                elif SKtn < 1.0 and SKtn < 0.75:\n",
    "                    GMNdf.loc[j, \"gnps_maw\"] = \"similar\"\n",
    "                    only_GNPS.loc[i, \"index_MN_nodes\"] = j\n",
    "    only_GNPS.to_csv(input_dir + \"/MetabolomicsResults/only_GNPS.csv\")\n",
    "    GMNdf.to_csv(input_dir + \"/MetabolomicsResults/GMNdf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a1868fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gnpsMNvsgnpsMAW(input_dir = \"/Users/mahnoorzulfiqar/Downloads/MAW-main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4a8f223c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the compounds in the cluster extracted out also in the same fucntion and store in a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fcd0d043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the MN vs MCSS results to see any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5338fcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a90d1d3",
   "metadata": {},
   "source": [
    "### MN vs MCSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28b1aabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_list = pd.read_csv(\"/Users/mahnoorzulfiqar/OneDriveUNI/MZML/MetabolomicsResults/Final_Candidate_List.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c142e64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_list = final_list.rename(columns = {'SMILES_final':'SMILES'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ec129a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_list.to_csv(\"/Users/mahnoorzulfiqar/OneDriveUNI/MZML/MetabolomicsResults/Final_Candidate_List.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d7daf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mawRpy)",
   "language": "python",
   "name": "mawrpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
