{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d79995f",
   "metadata": {},
   "source": [
    "## SIRIUS_Metfrag_SList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "611cc70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import pubchempy as pcp\n",
    "import numpy as np\n",
    "def isNaN(string):\n",
    "    return string != string\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "from pybatchclassyfire import *\n",
    "import csv \n",
    "import time\n",
    "import json\n",
    "from pandas import json_normalize\n",
    "\n",
    "\n",
    "#import openpyxl\n",
    "import statistics\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42327aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:rdkit:Enabling RDKit 2021.09.4 jupyter extensions\n"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdFMCS\n",
    "from rdkit.Chem import PandasTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd8a7e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure your Smiles entries in the suspect list csv are in a column named \"SMILES\"\n",
    "def slist_metfrag(input_dir, slist_csv):\n",
    "    \"\"\"slist_metfrag is used to create a txt file that contains a list of \n",
    "    InChIKeys. This list is later used by MetFrag to use these compounds \n",
    "    as a Suspect List.\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored. For this \n",
    "    function this directory must contain a csv file that has a column \n",
    "    named \"SMILES\".\n",
    "    \n",
    "    slist_csv (str): This is the csv file that contains a column of \n",
    "    \"SMILES\". Additionally this file can contain other information \n",
    "    about the compounds, but for this function, column of \"SMILES\", \n",
    "    named as \"SMILES\" is necessary.\n",
    "\n",
    "    Returns:\n",
    "    list: list of InChIKeys\n",
    "    txt: a txt file of list of InChIKeys, is stored in input_dir\n",
    "    \n",
    "    Usage:\n",
    "    slist_metfrag(input_dir = \"/user/project/\", slist_csv = \n",
    "    \"suspectlist.csv\")\n",
    "    \n",
    "    \"\"\"\n",
    "    sl = pd.read_csv(slist_csv)\n",
    "    sl_mtfrag= []\n",
    "    for i, rows in sl.iterrows():\n",
    "        mols = Chem.MolFromSmiles(sl['SMILES'][i])\n",
    "        sl.loc[i, 'InChIKey'] = Chem.inchi.MolToInchiKey(mols)\n",
    "        sl_mtfrag.append(sl['InChIKey'][i])\n",
    "    with open((input_dir + 'SLS_metfrag.txt'), 'w') as filehandle:\n",
    "        for listitem in sl_mtfrag:\n",
    "            filehandle.write('%s\\n' % listitem)\n",
    "    return(sl_mtfrag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09b81b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(slist_metfrag.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad461a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cd169d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slist_sirius(input_dir, slist_csv, substring = None):\n",
    "    \n",
    "    \"\"\"slist_sirius is used to create a tsv file that contains a list of \n",
    "    SMILES. The function also runs the sirius command custom db to create\n",
    "    fingerprints for each SMILES in a folder that we by default name as\n",
    "    SL_Frag/. This fingerprints folder is later used by SIRIUS to use \n",
    "    these compounds as a another small list of compounds to match against\n",
    "    the input spectra fingerprints.\n",
    "    Since SIRIUS doesn't take disconnected structure, Multiply charged, \n",
    "    Incorrect syntax, wild card(*) in smiles; this function removes all\n",
    "    such SMILES from the Suspect List.\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored. For this \n",
    "    function this directory must contain a csv file that has a column \n",
    "    named \"SMILES\".\n",
    "    \n",
    "    slist_csv (str): This is the csv file that contains a column of \n",
    "    \"SMILES\". Additionally this file can contain other information \n",
    "    about the compounds, but for this function, column of \"SMILES\", \n",
    "    named as \"SMILES\" is necessary.\n",
    "    \n",
    "    substring (list): provide a list of strings of SMILES that \n",
    "    shouldn't be considered, provide a list even if there is one string\n",
    "    that shouldnt be considered. e.g: \"[Fe+2]\". \n",
    "\n",
    "    Returns:\n",
    "    tsv: a tsv file of list of SMILES, named as SL_Sirius.tsv, is stored \n",
    "    in input_dir\n",
    "    directory: a directory with compound fragmentations will be created \n",
    "    in a folder named SL_Frag/ within the same input_dir\n",
    "    \n",
    "    \n",
    "    Usage:\n",
    "    slist_sirius(\"/user/project/\", \"suspectlist.csv\", \n",
    "    substring = None)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    sl = pd.read_csv(slist_csv)\n",
    "    \n",
    "    # define function to neutralize the charged SMILES\n",
    "    def neutralize_atoms(mol):\n",
    "        \n",
    "        pattern = Chem.MolFromSmarts(\"[+1!h0!$([*]~[-1,-2,-3,-4]),-1!$([*]~[+1,+2,+3,+4])]\")\n",
    "        at_matches = mol.GetSubstructMatches(pattern)\n",
    "        at_matches_list = [y[0] for y in at_matches]\n",
    "        if len(at_matches_list) > 0:\n",
    "            for at_idx in at_matches_list:\n",
    "                atom = mol.GetAtomWithIdx(at_idx)\n",
    "                chg = atom.GetFormalCharge()\n",
    "                hcount = atom.GetTotalNumHs()\n",
    "                atom.SetFormalCharge(0)\n",
    "                atom.SetNumExplicitHs(hcount - chg)\n",
    "                atom.UpdatePropertyCache()\n",
    "        return mol\n",
    "    \n",
    "\n",
    "    for i, row in sl.iterrows():\n",
    "        # remove SMILES with wild card\n",
    "        if \"*\" in sl[\"SMILES\"][i]:\n",
    "            sl = sl.drop(labels = i, axis = 0) \n",
    "    for i, row in sl.iterrows():\n",
    "        # remove SMILES with any string present in the substring\n",
    "        if substring:\n",
    "            if bool([ele for ele in substring if(ele in sl[\"SMILES\"][i])]):\n",
    "                sl = sl.drop(labels = i, axis = 0)\n",
    "    for i, row in sl.iterrows():\n",
    "        if \".\" in sl[\"SMILES\"][i]:\n",
    "            sl.loc[i, \"SMILES\"] = sl[\"SMILES\"][i].split('.')[0]\n",
    "    # Neutralize the charged SMILES\n",
    "    for i, row in sl.iterrows():\n",
    "        if \"+\" in sl[\"SMILES\"][i] or \"-\" in sl[\"SMILES\"][i]:\n",
    "            mol = Chem.MolFromSmiles(sl[\"SMILES\"][i])\n",
    "            neutralize_atoms(mol)\n",
    "            sl.loc[i, \"SMILES\"] = Chem.MolToSmiles(mol)\n",
    "            \n",
    "            # Remove multiple charged SMILES\n",
    "            if \"+\" in sl[\"SMILES\"][i] or \"-\" in sl[\"SMILES\"][i]:\n",
    "                pos = sl[\"SMILES\"][i].count('+')\n",
    "                neg = sl[\"SMILES\"][i].count('-')\n",
    "                charge = pos + neg \n",
    "                if charge > 1:\n",
    "                    sl = sl.drop(labels = i, axis = 0) \n",
    "                    \n",
    "    slsirius = pd.DataFrame({'smiles':sl[\"SMILES\"]})\n",
    "    slsirius.to_csv(input_dir+ \"SL_Sirius.tsv\", sep = \"\\t\", header = False, index = False)\n",
    "    os.system(\"sirius --input \" + input_dir + \"SL_Sirius.tsv custom-db --name=SL_Frag --output \"+ input_dir)\n",
    "# Usage:\n",
    "# slist_sirius(\"/Users/user/project/SuspectList/\", \"SUSPECT_LIST.csv\", substring = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ef42aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(slist_sirius.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc7a19e",
   "metadata": {},
   "source": [
    "### SIRIUS post processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e00a88",
   "metadata": {},
   "source": [
    "### SIRIUS Result Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e047124f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sirius_postProc2(input_dir, input_table, slistcsv ,sl = True):\n",
    "    \n",
    "    \n",
    "    \"\"\"sirius_postProc2 is the second part of the function \n",
    "    sirius_postProc defined in R part of the workflow. This function\n",
    "    re-checks the Suspect list, if present or given as a parameter, \n",
    "    whether the candidates have a high similarity with compounds in\n",
    "    Suspect List. It also calculates the Maximum Common Substructure\n",
    "    (MCSS)\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored. For this \n",
    "    function this directory must contain a csv file that has a column \n",
    "    named \"SMILES\".\n",
    "    \n",
    "    input_table (str): This is the table in csv format (defined in R), \n",
    "    which stores a csv table containing columns \"mzml_files\", which \n",
    "    contains liat of all input files with their relative paths, second\n",
    "    column is \"ResultFileName\" which is a list of the corresponding\n",
    "    result relative directories to each mzml files. Lastly, \"file_id\", \n",
    "    contains a file directory. This table will be used to read the \n",
    "    SIRIUS json files\n",
    "    \n",
    "    sl (bool): True if a suspct list is to be used\n",
    "    \n",
    "    slistcsv (list): This is the csv file that contains a column of \n",
    "    \"SMILES\". Additionally this file can contain other information \n",
    "    about the compounds, but for this function, column of \"SMILES\", \n",
    "    named as \"SMILES\" is necessary.\n",
    "\n",
    "    Returns:\n",
    "    csv: a result file with additional columns such as those for suspect\n",
    "    list if one is used. It also adds columns on MCSS., named as \n",
    "    \"input_dir/ResultFileName/insilico/SiriusResults.csv\"\n",
    "    \n",
    "    \n",
    "    Usage:\n",
    "    sirius_postProc2(input_dir = \"/user/project/\", \n",
    "    input_table = \"/user/project/suspectlist.csv\", sl = True, slistcsv)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Describe the heavy atoms to be considered for MCSS\n",
    "    heavy_atoms = ['C', 'N', 'P', 'O', 'S']\n",
    "    \n",
    "    \n",
    "    for m, row in input_table.iterrows():\n",
    "        \n",
    "        # Read the file result_dir/insilico/MS1DATAsirius.csv. \n",
    "        # This file has been produced in R workflow and contains \n",
    "        # SIRIUS results.\n",
    "\n",
    "        file1 = pd.read_csv(input_dir + (input_table['ResultFileNames'][m] + '/insilico/MS1DATAsirius.csv').replace(\"./\", \"\"))\n",
    "        \n",
    "        for i, row in file1.iterrows():\n",
    "            \n",
    "            # if the entry has SMILES extracted for MCSS calculation\n",
    "            if not isNaN(file1['SMILESforMCSS'][i]):\n",
    "                \n",
    "                # split the SMILES using |\n",
    "                top_smiles = file1['SMILESforMCSS'][i].split(\"|\")\n",
    "                \n",
    "                # if there are more than 1 smiles in the top smiles, \n",
    "                if len(top_smiles) > 1:\n",
    "                    \n",
    "                    # define empty mol list to add mol objects from the top Smiles\n",
    "                    mol = []\n",
    "                    \n",
    "                    # is sl = True\n",
    "                    if sl:\n",
    "                        \n",
    "                        # Add columns \n",
    "                        file1['Top_can_SL'] = np.nan # top candidate among the top 5 candidates, according to similarity with a compound in suspect list\n",
    "                        file1['tanimotoSLvsCAN'] = np.nan # tanimoto score\n",
    "                        file1['SL_comp'] = np.nan # Smiles of the suspect listr compund with  high similairity with the one of the top 5 candidates\n",
    "                        \n",
    "                        # read the suspect list\n",
    "                        slist = pd.read_csv(slistcsv)\n",
    "                        \n",
    "                        for j in top_smiles:\n",
    "                            \n",
    "                            for k, row in slist.iterrows():\n",
    "                                \n",
    "                                # calculate the tanimoto between SMILES in top smiles and the suspect list\n",
    "                                SSms = [Chem.MolFromSmiles(j), Chem.MolFromSmiles(slist['SMILES'][k])]\n",
    "                                SSfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in SSms]\n",
    "                                SStn = DataStructs.FingerprintSimilarity(SSfps[0],SSfps[1])\n",
    "                                \n",
    "                                # if there is high similarity, keep that entity in the following columns to consider later\n",
    "                                if SStn >= 0.8:\n",
    "                                    file1.loc[i,'Top_can_SL'] = j\n",
    "                                    file1.loc[i,'tanimotoSLvsCAN'] = SStn\n",
    "                                    file1.loc[i,'SL_comp'] = slist['SMILES'][k]\n",
    "                            # calculate the mol object from each smiles\n",
    "                            mol_object = Chem.MolFromSmiles(j)\n",
    "                            # add all these mol objects to mol\n",
    "                            mol.append(mol_object)\n",
    "                    # list of mol used to calaculate the MCSS\n",
    "                    res = rdFMCS.FindMCS(mol)\n",
    "                    sm_res = Chem.MolToSmiles(Chem.MolFromSmarts(res.smartsString))\n",
    "                    \n",
    "                    # Check if the MCSS has one of the heavy atoms and whether they are\n",
    "                    # more than 3\n",
    "                    elem = [ele for ele in heavy_atoms if(ele in sm_res)]\n",
    "                    if elem and len(sm_res)>=3:\n",
    "                        file1.loc[i, 'MCSSstring'] = res.smartsString\n",
    "                        file1.loc[i, 'MCSS_SMILES'] = Chem.MolToSmiles(Chem.MolFromSmarts(res.smartsString))\n",
    "        file1.to_csv(input_table['ResultFileNames'][m] + '/insilico/SiriusResults.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b65479b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sirius_postProc2.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653e9b79",
   "metadata": {},
   "source": [
    "### MetFrag Result Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a01c9220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metfrag_postproc(input_dir, input_table, slistcsv ,sl = True):\n",
    "    \n",
    "    \n",
    "    \"\"\"metfrag_postproc function re-checks the Suspect list, if present \n",
    "    or given as a parameter, whether the candidates have a high \n",
    "    similarity with compounds in Suspect List. It also calculates the \n",
    "    Maximum Common Substructure (MCSS). This function adds top candidates\n",
    "    from PubChem and KEGG as these two databases are used with MetFrag\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored. For this \n",
    "    function this directory must contain a csv file that has a column \n",
    "    named \"SMILES\".\n",
    "    \n",
    "    input_table (str): This is the table in csv format (defined in R), \n",
    "    which stores a csv table containing columns \"mzml_files\", which \n",
    "    contains liat of all input files with their relative paths, second\n",
    "    column is \"ResultFileName\" which is a list of the corresponding\n",
    "    result relative directories to each mzml files. Lastly, \"file_id\", \n",
    "    contains a file directory. This table will be used to read the \n",
    "    MetFrag csv files\n",
    "    \n",
    "    sl (bool): True if a suspct list is to be used\n",
    "    \n",
    "    slistcsv (list): This is the csv file that contains a column of \n",
    "    \"SMILES\". Additionally this file can contain other information \n",
    "    about the compounds, but for this function, column of \"SMILES\", \n",
    "    named as \"SMILES\" is necessary.\n",
    "\n",
    "    Returns:\n",
    "    csv: a result file with additional columns such as those for suspect\n",
    "    list if one is used. It also adds columns on MCSS., named as \n",
    "    \"input_dir/ResultFileName/insilico/MetFragResults.csv\". It \n",
    "    contains columns for KEGG and PubChem\n",
    "    \n",
    "    \n",
    "    Usage:\n",
    "    metfrag_postproc(input_dir = \"/user/project/\", \n",
    "    input_table = \"/user/project/suspectlist.csv\", sl = True, slistcsv)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Describe the heavy atoms to be considered for MCSS\n",
    "    heavy_atoms = ['C', 'N', 'P', 'O', 'S']\n",
    "    \n",
    "    \n",
    "    for m, row in input_table.iterrows():\n",
    "    \n",
    "        # Result directory\n",
    "        result = input_dir + (input_table['ResultFileNames'][m] + \n",
    "                                 '/insilico/MetFrag').replace(\"./\", \"\")\n",
    "\n",
    "        # list of all the csv files in the result directory result_dir/inislico/MetFrag/\n",
    "        files_met = (glob.glob(result+'/*.csv'))\n",
    "\n",
    "        # read the csv file that contains all the features from the input .mzml file\n",
    "        file1  = pd.read_csv(input_dir + (input_table['ResultFileNames'][m] + '/insilico/MS1DATA.csv').replace(\"./\", \"\"))\n",
    "    \n",
    "        # for each feature in the MS1DATA.csv file\n",
    "        for i, row in file1.iterrows():\n",
    "        \n",
    "            # take id as a pattern to differentiate between different ids\n",
    "            pattern = file1.loc[i, \"id_X\"]\n",
    "        \n",
    "            #check which of the csv result files have the same pattern in their names\n",
    "            results = [i for i in files_met if pattern in i]\n",
    "        \n",
    "            # find which of the files with that id have KEGG in their names,\n",
    "            KEGG = [i for i in results if \"KEGG\" in i]\n",
    "        \n",
    "            # if kegg present in the name\n",
    "            if KEGG:\n",
    "            \n",
    "                # read the KEGG csv file for that feature\n",
    "                KEGG_file = pd.read_csv((KEGG)[0])\n",
    "            \n",
    "                # if the KEGG file isn't empty\n",
    "                if len(KEGG_file)>0:\n",
    "                \n",
    "                    # extract only the columns with >0.75 score\n",
    "                    KEGG_file = KEGG_file.drop(KEGG_file[KEGG_file.Score < 0.75].index)\n",
    "                \n",
    "                    # add the relavnt information to the original MS1DATA csv\n",
    "                    file1.loc[i, 'KG_ID'] = KEGG_file.loc[0, 'Identifier']\n",
    "                    file1.loc[i, 'KG_Name'] = KEGG_file.loc[0, 'CompoundName']\n",
    "                    file1.loc[i, 'KG_Formula'] = KEGG_file.loc[0, 'MolecularFormula']\n",
    "                    file1.loc[i, 'KG_expPeaks'] = KEGG_file.loc[0, 'NoExplPeaks']\n",
    "                    file1.loc[i, 'KG_SMILES'] = Chem.MolToSmiles(Chem.MolFromInchi(KEGG_file[\"InChI\"][0]))\n",
    "                    file1.loc[i, 'KG_file'] = KEGG\n",
    "                \n",
    "                    #create empty list of KEGG top smiles\n",
    "                    Kegg_smiles = []\n",
    "                \n",
    "                    # extract only the InChI of the top 5\n",
    "                    for j in KEGG_file[\"InChI\"][0:5].tolist():\n",
    "                        # convert the InChI to SMILES\n",
    "                        mol = Chem.MolToSmiles(Chem.MolFromInchi(j))\n",
    "                        if sl:\n",
    "                            # read the suspect list\n",
    "                            slist = pd.read_csv(slistcsv)\n",
    "                            \n",
    "                            # Add columns \n",
    "                            file1['KG_Top_can_SL'] = np.nan # top candidate among the top 5 candidates, according to similarity with a compound in suspect list\n",
    "                            file1['KG_tanimotoSLvsCAN'] = np.nan # tanimoto score\n",
    "                            file1['KG_SL_comp'] = np.nan # Smiles of the suspect listr compund with  high similairity with the one of the top 5 candidates\n",
    "                            \n",
    "                            # for each smiles in suspect list\n",
    "                            for k, row in slist.iterrows():\n",
    "                                # Calculate the tanimoto score\n",
    "                                SSms = [Chem.MolFromSmiles(mol), Chem.MolFromSmiles(slist['SMILES'][k])]\n",
    "                                SSfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in SSms]\n",
    "                                SStn = DataStructs.FingerprintSimilarity(SSfps[0],SSfps[1])\n",
    "                                if SStn >= 0.8:\n",
    "                                    file1.loc[i, 'KG_Top_can_SL'] = j\n",
    "                                    file1.loc[i, 'KG_tanimotoSLvsCAN'] = SStn\n",
    "                                    file1.loc[i, 'KG_SL_comp'] = slist['SMILES'][k]\n",
    "                        mol2 = Chem.MolFromSmiles(mol)\n",
    "                        Kegg_smiles.append(mol2)\n",
    "                    # if there are more than 1 top smiles\n",
    "                    if len(Kegg_smiles) > 1:\n",
    "                        #calculate the MCSS\n",
    "                        res = rdFMCS.FindMCS(Kegg_smiles)\n",
    "                        sm_res = Chem.MolToSmiles(Chem.MolFromSmarts(res.smartsString))\n",
    "                        # if there are atleast 3 heavy atoms in the MCSS, then add it to the result file\n",
    "                        elem = [ele for ele in heavy_atoms if(ele in sm_res)]\n",
    "                        if elem and len(sm_res)>=3:\n",
    "                            file1.loc[i, 'KG_MCSSstring'] = res.smartsString\n",
    "                            file1.loc[i, 'KG_MCSS_SMILES'] = Chem.MolToSmiles(Chem.MolFromSmarts(res.smartsString))\n",
    "                            \n",
    "            #start here for PubChem; find which of the files with that id have PubChem in their names,\n",
    "            PubChem = [i for i in results if \"PubChem\" in i]\n",
    "            \n",
    "            if PubChem:\n",
    "\n",
    "                PubChem_file = pd.read_csv(PubChem[0])\n",
    "                \n",
    "                # if more candidates\n",
    "                if len(PubChem_file)>0:\n",
    "                    \n",
    "                    # take the ones with more than 0.75 score\n",
    "                    PubChem_file = PubChem_file.drop(PubChem_file[PubChem_file.Score < 0.75].index)\n",
    "                    \n",
    "                    # add the relavnt information to the original MS1DATA csv\n",
    "                    file1.loc[i, 'PC_ID'] = PubChem_file.loc[0, 'Identifier']\n",
    "                    file1.loc[i, 'PC_Name'] = PubChem_file.loc[0, 'IUPACName']\n",
    "                    file1.loc[i, 'PC_Formula'] = PubChem_file.loc[0, 'MolecularFormula']\n",
    "                    file1.loc[i, 'PC_expPeaks'] = PubChem_file.loc[0, 'NoExplPeaks']\n",
    "                    file1.loc[i, 'PC_SMILES'] = PubChem_file[\"SMILES\"][0]\n",
    "                    file1.loc[i, 'PC_file'] = PubChem\n",
    "                    \n",
    "                    # empty object\n",
    "                    Pubchem_smiles = []\n",
    "                    \n",
    "                    # extract only the SMILES of the top 5\n",
    "                    for j in PubChem_file[\"SMILES\"][0:5].tolist():\n",
    "                        \n",
    "                        # if sl = True\n",
    "                        if sl:\n",
    "                            \n",
    "                            # read the suspect list\n",
    "                            slist = pd.read_csv(slistcsv)\n",
    "                            \n",
    "                            # Add columns \n",
    "                            file1['PC_Top_can_SL'] = np.nan # top candidate among the top 5 candidates, according to similarity with a compound in suspect list\n",
    "                            file1['PC_tanimotoSLvsCAN'] = np.nan # tanimoto score\n",
    "                            file1['PC_SL_comp'] = np.nan # Smiles of the suspect listr compund with  high similairity with the one of the top 5 candidates\n",
    "                            # calculate tanimoto\n",
    "                            for n, row in slist.iterrows():\n",
    "                                \n",
    "                                SSms = [Chem.MolFromSmiles(j), Chem.MolFromSmiles(slist['SMILES'][n])]\n",
    "                                SSfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in SSms]\n",
    "                                SStn2 = DataStructs.FingerprintSimilarity(SSfps[0],SSfps[1])\n",
    "                                \n",
    "                                if SStn2 >= 0.8:\n",
    "                                    file1.loc[i, 'PC_Top_can_SL'] = j\n",
    "                                    file1.loc[i, 'PC_tanimotoSLvsCAN'] = SStn2\n",
    "                                    file1.loc[i, 'PC_SL_comp'] = slist['SMILES'][n]\n",
    "                                    \n",
    "                        # Concert smiles to mol\n",
    "                        sm2 = Chem.MolFromSmiles(j)\n",
    "                        # store mol in Pubchem_smiles\n",
    "                        Pubchem_smiles.append(sm2)\n",
    "                    \n",
    "                    if len(Pubchem_smiles) > 1:\n",
    "                        # calculate MCSS\n",
    "                        res2 = rdFMCS.FindMCS(Pubchem_smiles)\n",
    "                        sm_res = Chem.MolToSmiles(Chem.MolFromSmarts(res2.smartsString))\n",
    "                        # If atleast 3 heavy atoms present\n",
    "                        elem = [ele for ele in heavy_atoms if(ele in sm_res)]\n",
    "                        if elem and len(sm_res)>=3:\n",
    "                            file1.loc[i, 'PC_MCSSstring']= res2.smartsString\n",
    "                            file1.loc[i, 'PC_MCSS_SMILES'] = Chem.MolToSmiles(Chem.MolFromSmarts(res2.smartsString))\n",
    "        file1.to_csv(input_table['ResultFileNames'][m] + '/insilico/MetFragResults.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7312134b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(metfrag_postproc.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80fae2f",
   "metadata": {},
   "source": [
    "### COMBINE IN SILICO -All files with SIRIUS results separate and with MetFragresults separate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a14bbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_insilico(input_dir, input_table, Source = \"SIRIUS\"):\n",
    "    \n",
    "    \"\"\"combine_insilico function combines the Sirius results from all\n",
    "    result directories for each input mzml file. It does same for \n",
    "    Metfrag.\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "    \n",
    "    input_table (str): This is the table in csv format (defined in R), \n",
    "    which stores a csv table containing columns \"mzml_files\", which \n",
    "    contains liat of all input files with their relative paths, second\n",
    "    column is \"ResultFileName\" which is a list of the corresponding\n",
    "    result relative directories to each mzml files. Lastly, \"file_id\", \n",
    "    contains a file directory. This table will be used to read the \n",
    "    Sirius and MetFrag result csv files\n",
    "    \n",
    "    Source (str): either \"SIRIUS\" or \"MetFrag\"\n",
    "\n",
    "    Returns:\n",
    "    \n",
    "    dataframe: of combined SIRIUS/MetFrag results\n",
    "    \n",
    "    csv: stores the dataframe in a csv, named as \n",
    "    \"input_dir/ResultFileName/MetabolomicsResults/SIRIUS_combined.csv\" \n",
    "    OR/AND \n",
    "    \"input_dir/ResultFileName/MetabolomicsResults/MetFrag_combined.csv\"\n",
    "    \n",
    "    \n",
    "    Usage:\n",
    "    combine_insilico(input_dir = \"/user/project/\", \n",
    "    input_table = \"/user/project/suspectlist.csv\", Source = \"SIRIUS\")\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # create a new directory to store all results /MetabolomicsResults/\n",
    "    path = os.path.join(input_dir, \"MetabolomicsResults\")\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)    \n",
    "    # if Sirius results are to be combined\n",
    "    if Source == \"SIRIUS\":\n",
    "        \n",
    "        # store all files paths here\n",
    "        all_files = []\n",
    "        for n, row in input_table.iterrows():\n",
    "            all_files.append(input_dir + input_table['ResultFileNames'][n].replace(\"./\", \"\") + '/insilico/SiriusResults.csv')\n",
    "        \n",
    "        # store all dataframes of the results here\n",
    "        li = []\n",
    "    \n",
    "        for filename in all_files:\n",
    "            df = pd.read_csv(filename, index_col=None, header=0)\n",
    "            df[\"ResultFileNames\"] = filename\n",
    "            li.append(df)\n",
    "            \n",
    "        # join all resulst dataframe\n",
    "        frame = pd.concat(li, axis=0, ignore_index=True)\n",
    "        \n",
    "\n",
    "        for i, row in frame.iterrows():\n",
    "            if frame[\"FormulaRank\"][i] == 1.0:\n",
    "                sep = 'json/'\n",
    "                strpd = frame[\"dir\"][i].split(sep, 1)[0] +\"json/canopus_summary.tsv\"\n",
    "                if os.path.isfile(strpd):\n",
    "\n",
    "                    canopus = pd.read_csv(strpd, sep='\\t')\n",
    "                    if len(canopus) > 0:\n",
    "                        frame.loc[i, 'most_specific_class'] = canopus[\"most specific class\"][0]\n",
    "                        frame.loc[i, 'level _5'] = canopus[\"level 5\"][0]\n",
    "                        frame.loc[i, 'subclass'] = canopus[\"subclass\"][0]\n",
    "                        frame.loc[i, 'class'] = canopus[\"class\"][0]\n",
    "                        frame.loc[i, 'superclass'] = canopus[\"superclass\"][0]\n",
    "                        frame.loc[i, 'all_classifications'] = canopus[\"all classifications\"][0]\n",
    "                        frame.loc[i, 'Classification_Source'] = 'CANOPUS'\n",
    "        frame.to_csv(input_dir + '/MetabolomicsResults/SIRIUS_combined.csv')\n",
    "        return(frame)\n",
    "    \n",
    "    # if MetFrag results are to be combined\n",
    "    elif Source == \"MetFrag\":\n",
    "        \n",
    "        # store all files paths here\n",
    "        all_files = []\n",
    "        for m, row in input_table.iterrows():\n",
    "            all_files.append(input_dir + input_table['ResultFileNames'][m].replace(\"./\", \"\") + '/insilico/MetFragResults.csv')\n",
    "        li = []\n",
    "\n",
    "        for filename in all_files:\n",
    "            df = pd.read_csv(filename, index_col=None, header=0)\n",
    "            df[\"result_dir\"] = filename\n",
    "            li.append(df)\n",
    "\n",
    "        frame = pd.concat(li, axis=0, ignore_index=True)\n",
    "        frame.to_csv(input_dir+'MetabolomicsResults/MetFrag_combined.csv')\n",
    "        return(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16c4edfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(combine_insilico.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aef0fd6",
   "metadata": {},
   "source": [
    "# Spectral DB Dereplication Results Post Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf3d8c1",
   "metadata": {},
   "source": [
    "### GNPS, MassBank and HMDB Results post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51160acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spec_postproc(input_dir, Source = \"all\"):\n",
    "    \n",
    "    \"\"\"spec_postproc function processes the resulst from dereplication \n",
    "    using different spectral DBs. \n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "    \n",
    "    Source (str): either \"mbank\" or \"hmdb\" or \"gnps\", or \"all\"\n",
    "\n",
    "    Returns:\n",
    "    \n",
    "    dataframe: of the paths of the processed DB results\n",
    "    \n",
    "    \n",
    "    Usage:\n",
    "    spec_postproc(input_dir = \"/user/project/\", Source = \"all\")\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # empty lists of csv files paths for each database\n",
    "    GNPScsvfiles = []\n",
    "    HMDBcsvfiles = []\n",
    "    MassBankcsvfiles = []\n",
    "    \n",
    "    #list all files and directories\n",
    "    for entry in os.listdir(input_dir):\n",
    "        if os.path.isdir(os.path.join(input_dir, entry)):\n",
    "            \n",
    "            # enter the directory with /spectral_dereplication/ results\n",
    "            sub_dir = input_dir + entry + '/spectral_dereplication'\n",
    "            if os.path.exists(sub_dir):\n",
    "                files = (glob.glob(sub_dir+'/*.csv'))\n",
    "\n",
    "                for f in files:\n",
    "                    if 'gnps.' in f: \n",
    "                        GNPScsvfiles.append(f)\n",
    "                    if 'hmdb.' in f: \n",
    "                        HMDBcsvfiles.append(f)\n",
    "                    if 'mbank.' in f: \n",
    "                        MassBankcsvfiles.append(f)\n",
    "                            \n",
    "    \n",
    "    if Source == \"hmdb\" or \"all\":\n",
    "        #download SDF structures\n",
    "        #os.system(\"wget https://hmdb.ca/system/downloads/current/structures.zip\")\n",
    "        #os.system(\"unzip \"+ input_dir + \"structures.zip\")\n",
    "            \n",
    "        # Load the sdf\n",
    "        dframe = PandasTools.LoadSDF((input_dir+\"structures.sdf\"),\n",
    "                                     idName='HMDB_ID',smilesName='SMILES',\n",
    "                                     molColName='Molecule', includeFingerprints=False)\n",
    "        \n",
    "        #### read sdf file from HMDB to collect names and smiles ####\n",
    "    \n",
    "        #HMDB CSV Result file pre_processing\n",
    "        \n",
    "        #open another csv path holding empty list, which will be filled \n",
    "        #with post processed csv results\n",
    "        HMDBcsvfiles2 = []\n",
    "        \n",
    "        for k in HMDBcsvfiles:\n",
    "            \n",
    "            # read the csv files\n",
    "            hmdb_df = pd.read_csv(k)\n",
    "            \n",
    "            # merge on basis of id, frame and hmdb result files\n",
    "            SmilesHM = pd.merge(hmdb_df, dframe, left_on=hmdb_df.HMDBcompoundID, right_on=dframe.DATABASE_ID)\n",
    "            \n",
    "            \n",
    "            for i, row in hmdb_df.iterrows():\n",
    "                \n",
    "                for j, row in SmilesHM.iterrows():\n",
    "                    \n",
    "                    # where index for both match, add the name and SMILES\n",
    "                    if hmdb_df['id_X'][i]== SmilesHM['id_X'][j]:\n",
    "                        hmdb_df.loc[i, 'HMDBSMILES'] = SmilesHM['SMILES'][j]#add SMILES\n",
    "                        hmdb_df.loc[i, 'HMDBcompound_name'] = SmilesHM[\"GENERIC_NAME\"][j]#add name\n",
    "                        hmdb_df.loc[i, 'HMDBformula'] = SmilesHM[\"FORMULA\"][j]#add formula\n",
    "                \n",
    "            csvname = (os.path.splitext(k)[0])+\"proc\"+\".csv\" # name for writing it in a new file\n",
    "            hmdb_df.to_csv(csvname) #write\n",
    "            HMDBcsvfiles2.append(csvname)# add to a list\n",
    "        \n",
    "    #MassBank CSV Result file pre_processing\n",
    "    \n",
    "    if Source == \"mbank\" or \"all\":\n",
    "        \n",
    "        #open another csv path holding empty list, which will be filled \n",
    "        #with post processed csv results\n",
    "        MassBankcsvfiles2 = []\n",
    "        \n",
    "        for l in MassBankcsvfiles:\n",
    "            \n",
    "            # read mbank csv file\n",
    "            mbank_df = pd.read_csv(l)\n",
    "            \n",
    "            for i, row in mbank_df.iterrows():\n",
    "                \n",
    "                inchiK = str(mbank_df[\"MBinchiKEY\"][i])\n",
    "                \n",
    "                #extract inchikeys\n",
    "                y = pcp.get_compounds(inchiK, 'inchikey')#compound based on inchikey\n",
    "                \n",
    "                for compound in y:\n",
    "                    \n",
    "                    #add smiles\n",
    "                    smles = compound.isomeric_smiles   \n",
    "                    mbank_df.loc[i, 'MBSMILES'] = smles\n",
    "                    \n",
    "            csvname = (os.path.splitext(l)[0])+\"proc\"+\".csv\"\n",
    "            mbank_df.to_csv(csvname)\n",
    "            MassBankcsvfiles2.append(csvname)\n",
    "    \n",
    "    # GNPS CSV Result file pre_processing\n",
    "    if Source == \"gnps\" or \"all\":\n",
    "        \n",
    "        #currently only these subsets are removed from the names from GNPS\n",
    "        matches = [\"M+\",\"[M\", \"M-\", \"2M\", \"M*\" \"20.0\", \"50.0\", \"30.0\", \"40.0\", \"60.0\", \"70.0\", \"eV\", \"Massbank\"\n",
    "               , \"Spectral\", \"Match\", \"to\", \"from\", \"NIST14\", \"MoNA\", '[IIN-based:',  '[IIN-based', 'on:', 'CCMSLIB00003136269]']\n",
    "        \n",
    "        #open another csv path holding empty list, which will be filled \n",
    "        #with post processed csv results\n",
    "        GNPScsvfiles2 = []\n",
    "        \n",
    "        for l in GNPScsvfiles:\n",
    "            \n",
    "            gnps_df = pd.read_csv(l)\n",
    "    \n",
    "            for i, row in gnps_df.iterrows():\n",
    "            \n",
    "                # if compound name is present\n",
    "                if not isNaN(gnps_df['GNPScompound_name'][i]):\n",
    "                    \n",
    "                    # split if there is a gap in the names\n",
    "                    string_chng = (gnps_df['GNPScompound_name'][i].split(\" \"))\n",
    "                    \n",
    "                    # create an empty list\n",
    "                    newstr = []\n",
    "                    \n",
    "                    # for each part of the string in the names\n",
    "                    chng = []\n",
    "                    \n",
    "                    for j in range(len(string_chng)):\n",
    "                        \n",
    "                        # check if the substrings re present in the matches and no - is present\n",
    "                        if not any(x in string_chng[j] for x in matches) and not '-' == string_chng[j]:\n",
    "                            \n",
    "                            # IF | and ! not in the substring\n",
    "                            if '|' not in string_chng[j] or '!' not in string_chng[j]:\n",
    "                                newstr.append(string_chng[j])\n",
    "                                \n",
    "                            # if | present in the substring   \n",
    "                            elif '|' in string_chng[j]:\n",
    "                                \n",
    "                                #split the string\n",
    "                                jlen = string_chng[j].split(\"|\")\n",
    "                                #how many substrings are left now\n",
    "                                lst = len(jlen)-1\n",
    "                                #append this to chng\n",
    "                                chng.append(jlen[lst])\n",
    "                                break\n",
    "                                \n",
    "                    # now append chng to newstr            \n",
    "                    chng.append(' '.join(newstr))\n",
    "                    #save this as the correct name\n",
    "                    gnps_df.loc[i, \"corr_names\"] = chng[0]\n",
    "                    \n",
    "                    if not isNaN(gnps_df['GNPSSMILES'][i]):\n",
    "                        if chng == '':\n",
    "                            break\n",
    "                        elif gnps_df['GNPSSMILES'][i].isalpha():\n",
    "                            s = pcp.get_compounds(chng[0], 'name')\n",
    "                            if s:\n",
    "                                for comp in s:\n",
    "                                    gnps_df[\"GNPSSMILES\"][i] = comp.isomeric_smiles\n",
    "                            else:\n",
    "                                gnps_df[\"GNPSSMILES\"][i] = ''\n",
    "                else:\n",
    "                    gnps_df[\"GNPSSMILES\"][i] = ''\n",
    "                    \n",
    "            for i, row in gnps_df.iterrows():\n",
    "                if not isNaN(gnps_df['GNPSSMILES'][i]):\n",
    "                    try:\n",
    "                        sx = pcp.get_compounds(gnps_df['GNPSSMILES'][i], 'smiles')\n",
    "                        if sx:\n",
    "                            sx = str(sx)\n",
    "                            comp = pcp.Compound.from_cid([int(x) for x in re.findall(r'\\b\\d+\\b', sx)])\n",
    "                            gnps_df.loc[i, 'GNPSformula'] = comp.molecular_formula\n",
    "                    except:\n",
    "                        gnps_df.loc[i, 'GNPSformula'] = ''\n",
    "                        \n",
    "            csvname = (os.path.splitext(l)[0])+\"_with_cor_names\"+\".csv\"\n",
    "            gnps_df.to_csv(csvname)\n",
    "            GNPScsvfiles2.append(csvname)\n",
    "    \n",
    "\n",
    "    if Source == \"all\":\n",
    "        \n",
    "        dict1 = {'GNPSr': GNPScsvfiles2, 'HMDBr': HMDBcsvfiles2, 'MBr': MassBankcsvfiles2} \n",
    "        df = pd.DataFrame(dict1)\n",
    "\n",
    "        return(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8889acf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(spec_postproc.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6e281b",
   "metadata": {},
   "source": [
    "### Combine_all Spectral DBs for one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60009629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_specdb(input_dir):\n",
    "    \n",
    "    \"\"\"combine_specdb function combines all results from different\n",
    "    spectral dbs. Can only be used if more than one db is used \n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "\n",
    "    Returns:\n",
    "    dataframe: of the paths of the merged results\n",
    "    \n",
    "    \n",
    "    Usage:\n",
    "    combine_specdb(input_dir)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # empty lists of csv files paths for each database\n",
    "    GNPScsvfiles2 = []\n",
    "    HMDBcsvfiles2 = []\n",
    "    MassBankcsvfiles2 = []\n",
    "    \n",
    "    #list all files and directories\n",
    "    for entry in os.listdir(input_dir):\n",
    "        if os.path.isdir(os.path.join(input_dir, entry)):\n",
    "            \n",
    "            # enter the directory with /spectral_dereplication/ results\n",
    "            sub_dir = input_dir + entry + '/spectral_dereplication'\n",
    "            if os.path.exists(sub_dir):\n",
    "                files = (glob.glob(sub_dir+'/*.csv'))\n",
    "\n",
    "                for f in files:\n",
    "                    if 'gnps_with_' in f: \n",
    "                        GNPScsvfiles2.append(f)\n",
    "                    if 'hmdbproc.' in f: \n",
    "                        HMDBcsvfiles2.append(f)\n",
    "                    if 'mbankproc.' in f: \n",
    "                        MassBankcsvfiles2.append(f)\n",
    "    \n",
    "    dict1 = {'GNPSr': GNPScsvfiles2, 'HMDBr': HMDBcsvfiles2, 'MBr': MassBankcsvfiles2} \n",
    "    df = pd.DataFrame(dict1)\n",
    "    \n",
    "    Merged_Result_df = []\n",
    "    \n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        \n",
    "        \n",
    "        CSVfileG = pd.read_csv(df[\"GNPSr\"][i])\n",
    "        CSVfileH = pd.read_csv(df[\"HMDBr\"][i])\n",
    "        CSVfileM = pd.read_csv(df[\"MBr\"][i])\n",
    "        \n",
    "        # merge on the basis of Idx\n",
    "        MergedRE = CSVfileG.merge(CSVfileH,on='id_X').merge(CSVfileM,on='id_X')\n",
    "\n",
    "        csvname = (df[\"GNPSr\"][i]).replace(\"gnps_with_cor_names\", \"mergedR\")\n",
    "        MergedRE.to_csv(csvname)\n",
    "        Merged_Result_df.append(csvname)\n",
    "        \n",
    "    return(Merged_Result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7174f3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(combine_specdb.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e951a6a3",
   "metadata": {},
   "source": [
    "### Combine all files for spectral db dereplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6dbeda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_allspec(input_dir):\n",
    "    \n",
    "    \"\"\"combine_allspec function combines all results from different\n",
    "    spectral dbs. Can only be used if more than one db is used \n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "    df (dataframe): dataframe from combine_specdb\n",
    "    \n",
    "    Returns:\n",
    "    dataframe: of the paths of the merged results from all files\n",
    "    \n",
    "    Usage:\n",
    "    combine_allspec(input_dir = \"usr/project/\", comb_df)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    Mergedcsvfiles = []\n",
    "    \n",
    "    #list all files and directories\n",
    "    for entry in os.listdir(input_dir):\n",
    "        if os.path.isdir(os.path.join(input_dir, entry)):\n",
    "            \n",
    "            # enter the directory with /spectral_dereplication/ results\n",
    "            sub_dir = input_dir + entry + '/spectral_dereplication'\n",
    "            if os.path.exists(sub_dir):\n",
    "                files = (glob.glob(sub_dir+'/*.csv'))\n",
    "\n",
    "                for f in files:\n",
    "                    if 'mergedR.csv' in f: \n",
    "                        Mergedcsvfiles.append(f)\n",
    "    \n",
    "    combined_csv = pd.concat([pd.read_csv(l) for l in Mergedcsvfiles], ignore_index=True)\n",
    "    \n",
    "    for i, row in combined_csv.iterrows():\n",
    "        if combined_csv['GNPSSMILES'][i] == ' ' or isNaN(combined_csv['GNPSSMILES'][i]):\n",
    "            combined_csv['GNPSSMILES'][i] = ''\n",
    "            \n",
    "    #for i, row in combined_csv.iterrows():\n",
    "        #if not isNaN(combined_csv['MBinchiKEY'][i]):\n",
    "            #try:\n",
    "                #y = pcp.get_compounds(combined_csv['MBinchiKEY'][i], 'inchikey')\n",
    "                #if len(y)>1:\n",
    "                    #combined_csv['MBSMILES'][i] = y[0].isomeric_smiles\n",
    "            #except:\n",
    "                #pass\n",
    "            \n",
    "    combined_csv.to_csv(input_dir + 'MetabolomicsResults/SD_post_processed_combined_results.csv')\n",
    "    return(combined_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56418fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(combine_allspec.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0558d699",
   "metadata": {},
   "source": [
    "### Scoring Scheme for Spectral DB Dereplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cfc095",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f0f28c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring_spec(input_dir, combined):\n",
    "    \n",
    "    \"\"\"scoring_spec extracts the candidates with high scores from\n",
    "    the results from combine_allspec function \n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "    combined (dataframe): dataframe from combine_allspec\n",
    "    \n",
    "    Returns:\n",
    "    dataframe: of the all features and their results\n",
    "    csv: CSV reuslt file named MetabolomicsResults/combinedSpecDB.csv\n",
    "    which contains all the features and their Spec DB annotations\n",
    "    \n",
    "    Usage:\n",
    "    scoring_spec(input_dir = \"usr/project/\", combined)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # the scoring highly depends on the following information:\n",
    "    # similarity scores should be higher than 0.75\n",
    "    # intScore >=0.50\n",
    "    # mzScore >= 0.50\n",
    "    # ratio of the matchingpeaks by the totalpeaks in the query >= 0.50\n",
    "    \n",
    "    combined = pd.read_csv(combined)\n",
    "    \n",
    "    def HMDB_Scoring(db, i):\n",
    "        if db['HMDBmax_similarity'][i] >= 0.75 and db['HMDBintScore'][i] >= 0.50 and db['HMDBmzScore'][i] >= 0.50 and db['HQMatchingPeaks'][i]/db['hQueryTotalPeaks'][i] >= 0.50:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    \n",
    "    def GNPS_Scoring(db, i):\n",
    "        if db['GNPSmax_similarity'][i] >= 0.75 and db['GNPSintScore'][i] >= 0.50 and db['GNPSmzScore'][i] >= 0.50 and db['GQMatchingPeaks'][i]/db['gQueryTotalPeaks'][i] >= 0.50:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    \n",
    "    def MB_Scoring(db, i):\n",
    "        if db['MBmax_similarity'][i] >= 0.75 and db['MBintScore'][i] >= 0.50 and db['MBmzScore'][i] >= 0.50 and db['MQMatchingPeaks'][i]/db['mQueryTotalPeaks'][i] >= 0.50:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    \n",
    "    for i, row in combined.iterrows():\n",
    "        \n",
    "        # if all DBs show good candidates accorindg to the scoring\n",
    "        if HMDB_Scoring(combined, i) and GNPS_Scoring(combined, i) and MB_Scoring(combined, i) and not isNaN(combined['GNPSSMILES'][i]) and not isNaN(combined['MBSMILES'][i]) and not isNaN(combined['HMDBSMILES'][i]):\n",
    "            \n",
    "            # calulate the tanimoto similarity between the candidates from three DBs\n",
    "            \n",
    "            # hmdb and gnps\n",
    "            HGms = [Chem.MolFromSmiles(combined['HMDBSMILES'][i]), Chem.MolFromSmiles(combined['GNPSSMILES'][i])]\n",
    "            HGfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in HGms]\n",
    "            HGtn = DataStructs.FingerprintSimilarity(HGfps[0],HGfps[1])\n",
    "            \n",
    "            # gnps and mbank\n",
    "            GMms = [Chem.MolFromSmiles(combined['GNPSSMILES'][i]), Chem.MolFromSmiles(combined['MBSMILES'][i])]\n",
    "            GMfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in GMms]\n",
    "            GMtn = DataStructs.FingerprintSimilarity(GMfps[0],GMfps[1])\n",
    "            \n",
    "            # mbank and hmdb\n",
    "            HMms = [Chem.MolFromSmiles(combined['HMDBSMILES'][i]), Chem.MolFromSmiles(combined['MBSMILES'][i])]\n",
    "            HMfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in HMms]\n",
    "            HMtn = DataStructs.FingerprintSimilarity(HMfps[0],HMfps[1])\n",
    "            \n",
    "            # add the following columns\n",
    "            combined.loc[i, 'annotation'] = 'HMDB, GNPS, MassBank'\n",
    "            combined.loc[i, 'tanimotoHG'] = HGtn\n",
    "            combined.loc[i, 'tanimotoGM'] = GMtn\n",
    "            combined.loc[i, 'tanimotoHM'] = HMtn\n",
    "            combined.loc[i, 'occurence'] = 3\n",
    "        \n",
    "        # if HMDB and GNPS show good candidates accorindg to the scoring\n",
    "        if HMDB_Scoring(combined, i) and GNPS_Scoring(combined, i) and not MB_Scoring(combined, i) and not isNaN(combined['GNPSSMILES'][i]) and not isNaN(combined['HMDBSMILES'][i]):\n",
    "            HGms = [Chem.MolFromSmiles(combined['HMDBSMILES'][i]), Chem.MolFromSmiles(combined['GNPSSMILES'][i])]\n",
    "            HGfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in HGms]\n",
    "            HGtn = DataStructs.FingerprintSimilarity(HGfps[0],HGfps[1])\n",
    "        \n",
    "            combined.loc[i, 'annotation'] = 'HMDB, GNPS'\n",
    "            combined.loc[i, 'tanimotoHG'] = HGtn\n",
    "            combined.loc[i, 'tanimotoGM'] = np.nan\n",
    "            combined.loc[i, 'tanimotoHM'] = np.nan\n",
    "            combined.loc[i, 'occurence'] = 2\n",
    "        \n",
    "        # if MassBank and GNPS show good candidates accorindg to the scoring\n",
    "        if not HMDB_Scoring(combined, i) and GNPS_Scoring(combined, i) and MB_Scoring(combined, i) and not isNaN(combined['MBSMILES'][i]) and not isNaN(combined['GNPSSMILES'][i]):\n",
    "            GMms = [Chem.MolFromSmiles(combined['GNPSSMILES'][i]), Chem.MolFromSmiles(combined['MBSMILES'][i])]\n",
    "            GMfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in GMms]\n",
    "            GMtn = DataStructs.FingerprintSimilarity(GMfps[0],GMfps[1])\n",
    "        \n",
    "            combined.loc[i, 'annotation'] = 'GNPS, MassBank'\n",
    "            combined.loc[i, 'tanimotoHG'] = np.nan\n",
    "            combined.loc[i, 'tanimotoGM'] = GMtn\n",
    "            combined.loc[i, 'tanimotoHM'] = np.nan\n",
    "            combined.loc[i, 'occurence'] = 2\n",
    "        \n",
    "        # if MassBank and HMDB show good candidates accorindg to the scoring\n",
    "        if HMDB_Scoring(combined, i) and not GNPS_Scoring(combined, i) and MB_Scoring(combined, i) and not isNaN(combined['MBSMILES'][i]) and not isNaN(combined['HMDBSMILES'][i]):\n",
    "            HMms = [Chem.MolFromSmiles(combined['HMDBSMILES'][i]), Chem.MolFromSmiles(combined['MBSMILES'][i])]\n",
    "            HMfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in HMms]\n",
    "            HMtn = DataStructs.FingerprintSimilarity(HMfps[0],HMfps[1])\n",
    "        \n",
    "            combined.loc[i, 'annotation'] = 'HMDB, MassBank'\n",
    "            combined.loc[i, 'tanimotoHG'] = np.nan\n",
    "            combined.loc[i, 'tanimotoGM'] = np.nan\n",
    "            combined.loc[i, 'tanimotoHM'] = HMtn\n",
    "            combined.loc[i, 'occurence'] = 2\n",
    "        \n",
    "        # only HMDB\n",
    "        if HMDB_Scoring(combined, i) and not GNPS_Scoring(combined, i) and not MB_Scoring(combined, i):\n",
    "        \n",
    "            combined.loc[i, 'annotation'] = 'HMDB'\n",
    "            combined.loc[i, 'tanimotoHG'] = np.nan\n",
    "            combined.loc[i, 'tanimotoGM'] = np.nan\n",
    "            combined.loc[i, 'tanimotoHM'] = np.nan\n",
    "            combined.loc[i, 'occurence'] = 1\n",
    "        \n",
    "        # only GNPS\n",
    "        if not HMDB_Scoring(combined, i) and GNPS_Scoring(combined, i) and not MB_Scoring(combined, i):\n",
    "        \n",
    "            combined.loc[i, 'annotation'] = 'GNPS'\n",
    "            combined.loc[i, 'tanimotoHG'] = np.nan\n",
    "            combined.loc[i, 'tanimotoGM'] = np.nan\n",
    "            combined.loc[i, 'tanimotoHM'] = np.nan\n",
    "            combined.loc[i, 'occurence'] = 1\n",
    "        \n",
    "        # only MassBank\n",
    "        if not HMDB_Scoring(combined, i) and not GNPS_Scoring(combined, i) and MB_Scoring(combined, i):\n",
    "        \n",
    "            combined.loc[i, 'annotation'] = 'MassBank'\n",
    "            combined.loc[i, 'tanimotoHG'] = np.nan\n",
    "            combined.loc[i, 'tanimotoGM'] = np.nan\n",
    "            combined.loc[i, 'tanimotoHM'] = np.nan\n",
    "            combined.loc[i, 'occurence'] = 1\n",
    "        \n",
    "        # none\n",
    "        if not HMDB_Scoring(combined, i) and not GNPS_Scoring(combined, i) and not MB_Scoring(combined, i):\n",
    "            combined.loc[i, 'annotation'] = 'none'\n",
    "            combined.loc[i, 'tanimotoHG'] = np.nan\n",
    "            combined.loc[i, 'tanimotoGM'] = np.nan\n",
    "            combined.loc[i, 'tanimotoHM'] = np.nan\n",
    "            combined.loc[i, 'occurence'] = 0\n",
    "    \n",
    "    combined.to_csv(input_dir + \"MetabolomicsResults/combinedSpecDB.csv\")\n",
    "    return(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "406fd6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(scoring_spec.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adc81a7",
   "metadata": {},
   "source": [
    "### Suspect List Screening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff1df5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suspectListScreening(input_dir, slistcsv, SpectralDB_Results):\n",
    "    \n",
    "    \"\"\"suspectListScreening runs tanoimoto similarity score to between\n",
    "    compounds from the results from spectral DBs and suspect list\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "    slistcsv (str): path to suspect list\n",
    "    SpectralDB_Results (dataframe): dataframe from scoring_spec\n",
    "    \n",
    "    Returns:\n",
    "    dataframe: all features and specDB reults and suspect list screening \n",
    "    results\n",
    "    csv: CSV reuslt file named MetabolomicsResults/SpecDBvsSL.csv\n",
    "    which contains all the features and their Spec DB annotations\n",
    "    and suspect list occurences if any\n",
    "    \n",
    "    Usage:\n",
    "    suspectListScreening(input_dir = \"usr/project/\",\n",
    "    slistcsv = \"usr/project/suspect_list.csv\", \n",
    "    SpectralDB_Results)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    SpectralDB_Results = pd.read_csv(SpectralDB_Results)\n",
    "    \n",
    "    # add columns to the resulst from scoring_spec\n",
    "    # these columns are for high similiarity canidtes between the databases and suspect list\n",
    "    SpectralDB_Results['HLsmiles'] = np.nan\n",
    "    SpectralDB_Results['HLname'] = np.nan\n",
    "    SpectralDB_Results['GLsmiles'] = np.nan\n",
    "    SpectralDB_Results['GLname'] = np.nan\n",
    "    SpectralDB_Results['MLsmiles'] = np.nan\n",
    "    SpectralDB_Results['MLname'] = np.nan\n",
    "    \n",
    "    Suspect_list = pd.read_csv(slistcsv)\n",
    "    \n",
    "    for i, row in SpectralDB_Results.iterrows():\n",
    "        if not isNaN(SpectralDB_Results['HMDBSMILES'][i]) and SpectralDB_Results['HMDBSMILES'][i] != \" \":\n",
    "            for j, row in Suspect_list.iterrows():\n",
    "                LHms2 = [Chem.MolFromSmiles(SpectralDB_Results['HMDBSMILES'][i]), Chem.MolFromSmiles(Suspect_list['SMILES'][j])]\n",
    "                LHfps2 = [AllChem.GetMorganFingerprintAsBitVect(x2,2, nBits=2048) for x2 in LHms2]\n",
    "                LHtn2 = DataStructs.FingerprintSimilarity(LHfps2[0],LHfps2[1])\n",
    "                if LHtn2 >= 0.9:\n",
    "                    SpectralDB_Results.loc[i, 'HLsmiles'] = Suspect_list['SMILES'][j]\n",
    "                    SpectralDB_Results.loc[i, 'HLname'] = Suspect_list['Name'][j]\n",
    "                    \n",
    "                    \n",
    "        if not isNaN(SpectralDB_Results['GNPSSMILES'][i]) and SpectralDB_Results['GNPSSMILES'][i] != \" \":\n",
    "            for k, row in Suspect_list.iterrows():\n",
    "                LGms2 = [Chem.MolFromSmiles(SpectralDB_Results['GNPSSMILES'][i]), Chem.MolFromSmiles(Suspect_list['SMILES'][k])]\n",
    "                LGfps2 = [AllChem.GetMorganFingerprintAsBitVect(x2,2, nBits=2048) for x2 in LGms2]\n",
    "                LGtn2 = DataStructs.FingerprintSimilarity(LGfps2[0],LGfps2[1])\n",
    "                if LGtn2 >= 0.9:\n",
    "                    SpectralDB_Results.loc[i, 'GLsmiles'] = Suspect_list['SMILES'][k]\n",
    "                    SpectralDB_Results.loc[i, 'GLname'] = Suspect_list['Name'][k]\n",
    "                    \n",
    "                    \n",
    "        if not isNaN(SpectralDB_Results['MBSMILES'][i]) and SpectralDB_Results['MBSMILES'][i] != \" \":\n",
    "            for l, row in Suspect_list.iterrows():\n",
    "                LMms2 = [Chem.MolFromSmiles(SpectralDB_Results['MBSMILES'][i]), Chem.MolFromSmiles(Suspect_list['SMILES'][l])]\n",
    "                LMfps2 = [AllChem.GetMorganFingerprintAsBitVect(x2,2, nBits=2048) for x2 in LMms2]\n",
    "                LMtn2 = DataStructs.FingerprintSimilarity(LMfps2[0],LMfps2[1])\n",
    "                if LMtn2 >= 0.9:\n",
    "                    SpectralDB_Results.loc[i, 'MLsmiles'] = Suspect_list['SMILES'][l]\n",
    "                    SpectralDB_Results.loc[i, 'MLname'] = Suspect_list['Name'][l]\n",
    "    \n",
    "    # add annotations and occurences\n",
    "    for i, row in SpectralDB_Results.iterrows():\n",
    "        if not isNaN(SpectralDB_Results['HLname'][i]) or not isNaN(SpectralDB_Results['GLname'][i]) or not isNaN(SpectralDB_Results['MLname'][i]):\n",
    "            SpectralDB_Results['occurence'][i] = SpectralDB_Results['occurence'][i] + 1\n",
    "            SpectralDB_Results['annotation'][i] = SpectralDB_Results['annotation'][i] + ', Suspect_List'\n",
    "            \n",
    "    SpectralDB_Results.to_csv(input_dir + \"MetabolomicsResults/SpecDBvsSL.csv\")\n",
    "    return(SpectralDB_Results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4a29d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(suspectListScreening.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82dbf13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce884025",
   "metadata": {},
   "source": [
    "# Final Candidate List Curation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e45b773",
   "metadata": {},
   "source": [
    "## MetFrag Curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc97b626",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0e33527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metfrag_curation(input_dir, metfragcsv, sl = True):\n",
    "    \n",
    "    \n",
    "    \"\"\"metfrag_curation checks which database produced results. If both \n",
    "    did, it checks whether it was the same compound as candidate, if not,\n",
    "    add PubChem or any of the two databases with similarity to Suspect\n",
    "    list\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "    metfragcsv (str): path to combined metfrag results:\n",
    "    MetabolomicsResults/MetFrag_combined.csv\n",
    "    \n",
    "    Returns:\n",
    "    dataframe: dataframe of curated metfrag results\n",
    "    csv: MetabolomicsResults/metfrag_curated.csv\n",
    "    \n",
    "    Usage:\n",
    "    metfrag_curation(input_dir = \"usr/project/\", \n",
    "    metfragcsv = \"usr/project/MetabolomicsResults/MetFrag_combined.csv\")\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    metfrag = pd.read_csv(metfragcsv)\n",
    "    for i, row in metfrag.iterrows():\n",
    "    \n",
    "        # If only Pubchem\n",
    "        if not isNaN(metfrag['PC_SMILES'][i]) and isNaN(metfrag['KG_SMILES'][i]):\n",
    "            metfrag.loc[i, 'Annotation_M'] = 'PubChem'\n",
    "            #metfrag.loc[i, 'SMILES_final'] = metfrag['PC_SMILES'][i]\n",
    "    \n",
    "        # If only KEGG\n",
    "        elif not isNaN(metfrag['KG_SMILES'][i]) and isNaN(metfrag['PC_SMILES'][i]):\n",
    "            metfrag.loc[i, 'Annotation_M'] = 'KEGG'\n",
    "            #metfrag.loc[i, 'SMILES_final'] = metfrag['KG_SMILES'][i]\n",
    "    \n",
    "        # If both, calculate the similarity\n",
    "        elif not isNaN(metfrag['PC_SMILES'][i]) and not isNaN(metfrag['KG_SMILES'][i]):\n",
    "        \n",
    "            PKms = [Chem.MolFromSmiles(metfrag['KG_SMILES'][i]), Chem.MolFromSmiles(metfrag['PC_SMILES'][i])]\n",
    "            PKfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in PKms]\n",
    "            PKtn = DataStructs.FingerprintSimilarity(PKfps[0],PKfps[1])\n",
    "        \n",
    "            # if both are similar, add both\n",
    "            if PKtn == 1:\n",
    "                metfrag.loc[i, 'Annotation_M'] = 'KEGG, PubChem'\n",
    "                #metfrag.loc[i, 'SMILES_final'] = metfrag['PC_SMILES'][i]\n",
    "        \n",
    "            #if not similar:\n",
    "            else:\n",
    "                # if there is NO entry from suspect list, then add PuBchem\n",
    "                if isNaN(metfrag['KG_SL_comp'][i]) and isNaN(metfrag['PC_SL_comp'][i]):\n",
    "                    metfrag.loc[i, 'Annotation_M'] = 'PubChem'\n",
    "                    #metfrag.loc[i, 'SMILES_final'] = metfrag['PC_SMILES'][i]\n",
    "                else:\n",
    "                    if sl:\n",
    "                        \n",
    "                        #if there is an entry from suspect list WITH kegg\n",
    "                        if not isNaN(metfrag['KG_SL_comp'][i]) and isNaN(metfrag['PC_SL_comp'][i]):\n",
    "                            metfrag.loc[i, 'Annotation_M'] = 'KEGG, SuspectList'\n",
    "                            #metfrag.loc[i, 'SMILES_final'] = metfrag['KG_SMILES'][i]\n",
    "                \n",
    "                        #if there is an entry from suspect list WITH kegg\n",
    "                        elif not isNaN(metfrag['PC_SL_comp'][i]) and isNaN(metfrag['KG_SL_comp'][i]):\n",
    "                            metfrag.loc[i, 'Annotation_M'] = 'PubChem, SuspectList'\n",
    "                            #metfrag.loc[i, 'SMILES_final'] = metfrag['PC_SMILES'][i]\n",
    "                    \n",
    "                                \n",
    "    metfrag.to_csv(input_dir + \"MetabolomicsResults/metfrag_curated.csv\")  \n",
    "    return(metfrag)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa2dca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(metfrag_curation.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bb7a26",
   "metadata": {},
   "source": [
    "## SIRIUS Results Curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73654ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sirius_curation(input_dir, siriuscsv, sl = True):\n",
    "    \n",
    "    \"\"\"sirius_curation checks if candidate selected has a good score for \n",
    "    explained intensity. It also checks if there was any similarity to\n",
    "    a compound from Suspect list\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "    siriuscsv (str): path to combined metfrag results:\n",
    "    MetabolomicsResults/Sirius_combined.csv\n",
    "    \n",
    "    Returns:\n",
    "    dataframe: dataframe of curated sirius results\n",
    "    csv: MetabolomicsResults/sirius_curated.csv\n",
    "    \n",
    "    Usage:\n",
    "    sirius_curation(input_dir = \"usr/project/\", \n",
    "    siriuscsv = \"usr/project/MetabolomicsResults/Sirius_combined.csv\")\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    sirius = pd.read_csv(siriuscsv)\n",
    "    for i, row in sirius.iterrows():\n",
    "    \n",
    "        # If the explained intensity is greater than 0.70 and there is no suspect list entry\n",
    "        if sirius['exp_int'][i] >= 0.70 and isNaN(sirius['SL_comp'][i]):\n",
    "            sirius.loc[i, 'Annotation_S'] = 'SIRIUS'\n",
    "            #sirius.loc[i, 'SMILES_final'] = sirius['SMILES'][i]\n",
    "        else:\n",
    "            if sl:\n",
    "                \n",
    "                #If the explained intensity is greater than 0.70 and there is an entry from suspect list\n",
    "                if sirius['exp_int'][i] >= 0.70 and not isNaN(sirius['SL_comp'][i]):\n",
    "                    sirius.loc[i, 'Annotation_S'] = 'SIRIUS, SuspectList'\n",
    "                    #sirius.loc[i, 'SMILES_final'] = sirius['SMILES'][i]\n",
    "    \n",
    "                # if the intensity is less thna 0.70 but it still is similar to an entry in Suspect list,\n",
    "                elif sirius['exp_int'][i] < 0.70 and not isNaN(sirius['SL_comp'][i]):\n",
    "                    sirius.loc[i, 'Annotation_S'] = 'SIRIUS, SuspectList'\n",
    "                    #sirius.loc[i, 'SMILES_final'] = sirius['SMILES'][i]\n",
    "        \n",
    "    sirius.to_csv(input_dir + \"MetabolomicsResults/sirius_curated.csv\")\n",
    "    return(sirius)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80945399",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sirius_curation.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2ff69c",
   "metadata": {},
   "source": [
    "## combine curated S and M results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d14f3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineSM(input_dir, metfrag, sirius):\n",
    "    \n",
    "    \"\"\"combineSM prioritizes Sirius and Suspect list over PubChem and\n",
    "    KEGG\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "    sirius (dataframe): result of sirius_curation\n",
    "    metfrag (dataframe): result of metfrag_curation\n",
    "    \n",
    "    Returns:\n",
    "    dataframe: dataframe of combined curated sirius and metfrag results\n",
    "    csv: \"MetabolomicsResults/combinedSM.csv\"\n",
    "    \n",
    "    Usage:\n",
    "    combineSM(input_dir = \"usr/project/\", metfrag, sirius)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    S_M_CSV = pd.concat([sirius, metfrag], axis = 1, levels = [\"id_X\"])\n",
    "    for i, rows in S_M_CSV.iterrows():\n",
    "    \n",
    "        # if results has Sirius Structure annotation, and the explained inetnsity is >= 0.70, keep the annotation as is.\n",
    "        if S_M_CSV[\"Result\"][i] == \"SIRIUS_STR\" and S_M_CSV['exp_int'][i] >= 0.70:\n",
    "            S_M_CSV.loc[i, 'Annotation_C'] = S_M_CSV['Annotation_S'][i]\n",
    "        \n",
    "            # to add to that annotation\n",
    "            if not isNaN(S_M_CSV[\"Annotation_M\"][i]):\n",
    "            \n",
    "                # if annotation has PubChem, by default add SIRIUS\n",
    "                if \"PubChem\" in S_M_CSV[\"Annotation_M\"][i]:\n",
    "                    S_M_CSV.loc[i, 'Annotation_C'] = S_M_CSV['Annotation_S'][i]\n",
    "    \n",
    "                    # And calculate the similarity between pubchem and SIrius results\n",
    "                    try:\n",
    "                        PSms = [Chem.MolFromSmiles(S_M_CSV['SMILES'][i]), Chem.MolFromSmiles(S_M_CSV['PC_SMILES'][i])]\n",
    "                        PSfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in PSms]\n",
    "                        PStn = DataStructs.FingerprintSimilarity(PSfps[0],PSfps[1])\n",
    "                        # if similar strcutres, then add Pubchme and sirius\n",
    "                        if PStn == 1:\n",
    "                            print(i)\n",
    "                            S_M_CSV.loc[i, 'Annotation_C'] = S_M_CSV['Annotation_S'][i] + ', PubChem'\n",
    "                            #S_M_CSV.loc[i, 'SMILES_final'] = S_M_CSV['SMILES'][i]\n",
    "                        # if not then just keep sirius\n",
    "                        else:\n",
    "                            S_M_CSV.loc[i, 'Annotation_C'] = S_M_CSV['Annotation_S'][i]\n",
    "                            #S_M_CSV.loc[i, 'SMILES_final'] = S_M_CSV['SMILES'][i]\n",
    "                    except:\n",
    "                        pass\n",
    "                #apply similar approach to KEGG, and by default add SIRIUS\n",
    "                elif not isNaN(S_M_CSV[\"KG_SMILES\"][i]):\n",
    "                    S_M_CSV.loc[i, 'Annotation_C'] = S_M_CSV['Annotation_S'][i]\n",
    "\n",
    "                    try:\n",
    "                        SKms = [Chem.MolFromSmiles(S_M_CSV['SMILES'][i]), Chem.MolFromSmiles(S_M_CSV['KG_SMILES'][i])]\n",
    "                        SKfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in SKms]\n",
    "                        SKtn = DataStructs.FingerprintSimilarity(SKfps[0],SKfps[1])\n",
    "                        if SKtn == 1:\n",
    "                            print(i)\n",
    "                            S_M_CSV.loc[i, 'Annotation_C'] = S_M_CSV['Annotation_S'][i] +', KEGG'\n",
    "\n",
    "                        else:\n",
    "                            S_M_CSV.loc[i, 'Annotation_C'] = S_M_CSV['Annotation_S'][i]\n",
    "\n",
    "                    except:\n",
    "                        pass\n",
    "    \n",
    "        # if there is no annotation from SIRIUS, then use Metfrag results\n",
    "        elif S_M_CSV[\"Result\"][i] != \"SIRIUS_STR\" and S_M_CSV['exp_int'][i] < 0.70:\n",
    "            if not isNaN(S_M_CSV['Annotation_M'][i]):\n",
    "                if 'PubChem' in S_M_CSV['Annotation_M'][i]:\n",
    "                    S_M_CSV.loc[i, 'Annotation_C'] = S_M_CSV['Annotation_M'][i]\n",
    "                    #S_M_CSV.loc[i, 'SMILES'] = S_M_CSV['PC_SMILES'][i]\n",
    "                else:\n",
    "                    S_M_CSV.loc[i, 'Annotation_C'] = S_M_CSV['Annotation_M'][i]\n",
    "                    #S_M_CSV.loc[i, 'SMILES'] = S_M_CSV['KG_SMILES'][i]\n",
    "                    \n",
    "    S_M_CSV.to_csv(input_dir + \"MetabolomicsResults/combinedSM.csv\")\n",
    "    return(S_M_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c04409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(combineSM.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28372129",
   "metadata": {},
   "source": [
    "## Spec DB Curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b12a109c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def specDB_Curation(input_dir, combinedx, sl = True):\n",
    "    \n",
    "    \"\"\"specDB_Curation prioritizes in the following manner: gnps>\n",
    "    mbank>suspectlist>hmdb\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "    \n",
    "    combined: dataframe from either suspectListScreening function if\n",
    "    sl = True OR from scoring_spec if sl = False\n",
    "    \n",
    "    Returns:\n",
    "    dataframe: with curated Spectral DB results\n",
    "    csv: \"MetabolomicsResults/curatedSDB.csv\"\n",
    "    \n",
    "    Usage:\n",
    "    specDB_Curation(input_dir = \"usr/project/\",combinedx, sl = True)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # define scores again to remove the entries with lower scores\n",
    "    def HMDB_Scoring(db, i):\n",
    "        if db['HMDBmax_similarity'][i] >= 0.75 and db['HMDBintScore'][i] >= 0.50 and db['HMDBmzScore'][i] >= 0.50 and db['HQMatchingPeaks'][i]/db['hQueryTotalPeaks'][i] >= 0.50:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    def GNPS_Scoring(db, i):\n",
    "        if db['GNPSmax_similarity'][i] >= 0.75 and db['GNPSintScore'][i] >= 0.50 and db['GNPSmzScore'][i] >= 0.50 and db['GQMatchingPeaks'][i]/db['gQueryTotalPeaks'][i] >= 0.50:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    def MB_Scoring(db, i):\n",
    "        if db['MBmax_similarity'][i] >= 0.75 and db['MBintScore'][i] >= 0.50 and db['MBmzScore'][i] >= 0.50 and db['MQMatchingPeaks'][i]/db['mQueryTotalPeaks'][i] >= 0.50:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    \n",
    "    combined = pd.read_csv(combinedx)\n",
    "    \n",
    "    \n",
    "    # remove the similarity scores from low scoring candidates\n",
    "    for i, row in combined.iterrows():\n",
    "        if HMDB_Scoring(combined, i) or GNPS_Scoring(combined, i) or MB_Scoring(combined, i):\n",
    "            print(i)\n",
    "        else:\n",
    "            combined['GNPSspectrumID'][i] = np.nan\n",
    "            combined['MBspectrumID'][i] = np.nan\n",
    "            combined['HMDBcompoundID'][i] = np.nan\n",
    "    \n",
    "    # if sl = True\n",
    "    if sl:\n",
    "    \n",
    "        for i, row in combined.iterrows():\n",
    "    \n",
    "        ##### When there is an annotaion from all DBs #####\n",
    "    \n",
    "        #all entries with a high scoring annotation in all DBs,\n",
    "            if not isNaN(combined['GNPSspectrumID'][i]) and not isNaN(combined['MBspectrumID'][i]) and not isNaN(combined['HMDBcompoundID'][i]):\n",
    "        \n",
    "                # entries with same candidate from all Spectral DBs\n",
    "                if combined['tanimotoHG'][i] == 1.0 and combined['tanimotoGM'][i] == 1.0 and combined['tanimotoHM'][i] == 1.0:\n",
    "                    combined.loc[i, 'Annotation'] = 'GNPS, HMDB, MassBank'\n",
    "                    #entries with same candidate in suspect list, as in all Spectral DBs\n",
    "                    if combined['GLname'][i] == combined['HLname'][i]== combined['MLname'][i]:\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS, HMDB, MassBank, SuspectList'\n",
    "                \n",
    "                # same candidate from GNPS and HMDB        \n",
    "                if combined['tanimotoHG'][i] == 1.0 and combined['tanimotoGM'][i] != 1.0 and combined['tanimotoHM'][i] != 1.0:\n",
    "                    combined.loc[i, 'Annotation'] = 'GNPS, HMDB'\n",
    "                    # if its present in Suspect List\n",
    "                    if combined['GLname'][i] == combined['HLname'][i]:\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS, HMDB, SuspectList'\n",
    "        \n",
    "                # same candidate from GNPS and MassBank        \n",
    "                if combined['tanimotoHG'][i] != 1.0 and combined['tanimotoGM'][i] == 1.0 and combined['tanimotoHM'][i] != 1.0:\n",
    "                    combined.loc[i, 'Annotation'] = 'GNPS, MassBank'\n",
    "                    # if its present in Suspect List\n",
    "                    if combined['GLname'][i] == combined['MLname'][i]:\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS, MassBank, SuspectList'\n",
    "                \n",
    "                # same candidate from MassBank and HMDB        \n",
    "                if combined['tanimotoHG'][i] != 1.0 and combined['tanimotoGM'][i] != 1.0 and combined['tanimotoHM'][i] == 1.0:\n",
    "                    combined.loc[i, 'Annotation'] = 'GNPS, HMDB'\n",
    "                    # if its present in Suspect List\n",
    "                    if combined['GLname'][i] == combined['HLname'][i]:\n",
    "                        combined.loc[i, 'Annotation'] = 'HMDB, MassBank, SuspectList'\n",
    "                \n",
    "                # only one database must be selected based on SuspectList annotation\n",
    "                if combined['tanimotoHG'][i] != 1.0 and combined['tanimotoGM'][i] != 1.0 and combined['tanimotoHM'][i] != 1.0:\n",
    "            \n",
    "                    # only GNPS has SuspectList annotation\n",
    "                    if not isNaN(combined['GLname'][i]):\n",
    "\n",
    "                        combined.loc[i, 'Annotation'] = 'GNPS, SuspectList'\n",
    "            \n",
    "            \n",
    "                    # only MassBank has SuspectList annotation\n",
    "                    elif not isNaN(combined['MLname'][i]):\n",
    "                        combined.loc[i, 'Annotation'] = 'MassBank, SuspectList'\n",
    "            \n",
    "            \n",
    "                    # only HMDB has SuspectList annotation\n",
    "                    elif not isNaN(combined['HLname'][i]):\n",
    "                        combined.loc[i, 'Annotation'] = 'HMDB, SuspectList'\n",
    "            \n",
    "        \n",
    "                    # all different annotations, take GNPS\n",
    "                    else:\n",
    "                        if not isNaN(combined['GNPSSMILES'][i]):\n",
    "                            combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "                        else:\n",
    "                            combined.loc[i, 'Annotation'] = 'MassBank'\n",
    "            \n",
    "        \n",
    "            ##### When there is an annotation from two DBs #####\n",
    "    \n",
    "    \n",
    "            # only GNPS and HMDB\n",
    "            if not isNaN(combined['GNPSspectrumID'][i]) and isNaN(combined['MBspectrumID'][i]) and not isNaN(combined['HMDBcompoundID'][i]):\n",
    "\n",
    "                if not isNaN(combined['GLname'][i]):\n",
    "                    combined.loc[i, 'Annotation'] = 'GNPS, SuspectList'\n",
    "                elif not isNaN(combined['HLname'][i]):\n",
    "                    combined.loc[i, 'Annotation'] = 'HMDB, SuspectList'\n",
    "                elif not isNaN(combined['GNPSSMILES'][i]):\n",
    "                    combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "                else:\n",
    "                    combined.loc[i, 'Annotation'] = 'HMDB'\n",
    "        \n",
    "    \n",
    "            # only GNPS and MassBank\n",
    "            if not isNaN(combined['GNPSspectrumID'][i]) and not isNaN(combined['MBspectrumID'][i]) and isNaN(combined['HMDBcompoundID'][i]):\n",
    "\n",
    "                if not isNaN(combined['GLname'][i]):\n",
    "                    combined.loc[i, 'Annotation'] = 'GNPS, SuspectList'\n",
    "                elif not isNaN(combined['MLname'][i]):\n",
    "                    combined.loc[i, 'Annotation'] = 'MassBank, SuspectList'\n",
    "                elif not isNaN(combined['GNPSSMILES'][i]):\n",
    "                    combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "                else:\n",
    "                    combined.loc[i, 'Annotation'] = 'MassBank'\n",
    "    \n",
    "            # only MassBank and HMDB\n",
    "    \n",
    "            if isNaN(combined['GNPSspectrumID'][i]) and not isNaN(combined['MBspectrumID'][i]) and not isNaN(combined['HMDBcompoundID'][i]):\n",
    "\n",
    "                if not isNaN(combined['MLname'][i]):\n",
    "                    combined.loc[i, 'Annotation'] = 'MassBank, SuspectList'\n",
    "                elif not isNaN(combined['HLname'][i]):\n",
    "                    combined.loc[i, 'Annotation'] = 'HMDB, SuspectList'\n",
    "                else:\n",
    "                    combined.loc[i, 'Annotation'] = 'MassBank'\n",
    "    \n",
    "    \n",
    "    \n",
    "            ##### When there is an annotation from one DBs #####\n",
    "    \n",
    "    \n",
    "            # only GNPS\n",
    "            if not isNaN(combined['GNPSspectrumID'][i]) and isNaN(combined['MBspectrumID'][i]) and isNaN(combined['HMDBcompoundID'][i]):\n",
    "        \n",
    "                #If also SuspectList\n",
    "                if not isNaN(combined['GLname'][i]):\n",
    "                    combined.loc[i, 'Annotation'] = 'GNPS, SuspectList'\n",
    "                elif not isNaN(combined['GNPSSMILES'][i]):\n",
    "                    combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "        \n",
    "            # only MassBank\n",
    "            if isNaN(combined['GNPSspectrumID'][i]) and not isNaN(combined['MBspectrumID'][i]) and isNaN(combined['HMDBcompoundID'][i]):\n",
    "                combined.loc[i, 'Annotation'] = 'MassBank'\n",
    "                #If also SuspectList\n",
    "                if not isNaN(combined['MLname'][i]):\n",
    "                    combined.loc[i, 'Annotation'] = 'MassBank, SuspectList'\n",
    "    \n",
    "            # only HMDB\n",
    "            if isNaN(combined['GNPSspectrumID'][i]) and isNaN(combined['MBspectrumID'][i]) and not isNaN(combined['HMDBcompoundID'][i]):\n",
    "                combined.loc[i, 'Annotation'] = 'HMDB'\n",
    "                #If also SuspectList\n",
    "                if not isNaN(combined['HLname'][i]):\n",
    "                    combined.loc[i, 'Annotation'] = 'HMDB, SuspectList'\n",
    "            \n",
    "    else:\n",
    "        for i, row in combined.iterrows():\n",
    "    \n",
    "        ##### When there is an annotaion from all DBs #####\n",
    "    \n",
    "        #all entries with a high scoring annotation in all DBs,\n",
    "            if not isNaN(combined['GNPSspectrumID'][i]) and not isNaN(combined['MBspectrumID'][i]) and not isNaN(combined['HMDBcompoundID'][i]):\n",
    "        \n",
    "                # entries with same candidate from all Spectral DBs\n",
    "                if combined['tanimotoHG'][i] == 1.0 and combined['tanimotoGM'][i] == 1.0 and combined['tanimotoHM'][i] == 1.0:\n",
    "                    combined.loc[i, 'Annotation'] = 'GNPS, HMDB, MassBank'\n",
    "                \n",
    "                # same candidate from GNPS and HMDB        \n",
    "                if combined['tanimotoHG'][i] == 1.0 and combined['tanimotoGM'][i] != 1.0 and combined['tanimotoHM'][i] != 1.0:\n",
    "                    combined.loc[i, 'Annotation'] = 'GNPS, HMDB'\n",
    "        \n",
    "                # same candidate from GNPS and MassBank        \n",
    "                if combined['tanimotoHG'][i] != 1.0 and combined['tanimotoGM'][i] == 1.0 and combined['tanimotoHM'][i] != 1.0:\n",
    "                    combined.loc[i, 'Annotation'] = 'GNPS, MassBank'\n",
    "                \n",
    "                # same candidate from MassBank and HMDB        \n",
    "                if combined['tanimotoHG'][i] != 1.0 and combined['tanimotoGM'][i] != 1.0 and combined['tanimotoHM'][i] == 1.0:\n",
    "                    combined.loc[i, 'Annotation'] = 'GNPS, HMDB'\n",
    "                \n",
    "                \n",
    "            # all different annotations, take GNPS\n",
    "            else:\n",
    "                if not isNaN(combined['GNPSSMILES'][i]):\n",
    "                    combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "                else:\n",
    "                    combined.loc[i, 'Annotation'] = 'MassBank'\n",
    "            \n",
    "        \n",
    "            ##### When there is an annotation from two DBs #####\n",
    "    \n",
    "    \n",
    "            # only GNPS and HMDB\n",
    "            if not isNaN(combined['GNPSspectrumID'][i]) and isNaN(combined['MBspectrumID'][i]) and not isNaN(combined['HMDBcompoundID'][i]):\n",
    "\n",
    "                if not isNaN(combined['GNPSSMILES'][i]):\n",
    "                    combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "                else:\n",
    "                    combined.loc[i, 'Annotation'] = 'HMDB'\n",
    "        \n",
    "    \n",
    "            # only GNPS and MassBank\n",
    "            if not isNaN(combined['GNPSspectrumID'][i]) and not isNaN(combined['MBspectrumID'][i]) and isNaN(combined['HMDBcompoundID'][i]):\n",
    "\n",
    "                if not isNaN(combined['GNPSSMILES'][i]):\n",
    "                    combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "                else:\n",
    "                    combined.loc[i, 'Annotation'] = 'MassBank'\n",
    "    \n",
    "            # only MassBank and HMDB\n",
    "            if isNaN(combined['GNPSspectrumID'][i]) and not isNaN(combined['MBspectrumID'][i]) and not isNaN(combined['HMDBcompoundID'][i]):\n",
    "                combined.loc[i, 'Annotation'] = 'MassBank'\n",
    "    \n",
    "    \n",
    "    \n",
    "            ##### When there is an annotation from one DBs #####\n",
    "    \n",
    "    \n",
    "            # only GNPS\n",
    "            if not isNaN(combined['GNPSspectrumID'][i]) and isNaN(combined['MBspectrumID'][i]) and isNaN(combined['HMDBcompoundID'][i]):\n",
    "                if not isNaN(combined['GNPSSMILES'][i]):\n",
    "                    combined.loc[i, 'Annotation'] = 'GNPS'\n",
    "        \n",
    "            # only MassBank\n",
    "            if isNaN(combined['GNPSspectrumID'][i]) and not isNaN(combined['MBspectrumID'][i]) and isNaN(combined['HMDBcompoundID'][i]):\n",
    "                combined.loc[i, 'Annotation'] = 'MassBank'\n",
    "    \n",
    "            # only HMDB\n",
    "            if isNaN(combined['GNPSspectrumID'][i]) and isNaN(combined['MBspectrumID'][i]) and not isNaN(combined['HMDBcompoundID'][i]):\n",
    "                combined.loc[i, 'Annotation'] = 'HMDB'\n",
    "                \n",
    "    combined.to_csv(input_dir + \"MetabolomicsResults/curatedSDB.csv\")\n",
    "    return(combined)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5d9adc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(specDB_Curation.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd0d1cb",
   "metadata": {},
   "source": [
    "# combine curated SDB and CDB (S+M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900ba52b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0a8aa852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_CuratedR(input_dir, combinedSDBs, combinedSMs):\n",
    "    \n",
    "    \"\"\"combine_CuratedR prioritizes in the following manner: gnps>\n",
    "    mbank>suspectlist>sirius>hmdb>metfrag\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "    \n",
    "    curatedSDB: df from specDB_Curation\n",
    "    combinedSM: df from combineSM\n",
    "    \n",
    "    Returns:\n",
    "    dataframe: with curated Spectral DB results and CDB (S+M) results\n",
    "    csv: \"MetabolomicsResults/final_curation_without_classes.csv\"\n",
    "    \n",
    "    Usage:\n",
    "    combine_CuratedR(input_dir = \"usr/project/\", curatedSDB, combinedSM)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    combinedSDB = pd.read_csv(combinedSDBs)\n",
    "    combinedSM = pd.read_csv(combinedSMs)\n",
    "    \n",
    "    mega = pd.concat([combinedSM, combinedSDB], axis = 1, levels = [\"id_X\"])\n",
    "    \n",
    "    for i, row in mega.iterrows():\n",
    "    \n",
    "        #if only compound database results\n",
    "        if isNaN(mega['Annotation'][i]) and not isNaN(mega['Annotation_C'][i]):\n",
    "            mega.loc[i, \"Annotation_Source\"] = mega['Annotation_C'][i]\n",
    "        \n",
    "        # if only spectral db results\n",
    "        if not isNaN(mega['Annotation'][i]) and isNaN(mega['Annotation_C'][i]):\n",
    "            mega.loc[i, \"Annotation_Source\"] = mega['Annotation'][i]\n",
    "        \n",
    "        \n",
    "        # if both have results\n",
    "        if not isNaN(mega['Annotation'][i]) and not isNaN(mega['Annotation_C'][i]):\n",
    "        \n",
    "            ########THREE OR FOUR SDB SOURCES########\n",
    "        \n",
    "            #if three sdb sources or more\n",
    "            # prioritize Spectral DBs\n",
    "            if len(mega['Annotation'][i].split()) >= 3 and 'SIRIUS' in mega['Annotation_C'][i]:\n",
    "            \n",
    "                if 'MassBank' in mega['Annotation'][i]:\n",
    "                    SKms = [Chem.MolFromSmiles(mega['MBSMILES'][i]), Chem.MolFromSmiles(mega['SMILES'][i])]\n",
    "                    SKfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in SKms]\n",
    "                    SKtn = DataStructs.FingerprintSimilarity(SKfps[0],SKfps[1])\n",
    "                    if SKtn == 1.0:\n",
    "                        print(SKtn)\n",
    "                        mega.loc[i, \"Annotation_Source\"] = mega['Annotation'][i] + ', ' + mega['Annotation_C'][i]\n",
    "                    else:\n",
    "                        mega.loc[i, \"Annotation_Source\"] = mega['Annotation'][i]\n",
    "            \n",
    "                elif 'HMDB' in mega['Annotation'][i]:\n",
    "                    SKms = [Chem.MolFromSmiles(mega['HMDBSMILES'][i]), Chem.MolFromSmiles(mega['SMILES'][i])]\n",
    "                    SKfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in SKms]\n",
    "                    SKtn = DataStructs.FingerprintSimilarity(SKfps[0],SKfps[1])\n",
    "                    if SKtn == 1.0:\n",
    "                        mega.loc[i, \"Annotation_Source\"] = mega['Annotation'][i] + ', ' + mega['Annotation_C'][i]\n",
    "                    else:\n",
    "                        mega.loc[i, \"Annotation_Source\"] = mega['Annotation'][i]\n",
    "                    \n",
    "            elif len(mega['Annotation'][i].split()) >= 3 and 'SIRIUS' not in mega['Annotation_C'][i]:\n",
    "                mega.loc[i, \"Annotation_Source\"] = mega['Annotation'][i]\n",
    "            \n",
    "            \n",
    "            \n",
    "            ########TWO SDB SOURCES########\n",
    "            \n",
    "            #if two sdb sources not HMDB\n",
    "            #still prioritize Spectral DBs\n",
    "            elif len(mega['Annotation'][i].split()) == 2 and 'HMDB' not in mega['Annotation'][i] and 'SIRIUS' in mega['Annotation_C'][i]:\n",
    "                if 'MassBank' in mega['Annotation'][i]:\n",
    "                    SKms = [Chem.MolFromSmiles(mega['MBSMILES'][i]), Chem.MolFromSmiles(mega['SMILES'][i])]\n",
    "                    SKfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in SKms]\n",
    "                    SKtn = DataStructs.FingerprintSimilarity(SKfps[0],SKfps[1])\n",
    "                    if SKtn == 1.0:\n",
    "    \n",
    "                        mega.loc[i, \"Annotation_Source\"] = mega['Annotation'][i] + ', ' + mega['Annotation_C'][i]\n",
    "                    else:\n",
    "                        mega.loc[i, \"Annotation_Source\"] = mega['Annotation'][i]\n",
    "            \n",
    "                #for GNPS also check if there are any smiles given\n",
    "                elif 'GNPS' in mega['Annotation'][i] and not isNaN(mega['GNPSSMILES'][i]):\n",
    "                    SKms = [Chem.MolFromSmiles(mega['GNPSSMILES'][i]), Chem.MolFromSmiles(mega['SMILES'][i])]\n",
    "                    SKfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=2048) for x in SKms]\n",
    "                    SKtn = DataStructs.FingerprintSimilarity(SKfps[0],SKfps[1])\n",
    "                    if SKtn == 1:\n",
    "                        print(SKtn)\n",
    "                        mega.loc[i, \"Annotation_Source\"] = mega['Annotation'][i] + ', ' + mega['Annotation_C'][i]\n",
    "                    else:\n",
    "                        mega.loc[i, \"Annotation_Source\"] = mega['Annotation'][i]\n",
    "                    \n",
    "                elif 'GNPS' in mega['Annotation'][i] and isNaN(mega['GNPSSMILES'][i]):\n",
    "                    mega.loc[i, \"Annotation_Source\"] = mega['Annotation'][i]\n",
    "                \n",
    "            elif len(mega['Annotation'][i].split()) == 2 and 'HMDB' not in mega['Annotation'][i] and 'SIRIUS' not in mega['Annotation_C'][i]:\n",
    "                mega.loc[i, \"Annotation_Source\"] = mega['Annotation'][i]\n",
    "            \n",
    "            \n",
    "            #if two sdb sources but with HMDB\n",
    "            #prioritize SIRIUS\n",
    "            elif len(mega['Annotation'][i].split()) == 2 and 'HMDB' in mega['Annotation'][i] and 'SIRIUS' in mega['Annotation_C'][i]:\n",
    "                mega.loc[i, \"Annotation_Source\"] = mega['Annotation_C'][i]\n",
    "            \n",
    "            \n",
    "            # if two sdb sources with HMDB but cant prioritize SIRIUS\n",
    "            #prioritize Spectral DB\n",
    "            elif len(mega['Annotation'][i].split()) == 2 and 'HMDB' not in mega['Annotation'][i] and 'SIRIUS' in mega['Annotation_C'][i]:\n",
    "\n",
    "                mega.loc[i, \"Annotation_Source\"] = mega['Annotation'][i]\n",
    "        \n",
    "        \n",
    "        \n",
    "            #######ONE SDB SOURCE#########\n",
    "        \n",
    "            #if only one sdb, but no SIRIUS\n",
    "            #prioritize Spectral DB\n",
    "            elif len(mega['Annotation'][i].split()) == 1 and 'SIRIUS' not in mega['Annotation_C'][i]:\n",
    "\n",
    "                mega.loc[i, \"Annotation_Source\"] = mega['Annotation'][i]\n",
    "        \n",
    "            #if only one sdb\n",
    "            #prioritize SIRIUS\n",
    "            elif len(mega['Annotation'][i].split()) == 1 and 'SIRIUS' in mega['Annotation_C'][i]:\n",
    "                mega.loc[i, \"Annotation_Source\"] = mega['Annotation_C'][i]\n",
    "            \n",
    "            \n",
    "        # if only spectral db results\n",
    "        if isNaN(mega['Annotation'][i]) and isNaN(mega['Annotation_C'][i]) and not isNaN(mega['Formula'][i]):\n",
    "            mega.loc[i, \"Annotation_Source\"] = 'SIRIUS_Formula'\n",
    "    \n",
    "    \n",
    "    \n",
    "    bef_mega = mega.loc[:,~mega.columns.duplicated()]\n",
    "    \n",
    "    for i, row in bef_mega.iterrows():\n",
    "        if not isNaN(bef_mega['Annotation_Source'][i]):\n",
    "        \n",
    "            if 'SIRIUS' in bef_mega['Annotation_Source'][i] and 'SIRIUS_Formula' not in bef_mega['Annotation_Source'][i]:\n",
    "                bef_mega.loc[i, 'SMILES_final'] = bef_mega['SMILES'][i]\n",
    "                bef_mega.loc[i,\"CompoundNames\"] = bef_mega['name'][i]\n",
    "                bef_mega['PC_MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['KG_MCSS_SMILES'][i] = np.nan\n",
    "            \n",
    "            \n",
    "            elif 'MassBank' in bef_mega['Annotation_Source'][i]:\n",
    "                bef_mega.loc[i, 'SMILES_final'] = bef_mega['MBSMILES'][i]\n",
    "                bef_mega.loc[i, 'CompoundNames'] = bef_mega['MBcompound_name'][i]\n",
    "                bef_mega['most_specific_class'][i] = np.nan\n",
    "                bef_mega['level _5'][i] = np.nan\n",
    "                bef_mega['subclass'][i] = np.nan\n",
    "                bef_mega['class'][i] = np.nan\n",
    "                bef_mega['superclass'][i] = np.nan\n",
    "                bef_mega['all_classifications'][i] = np.nan\n",
    "                bef_mega['Classification_Source'][i] = np.nan\n",
    "                bef_mega['MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['PC_MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['KG_MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['Formula'][i] = np.nan\n",
    "            \n",
    "            elif 'HMDB' in bef_mega['Annotation_Source'][i]:\n",
    "                bef_mega.loc[i, 'SMILES_final'] = bef_mega['HMDBSMILES'][i]\n",
    "                bef_mega.loc[i, 'CompoundNames'] = bef_mega['HMDBcompound_name'][i]\n",
    "                bef_mega['most_specific_class'][i] = np.nan\n",
    "                bef_mega['level _5'][i] = np.nan\n",
    "                bef_mega['subclass'][i] = np.nan\n",
    "                bef_mega['class'][i] = np.nan\n",
    "                bef_mega['superclass'][i] = np.nan\n",
    "                bef_mega['all_classifications'][i] = np.nan\n",
    "                bef_mega['Classification_Source'][i] = np.nan\n",
    "                bef_mega['MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['PC_MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['KG_MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['Formula'][i] = np.nan\n",
    "            \n",
    "            elif 'GNPS, SuspectList' in bef_mega['Annotation_Source'][i]:\n",
    "                bef_mega.loc[i,'SMILES_final'] = bef_mega['GLsmiles'][i]\n",
    "                bef_mega.loc[i, 'CompoundNames'] = bef_mega['GLname'][i]\n",
    "                bef_mega.loc[i, 'CompoundNames']\n",
    "                bef_mega['most_specific_class'][i] = np.nan\n",
    "                bef_mega['level _5'][i] = np.nan\n",
    "                bef_mega['subclass'][i] = np.nan\n",
    "                bef_mega['class'][i] = np.nan\n",
    "                bef_mega['superclass'][i] = np.nan\n",
    "                bef_mega['all_classifications'][i] = np.nan\n",
    "                bef_mega['MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['Classification_Source'][i] = np.nan\n",
    "                bef_mega['PC_MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['KG_MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['Formula'][i] = np.nan\n",
    "        \n",
    "            elif 'GNPS' in bef_mega['Annotation_Source'][i]:\n",
    "                bef_mega.loc[i,'SMILES_final'] = bef_mega['GNPSSMILES'][i]\n",
    "                bef_mega.loc[i, 'CompoundNames'] = bef_mega['GNPScompound_name'][i]\n",
    "                bef_mega['most_specific_class'][i] = np.nan\n",
    "                bef_mega['level _5'][i] = np.nan\n",
    "                bef_mega['subclass'][i] = np.nan\n",
    "                bef_mega['class'][i] = np.nan\n",
    "                bef_mega['superclass'][i] = np.nan\n",
    "                bef_mega['all_classifications'][i] = np.nan\n",
    "                bef_mega['MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['Classification_Source'][i] = np.nan\n",
    "                bef_mega['PC_MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['KG_MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['Formula'][i] = np.nan\n",
    "            \n",
    "            elif 'PubChem' in bef_mega['Annotation_Source'][i]:\n",
    "                bef_mega.loc[i, 'SMILES_final'] = bef_mega['PC_SMILES'][i]\n",
    "                bef_mega.loc[i, 'CompoundNames'] = bef_mega['PC_Name'][i]\n",
    "                bef_mega['most_specific_class'][i] = np.nan\n",
    "                bef_mega['level _5'][i] = np.nan\n",
    "                bef_mega['subclass'][i] = np.nan\n",
    "                bef_mega['class'][i] = np.nan\n",
    "                bef_mega['superclass'][i] = np.nan\n",
    "                bef_mega['all_classifications'][i] = np.nan\n",
    "                bef_mega['Classification_Source'][i] = np.nan\n",
    "                bef_mega['MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['KG_MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['Formula'][i] = np.nan\n",
    "\n",
    "            elif 'KEGG' in bef_mega['Annotation_Source'][i]:\n",
    "                bef_mega.loc[i, 'SMILES_final'] = bef_mega['KG_SMILES'][i]\n",
    "                bef_mega.loc[i, 'CompoundNames'] = bef_mega['KG_Name'][i]\n",
    "                bef_mega['most_specific_class'][i] = np.nan\n",
    "                bef_mega['level _5'][i] = np.nan\n",
    "                bef_mega['subclass'][i] = np.nan\n",
    "                bef_mega['class'][i] = np.nan\n",
    "                bef_mega['superclass'][i] = np.nan\n",
    "                bef_mega['all_classifications'][i] = np.nan\n",
    "                bef_mega['Classification_Source'][i] = np.nan\n",
    "                bef_mega['MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['PC_MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['Formula'][i] = np.nan\n",
    "            \n",
    "            elif 'SIRIUS_Formula' in bef_mega['Annotation_Source'][i]:\n",
    "                bef_mega['PC_MCSS_SMILES'][i] = np.nan\n",
    "                bef_mega['KG_MCSS_SMILES'][i] = np.nan\n",
    "                \n",
    "    \n",
    "    \n",
    "    \n",
    "    bef_megaA = bef_mega[['id_X', \n",
    "                          'premz', \n",
    "                          'rtmed', \n",
    "                          'rtmean',\n",
    "                          'int', \n",
    "                          'col_eng', \n",
    "                          'pol', \n",
    "                          'SMILES_final', \n",
    "                          'CompoundNames', \n",
    "                          'MCSS_SMILES', \n",
    "                          'PC_MCSS_SMILES', \n",
    "                          'KG_MCSS_SMILES', \n",
    "                          'subclass', \n",
    "                          'class', \n",
    "                          'superclass', \n",
    "                          'Classification_Source', \n",
    "                          'Annotation_Source'\n",
    "                         ]]\n",
    "    bef_megaA.to_csv(input_dir + \"MetabolomicsResults/final_curation_without_classes.csv\")\n",
    "    return(bef_megaA)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e1e4ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(combine_CuratedR.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "82f0a82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkSMILES_validity(input_dir, resultcsv):\n",
    "    \n",
    "    \"\"\"checkSMILES_validity does exactly as the name says, using\n",
    "    RDKit, whether the SMILES are invalid or have invalid \n",
    "    chemistry\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "    \n",
    "    results: df from combine_CuratedR\n",
    "    \n",
    "    Returns:\n",
    "    dataframe: with valid SMILES\n",
    "    csv: \"MetabolomicsResults/final_curation_with_validSMILES.csv\"\n",
    "    \n",
    "    Usage:\n",
    "    checkSMILES_validity(input_dir = \"usr/project/\", results)\n",
    "\n",
    "    \"\"\"\n",
    "    results = pd.read_csv(resultcsv)\n",
    "    # check validity of SMILES\n",
    "    for i, row in results.iterrows():\n",
    "        if not isNaN(results['SMILES_final'][i]):\n",
    "            m = Chem.MolFromSmiles(results['SMILES_final'][i] ,sanitize=False)\n",
    "            if m is None:\n",
    "                results['SMILES_final'][i] = 'invalid_SMILES'\n",
    "            else:\n",
    "                try:\n",
    "                    Chem.SanitizeMol(m)\n",
    "                except:\n",
    "                    results['SMILES_final'][i] = 'invalid_chemistry'\n",
    "    results.to_csv(input_dir + \"MetabolomicsResults/final_curation_with_validSMILES.csv\")\n",
    "    return(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "76c18e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(checkSMILES_validity.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e99e6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(input_dir, resultcsv):\n",
    "    \n",
    "    \"\"\"classification function uses ClassyFire ChemONT\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "    \n",
    "    resultcsv: csv of df from combine_CuratedR or checkSMILES_validity\n",
    "    \n",
    "    Returns:\n",
    "    dataframe: with classification\n",
    "    csv: \"MetabolomicsResults/final_curationList.csv\"\n",
    "    \n",
    "    Usage:\n",
    "    checkSMILES_validity(input_dir = \"usr/project/\", frame)\n",
    "\n",
    "    \"\"\"\n",
    "    frame = pd.read_csv(resultcsv)\n",
    "    inchis = []\n",
    "    for i, row in frame.iterrows():\n",
    "        if not isNaN(frame['SMILES_final'][i]) and isNaN(frame['Classification_Source'][i]):\n",
    "            try:\n",
    "                InChI = Chem.MolToInchi(Chem.MolFromSmiles(frame[\"SMILES_final\"][i]))\n",
    "                InChIKey = Chem.inchi.InchiToInchiKey(InChI)\n",
    "                inchis.append({\n",
    "                    'index': i,\n",
    "                    'smiles':frame[\"SMILES_final\"][i],\n",
    "                    'inchi': InChI,\n",
    "                    'inchikey': InChIKey\n",
    "                })\n",
    "            except:\n",
    "                pass\n",
    "    inchis = pd.DataFrame(inchis)\n",
    "    inchis = inchis.loc[-isNaN(inchis['inchikey'])]\n",
    "    ## Retrieve ClassyFire classifications ##\n",
    "    \n",
    "    # This first step is done using inchikey and interrogation of the gnps classified structures\n",
    "    gnps_proxy = True \n",
    "    url = \"http://classyfire.wishartlab.com\"\n",
    "    proxy_url =  \"https://gnps-classyfire.ucsd.edu\"\n",
    "    chunk_size = 1000\n",
    "    sleep_interval = 12\n",
    "    \n",
    "    all_inchi_keys = list(inchis['inchikey'].drop_duplicates())\n",
    "\n",
    "    resolved_ik_number_list = [0, 0]\n",
    "    total_inchikey_number = len(all_inchi_keys)\n",
    "\n",
    "    while True:\n",
    "    \n",
    "        #start_time = time.time()\n",
    "    \n",
    "        #print('%s inchikey to resolve' % total_inchikey_number )\n",
    "        get_classifications_cf_mod(all_inchi_keys, par_level = 6)\n",
    "    \n",
    "        cleanse('all_json.json', 'all_json.json')\n",
    "    \n",
    "        with open(\"all_json.json\") as tweetfile:\n",
    "            jsondic = json.loads(tweetfile.read())\n",
    "\n",
    "        df = json_normalize(jsondic)\n",
    "        df = df.drop_duplicates( 'inchikey' )\n",
    "        resolved_ik_number = len( df.drop_duplicates('inchikey').inchikey )\n",
    "        resolved_ik_number_list.append( resolved_ik_number )\n",
    "        #print('%s resolved inchikeys' % resolved_ik_number )\n",
    "        #print(\"done in --- %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "        if resolved_ik_number_list[-1] < resolved_ik_number_list[-2] or resolved_ik_number_list[-1] == resolved_ik_number_list[-3]:\n",
    "            break\n",
    "        cleanse('all_json.json', 'all_json_cleaned.json')\n",
    "        \n",
    "        with open(\"all_json_cleaned.json\") as tweetfile:\n",
    "            jsondic = json.loads(tweetfile.read())\n",
    "            \n",
    "    flattened_classified_json = json_normalize(jsondic)\n",
    "    flattened_df = flattened_classified_json.drop_duplicates('inchikey')\n",
    "    flattened_df['inchikey'] = flattened_df['inchikey'].str.replace(r'InChIKey=', '')\n",
    "    df_merged = pd.merge(inchis, flattened_df, left_on='inchikey', right_on='inchikey', how='left')\n",
    "    \n",
    "    for p, rowp in df_merged.iterrows():\n",
    "        for q, rowq in frame.iterrows():\n",
    "            if df_merged[\"smiles_x\"][p] is frame[\"SMILES_final\"][q]:\n",
    "                frame.loc[q, 'subclass'] = df_merged[\"subclass.name\"][p]\n",
    "                frame.loc[q, 'class'] = df_merged[\"class.name\"][p]\n",
    "                frame.loc[q, 'superclass'] = df_merged[\"superclass.name\"][p]\n",
    "                frame.loc[q, 'Classification_Source'] = \"ClassyFire\"\n",
    "    #frame.to_csv(input_dir, '/SIRIUS_combined.csv')\n",
    "    \n",
    "\n",
    "\n",
    "    frame.to_csv(input_dir + \"MetabolomicsResults/final_curationList.csv\")\n",
    "    return(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d0561a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(classification.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5e1ccf",
   "metadata": {},
   "source": [
    "# Comparison with a list of SMILES from any Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "77bdce92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMILESscreening(input_dir, results, listname):\n",
    "    \n",
    "    \"\"\"SMILESscreening takes a list of SMILES\n",
    "\n",
    "    Parameters:\n",
    "    input_dir (str): This is the input directory where all the .mzML \n",
    "    files and their respective result directories are stored.\n",
    "    \n",
    "    results: df from combine_CuratedR or checkSMILES_validity or classification\n",
    "    \n",
    "    Returns:\n",
    "    dataframe: comparison with another list of compounds\n",
    "    csv: \"MetabolomicsResults/final_curation_with_validSMILES.csv\"\n",
    "    \n",
    "    Usage:\n",
    "    checkSMILES_validity(input_dir = \"usr/project/\", results)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    for i, row in results.iterrows():\n",
    "        if not isNaN(results['SMILES_final'][i]):\n",
    "            if 'invalid_SMILES' not in results['SMILES_final'][i] and 'invalid_chemistry' not in results['SMILES_final'][i]:\n",
    "                for j in cd:\n",
    "                    if not isNaN(j):\n",
    "                        CGms = [Chem.MolFromSmiles(results['SMILES_final'][i]), Chem.MolFromSmiles(j)]\n",
    "                        CGfps = [AllChem.GetMorganFingerprintAsBitVect(x,2, nBits=1024) for x in CGms]\n",
    "                        CGtn = DataStructs.FingerprintSimilarity(CGfps[0],CGfps[1])\n",
    "                        if CGtn == 1 and 'CompoundDiscoverer' not in results['Annotation_Source'][i]:\n",
    "                            results['Annotation_Source'][i] = results['Annotation_Source'][i] + ', ' + listname\n",
    "                            results['Occurence'][i] = results['Occurence'][i] + 1\n",
    "    return(frame)\n",
    "\n",
    "    frame.to_csv(input_dir + \"MetabolomicsResults/final_curationListVS\"+listname+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "82049e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(SMILESscreening.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968d2ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba72546",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1e4041",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mawRpy)",
   "language": "python",
   "name": "mawrpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
